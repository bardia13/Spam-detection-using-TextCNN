{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.contrib import learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "  \n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_CNN(object):\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        \n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "        \n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "        \n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "    text = df.v2\n",
    "    label = df.v1\n",
    "    labels = []\n",
    "    for _ in label:\n",
    "        if _ == 'spam' :\n",
    "            labels.append([1,0])\n",
    "        else :\n",
    "            labels.append([0,1])\n",
    "    labels = np.array(labels)\n",
    "    text = [clean_str(x) for x in text]\n",
    "    \n",
    "    return [text, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess():\n",
    "    \n",
    "    x_text, y = load_data_and_labels()\n",
    "\n",
    "    print(y[:5])\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    \n",
    "    dev_sample_index = -1 * int(.1 * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=True,\n",
    "          log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = Text_CNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=128,\n",
    "                filter_sizes=list(map(int, [3,4,5])),\n",
    "                num_filters=128,\n",
    "                l2_reg_lambda=0.0)\n",
    "\n",
    "            \n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            \n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            \n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            \n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            \n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            \n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            \n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "            \n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "            \n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 0.5\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "            \n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            \n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), 64, 50) #number of epochs \n",
    "            \n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 100 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % 100 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Vocabulary Size: 8705\n",
      "Train/Dev split: 5015/557\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/bardia/Documents/TA/CI/Project/runs/1548532450\n",
      "\n",
      "2019-01-26T23:24:11.499774: step 1, loss 1.26037, acc 0.65625\n",
      "2019-01-26T23:24:11.884438: step 2, loss 1.31105, acc 0.734375\n",
      "2019-01-26T23:24:12.279659: step 3, loss 1.2422, acc 0.78125\n",
      "2019-01-26T23:24:12.652892: step 4, loss 1.17821, acc 0.828125\n",
      "2019-01-26T23:24:13.024220: step 5, loss 0.905987, acc 0.890625\n",
      "2019-01-26T23:24:13.397792: step 6, loss 0.963111, acc 0.859375\n",
      "2019-01-26T23:24:13.805652: step 7, loss 0.574098, acc 0.890625\n",
      "2019-01-26T23:24:14.179073: step 8, loss 0.400875, acc 0.90625\n",
      "2019-01-26T23:24:14.550563: step 9, loss 0.805866, acc 0.84375\n",
      "2019-01-26T23:24:14.921921: step 10, loss 0.515335, acc 0.875\n",
      "2019-01-26T23:24:15.288320: step 11, loss 0.333399, acc 0.875\n",
      "2019-01-26T23:24:15.657816: step 12, loss 0.792423, acc 0.765625\n",
      "2019-01-26T23:24:16.031002: step 13, loss 0.993468, acc 0.75\n",
      "2019-01-26T23:24:16.434292: step 14, loss 0.673095, acc 0.8125\n",
      "2019-01-26T23:24:16.853958: step 15, loss 0.828184, acc 0.8125\n",
      "2019-01-26T23:24:17.311893: step 16, loss 0.624962, acc 0.796875\n",
      "2019-01-26T23:24:17.781021: step 17, loss 0.714498, acc 0.75\n",
      "2019-01-26T23:24:18.164292: step 18, loss 0.78107, acc 0.796875\n",
      "2019-01-26T23:24:18.616329: step 19, loss 1.26083, acc 0.75\n",
      "2019-01-26T23:24:19.087724: step 20, loss 1.05529, acc 0.71875\n",
      "2019-01-26T23:24:19.570524: step 21, loss 0.486457, acc 0.84375\n",
      "2019-01-26T23:24:19.965173: step 22, loss 0.60342, acc 0.8125\n",
      "2019-01-26T23:24:20.355462: step 23, loss 0.827757, acc 0.828125\n",
      "2019-01-26T23:24:20.732195: step 24, loss 0.810797, acc 0.765625\n",
      "2019-01-26T23:24:21.105566: step 25, loss 0.957686, acc 0.796875\n",
      "2019-01-26T23:24:21.478538: step 26, loss 0.428862, acc 0.875\n",
      "2019-01-26T23:24:21.855983: step 27, loss 0.59525, acc 0.890625\n",
      "2019-01-26T23:24:22.231591: step 28, loss 0.19901, acc 0.9375\n",
      "2019-01-26T23:24:22.606330: step 29, loss 0.35651, acc 0.875\n",
      "2019-01-26T23:24:22.973793: step 30, loss 0.150172, acc 0.953125\n",
      "2019-01-26T23:24:23.347601: step 31, loss 0.775404, acc 0.859375\n",
      "2019-01-26T23:24:23.730258: step 32, loss 0.494875, acc 0.921875\n",
      "2019-01-26T23:24:24.101569: step 33, loss 0.330239, acc 0.921875\n",
      "2019-01-26T23:24:24.474474: step 34, loss 0.571168, acc 0.84375\n",
      "2019-01-26T23:24:24.851681: step 35, loss 0.309954, acc 0.921875\n",
      "2019-01-26T23:24:25.226960: step 36, loss 0.323705, acc 0.875\n",
      "2019-01-26T23:24:25.606870: step 37, loss 0.463319, acc 0.921875\n",
      "2019-01-26T23:24:25.979963: step 38, loss 0.536615, acc 0.875\n",
      "2019-01-26T23:24:26.350634: step 39, loss 0.684093, acc 0.84375\n",
      "2019-01-26T23:24:26.724685: step 40, loss 0.376877, acc 0.890625\n",
      "2019-01-26T23:24:27.113453: step 41, loss 0.292982, acc 0.921875\n",
      "2019-01-26T23:24:27.492376: step 42, loss 0.360516, acc 0.84375\n",
      "2019-01-26T23:24:27.867744: step 43, loss 0.420699, acc 0.875\n",
      "2019-01-26T23:24:28.240514: step 44, loss 0.486352, acc 0.875\n",
      "2019-01-26T23:24:28.617394: step 45, loss 0.229472, acc 0.9375\n",
      "2019-01-26T23:24:28.987879: step 46, loss 0.220879, acc 0.90625\n",
      "2019-01-26T23:24:29.358106: step 47, loss 0.445968, acc 0.84375\n",
      "2019-01-26T23:24:29.729437: step 48, loss 0.242286, acc 0.90625\n",
      "2019-01-26T23:24:30.098736: step 49, loss 0.337341, acc 0.875\n",
      "2019-01-26T23:24:30.478510: step 50, loss 0.775705, acc 0.84375\n",
      "2019-01-26T23:24:30.858991: step 51, loss 0.515217, acc 0.78125\n",
      "2019-01-26T23:24:31.232659: step 52, loss 0.506358, acc 0.875\n",
      "2019-01-26T23:24:31.606301: step 53, loss 0.792046, acc 0.828125\n",
      "2019-01-26T23:24:31.988847: step 54, loss 0.467476, acc 0.84375\n",
      "2019-01-26T23:24:32.358670: step 55, loss 0.509964, acc 0.890625\n",
      "2019-01-26T23:24:32.754181: step 56, loss 0.474403, acc 0.859375\n",
      "2019-01-26T23:24:33.125088: step 57, loss 0.471603, acc 0.8125\n",
      "2019-01-26T23:24:33.498234: step 58, loss 0.216728, acc 0.921875\n",
      "2019-01-26T23:24:33.882082: step 59, loss 0.21405, acc 0.953125\n",
      "2019-01-26T23:24:34.257389: step 60, loss 0.965185, acc 0.859375\n",
      "2019-01-26T23:24:34.644101: step 61, loss 0.833405, acc 0.875\n",
      "2019-01-26T23:24:35.037075: step 62, loss 0.503639, acc 0.921875\n",
      "2019-01-26T23:24:35.450181: step 63, loss 0.679362, acc 0.875\n",
      "2019-01-26T23:24:35.887424: step 64, loss 0.678378, acc 0.875\n",
      "2019-01-26T23:24:36.348033: step 65, loss 0.436401, acc 0.875\n",
      "2019-01-26T23:24:36.831790: step 66, loss 0.360613, acc 0.890625\n",
      "2019-01-26T23:24:37.344161: step 67, loss 0.23773, acc 0.9375\n",
      "2019-01-26T23:24:37.944722: step 68, loss 0.302608, acc 0.921875\n",
      "2019-01-26T23:24:38.664305: step 69, loss 0.51151, acc 0.875\n",
      "2019-01-26T23:24:39.440388: step 70, loss 0.525136, acc 0.84375\n",
      "2019-01-26T23:24:40.271507: step 71, loss 0.46202, acc 0.890625\n",
      "2019-01-26T23:24:41.123169: step 72, loss 0.555096, acc 0.78125\n",
      "2019-01-26T23:24:41.971016: step 73, loss 0.152512, acc 0.9375\n",
      "2019-01-26T23:24:42.824514: step 74, loss 0.33944, acc 0.90625\n",
      "2019-01-26T23:24:43.609476: step 75, loss 0.448727, acc 0.890625\n",
      "2019-01-26T23:24:44.377840: step 76, loss 0.360173, acc 0.921875\n",
      "2019-01-26T23:24:45.100718: step 77, loss 0.278591, acc 0.953125\n",
      "2019-01-26T23:24:45.802610: step 78, loss 0.183514, acc 0.90625\n",
      "2019-01-26T23:24:46.115577: step 79, loss 0.0979693, acc 0.956522\n",
      "2019-01-26T23:24:46.809704: step 80, loss 0.255096, acc 0.921875\n",
      "2019-01-26T23:24:47.445999: step 81, loss 0.459655, acc 0.9375\n",
      "2019-01-26T23:24:48.073758: step 82, loss 0.228393, acc 0.921875\n",
      "2019-01-26T23:24:48.663997: step 83, loss 0.229265, acc 0.921875\n",
      "2019-01-26T23:24:49.252116: step 84, loss 0.233443, acc 0.9375\n",
      "2019-01-26T23:24:49.832718: step 85, loss 0.344273, acc 0.90625\n",
      "2019-01-26T23:24:50.409314: step 86, loss 0.0882724, acc 0.9375\n",
      "2019-01-26T23:24:50.973712: step 87, loss 0.304076, acc 0.921875\n",
      "2019-01-26T23:24:51.508913: step 88, loss 0.130422, acc 0.9375\n",
      "2019-01-26T23:24:52.049109: step 89, loss 0.132841, acc 0.90625\n",
      "2019-01-26T23:24:52.573080: step 90, loss 0.127904, acc 0.953125\n",
      "2019-01-26T23:24:53.117927: step 91, loss 0.139233, acc 0.953125\n",
      "2019-01-26T23:24:53.641517: step 92, loss 0.282552, acc 0.921875\n",
      "2019-01-26T23:24:54.159922: step 93, loss 0.323197, acc 0.84375\n",
      "2019-01-26T23:24:54.671605: step 94, loss 0.0898793, acc 0.984375\n",
      "2019-01-26T23:24:55.180238: step 95, loss 0.469175, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:24:55.693972: step 96, loss 0.215805, acc 0.9375\n",
      "2019-01-26T23:24:56.178082: step 97, loss 0.0373711, acc 0.984375\n",
      "2019-01-26T23:24:56.660885: step 98, loss 0.177787, acc 0.9375\n",
      "2019-01-26T23:24:57.140825: step 99, loss 0.0679193, acc 0.96875\n",
      "2019-01-26T23:24:57.621962: step 100, loss 0.250917, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:24:59.012399: step 100, loss 0.0922412, acc 0.97307\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-100\n",
      "\n",
      "2019-01-26T23:24:59.577050: step 101, loss 0.304323, acc 0.921875\n",
      "2019-01-26T23:25:00.041567: step 102, loss 0.338927, acc 0.921875\n",
      "2019-01-26T23:25:00.508816: step 103, loss 0.266717, acc 0.890625\n",
      "2019-01-26T23:25:00.976647: step 104, loss 0.188882, acc 0.90625\n",
      "2019-01-26T23:25:01.438758: step 105, loss 0.15191, acc 0.9375\n",
      "2019-01-26T23:25:01.906476: step 106, loss 0.123264, acc 0.9375\n",
      "2019-01-26T23:25:02.350126: step 107, loss 0.300438, acc 0.921875\n",
      "2019-01-26T23:25:02.799122: step 108, loss 0.216834, acc 0.921875\n",
      "2019-01-26T23:25:03.238246: step 109, loss 0.259363, acc 0.9375\n",
      "2019-01-26T23:25:03.681336: step 110, loss 0.103509, acc 0.953125\n",
      "2019-01-26T23:25:04.128780: step 111, loss 0.236088, acc 0.921875\n",
      "2019-01-26T23:25:04.570380: step 112, loss 0.367252, acc 0.890625\n",
      "2019-01-26T23:25:04.997891: step 113, loss 0.19378, acc 0.9375\n",
      "2019-01-26T23:25:05.422614: step 114, loss 0.120205, acc 0.9375\n",
      "2019-01-26T23:25:05.854311: step 115, loss 0.142394, acc 0.96875\n",
      "2019-01-26T23:25:06.285985: step 116, loss 0.274971, acc 0.890625\n",
      "2019-01-26T23:25:06.718072: step 117, loss 0.206508, acc 0.921875\n",
      "2019-01-26T23:25:07.145019: step 118, loss 0.130132, acc 0.96875\n",
      "2019-01-26T23:25:07.570799: step 119, loss 0.103675, acc 0.953125\n",
      "2019-01-26T23:25:07.993528: step 120, loss 0.127011, acc 0.96875\n",
      "2019-01-26T23:25:08.408073: step 121, loss 0.118072, acc 0.9375\n",
      "2019-01-26T23:25:08.859692: step 122, loss 0.210426, acc 0.953125\n",
      "2019-01-26T23:25:09.284202: step 123, loss 0.355992, acc 0.953125\n",
      "2019-01-26T23:25:09.698304: step 124, loss 0.34853, acc 0.90625\n",
      "2019-01-26T23:25:10.110873: step 125, loss 0.141631, acc 0.953125\n",
      "2019-01-26T23:25:10.509856: step 126, loss 0.022079, acc 1\n",
      "2019-01-26T23:25:10.914249: step 127, loss 0.511801, acc 0.84375\n",
      "2019-01-26T23:25:11.323820: step 128, loss 0.276995, acc 0.953125\n",
      "2019-01-26T23:25:11.726551: step 129, loss 0.305421, acc 0.953125\n",
      "2019-01-26T23:25:12.125523: step 130, loss 0.142368, acc 0.953125\n",
      "2019-01-26T23:25:12.522386: step 131, loss 0.454494, acc 0.890625\n",
      "2019-01-26T23:25:12.932061: step 132, loss 0.228722, acc 0.96875\n",
      "2019-01-26T23:25:13.332060: step 133, loss 0.129223, acc 0.921875\n",
      "2019-01-26T23:25:13.739245: step 134, loss 0.438578, acc 0.921875\n",
      "2019-01-26T23:25:14.134617: step 135, loss 0.0855911, acc 0.984375\n",
      "2019-01-26T23:25:14.528814: step 136, loss 0.190612, acc 0.9375\n",
      "2019-01-26T23:25:14.954563: step 137, loss 0.179425, acc 0.953125\n",
      "2019-01-26T23:25:15.341581: step 138, loss 0.265625, acc 0.921875\n",
      "2019-01-26T23:25:15.734203: step 139, loss 0.061119, acc 0.984375\n",
      "2019-01-26T23:25:16.131716: step 140, loss 0.146582, acc 0.953125\n",
      "2019-01-26T23:25:16.526763: step 141, loss 0.201614, acc 0.953125\n",
      "2019-01-26T23:25:16.924618: step 142, loss 0.0669783, acc 0.96875\n",
      "2019-01-26T23:25:17.319854: step 143, loss 0.490536, acc 0.9375\n",
      "2019-01-26T23:25:17.718404: step 144, loss 0.100682, acc 0.953125\n",
      "2019-01-26T23:25:18.118223: step 145, loss 0.160829, acc 0.9375\n",
      "2019-01-26T23:25:18.515842: step 146, loss 0.135576, acc 0.953125\n",
      "2019-01-26T23:25:18.911574: step 147, loss 0.079368, acc 0.96875\n",
      "2019-01-26T23:25:19.301203: step 148, loss 0.22432, acc 0.921875\n",
      "2019-01-26T23:25:19.706393: step 149, loss 0.123249, acc 0.96875\n",
      "2019-01-26T23:25:20.103510: step 150, loss 0.185046, acc 0.953125\n",
      "2019-01-26T23:25:20.492147: step 151, loss 0.169974, acc 0.953125\n",
      "2019-01-26T23:25:20.893107: step 152, loss 0.153268, acc 0.9375\n",
      "2019-01-26T23:25:21.286286: step 153, loss 0.156346, acc 0.96875\n",
      "2019-01-26T23:25:21.681444: step 154, loss 0.152735, acc 0.953125\n",
      "2019-01-26T23:25:22.073727: step 155, loss 0.104407, acc 0.96875\n",
      "2019-01-26T23:25:22.464931: step 156, loss 0.0673085, acc 0.984375\n",
      "2019-01-26T23:25:22.865297: step 157, loss 0.182477, acc 0.984375\n",
      "2019-01-26T23:25:23.037074: step 158, loss 0.0777799, acc 0.956522\n",
      "2019-01-26T23:25:23.431462: step 159, loss 0.162355, acc 0.953125\n",
      "2019-01-26T23:25:23.824623: step 160, loss 0.191962, acc 0.953125\n",
      "2019-01-26T23:25:24.220343: step 161, loss 0.300472, acc 0.90625\n",
      "2019-01-26T23:25:24.617223: step 162, loss 0.0396983, acc 0.984375\n",
      "2019-01-26T23:25:25.014585: step 163, loss 0.234112, acc 0.921875\n",
      "2019-01-26T23:25:25.406520: step 164, loss 0.0291492, acc 0.984375\n",
      "2019-01-26T23:25:25.801539: step 165, loss 0.15045, acc 0.9375\n",
      "2019-01-26T23:25:26.195694: step 166, loss 0.064268, acc 0.984375\n",
      "2019-01-26T23:25:26.593331: step 167, loss 0.0913055, acc 0.921875\n",
      "2019-01-26T23:25:27.014631: step 168, loss 0.0280966, acc 0.984375\n",
      "2019-01-26T23:25:27.415585: step 169, loss 0.112299, acc 0.96875\n",
      "2019-01-26T23:25:27.828831: step 170, loss 0.23123, acc 0.9375\n",
      "2019-01-26T23:25:28.227813: step 171, loss 0.132885, acc 0.9375\n",
      "2019-01-26T23:25:28.629711: step 172, loss 0.280619, acc 0.921875\n",
      "2019-01-26T23:25:29.035137: step 173, loss 0.242038, acc 0.953125\n",
      "2019-01-26T23:25:29.435981: step 174, loss 0.0669171, acc 0.96875\n",
      "2019-01-26T23:25:29.838378: step 175, loss 0.15619, acc 0.96875\n",
      "2019-01-26T23:25:30.239177: step 176, loss 0.0698459, acc 0.96875\n",
      "2019-01-26T23:25:30.682692: step 177, loss 0.0798763, acc 0.96875\n",
      "2019-01-26T23:25:31.089519: step 178, loss 0.117295, acc 0.953125\n",
      "2019-01-26T23:25:31.502560: step 179, loss 0.0585736, acc 0.984375\n",
      "2019-01-26T23:25:31.924102: step 180, loss 0.249698, acc 0.953125\n",
      "2019-01-26T23:25:32.341055: step 181, loss 0.189704, acc 0.9375\n",
      "2019-01-26T23:25:32.769688: step 182, loss 0.0435291, acc 0.984375\n",
      "2019-01-26T23:25:33.196990: step 183, loss 0.276694, acc 0.9375\n",
      "2019-01-26T23:25:33.636577: step 184, loss 0.0748677, acc 0.96875\n",
      "2019-01-26T23:25:34.069988: step 185, loss 0.117666, acc 0.96875\n",
      "2019-01-26T23:25:34.499125: step 186, loss 0.131355, acc 0.9375\n",
      "2019-01-26T23:25:34.941146: step 187, loss 0.00951291, acc 1\n",
      "2019-01-26T23:25:35.371088: step 188, loss 0.077448, acc 0.953125\n",
      "2019-01-26T23:25:35.803482: step 189, loss 0.036725, acc 0.984375\n",
      "2019-01-26T23:25:36.237621: step 190, loss 0.0872015, acc 0.953125\n",
      "2019-01-26T23:25:36.678405: step 191, loss 0.127487, acc 0.953125\n",
      "2019-01-26T23:25:37.110608: step 192, loss 0.353985, acc 0.9375\n",
      "2019-01-26T23:25:37.542905: step 193, loss 0.353219, acc 0.9375\n",
      "2019-01-26T23:25:37.966480: step 194, loss 0.0516921, acc 0.96875\n",
      "2019-01-26T23:25:38.385784: step 195, loss 0.123372, acc 0.953125\n",
      "2019-01-26T23:25:38.810068: step 196, loss 0.0586191, acc 0.96875\n",
      "2019-01-26T23:25:39.225906: step 197, loss 0.154972, acc 0.96875\n",
      "2019-01-26T23:25:39.642083: step 198, loss 0.136667, acc 0.921875\n",
      "2019-01-26T23:25:40.061529: step 199, loss 0.188133, acc 0.921875\n",
      "2019-01-26T23:25:40.479225: step 200, loss 0.0560297, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:25:41.514026: step 200, loss 0.056922, acc 0.983842\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-200\n",
      "\n",
      "2019-01-26T23:25:42.004207: step 201, loss 0.15269, acc 0.96875\n",
      "2019-01-26T23:25:42.408901: step 202, loss 0.0433195, acc 0.984375\n",
      "2019-01-26T23:25:42.823936: step 203, loss 0.135946, acc 0.953125\n",
      "2019-01-26T23:25:43.245290: step 204, loss 0.0329991, acc 0.984375\n",
      "2019-01-26T23:25:43.667859: step 205, loss 0.0212046, acc 1\n",
      "2019-01-26T23:25:44.094479: step 206, loss 0.165509, acc 0.96875\n",
      "2019-01-26T23:25:44.512681: step 207, loss 0.122504, acc 0.96875\n",
      "2019-01-26T23:25:44.978501: step 208, loss 0.0664649, acc 0.953125\n",
      "2019-01-26T23:25:45.394273: step 209, loss 0.0317915, acc 1\n",
      "2019-01-26T23:25:45.818932: step 210, loss 0.140499, acc 0.953125\n",
      "2019-01-26T23:25:46.238489: step 211, loss 0.0213947, acc 1\n",
      "2019-01-26T23:25:46.655366: step 212, loss 0.0406381, acc 0.984375\n",
      "2019-01-26T23:25:47.073016: step 213, loss 0.0301471, acc 0.984375\n",
      "2019-01-26T23:25:47.492081: step 214, loss 0.00654387, acc 1\n",
      "2019-01-26T23:25:47.910827: step 215, loss 0.0563861, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:25:48.332377: step 216, loss 0.140653, acc 0.953125\n",
      "2019-01-26T23:25:48.736602: step 217, loss 0.0882356, acc 0.96875\n",
      "2019-01-26T23:25:49.141230: step 218, loss 0.17978, acc 0.953125\n",
      "2019-01-26T23:25:49.579787: step 219, loss 0.0486041, acc 0.96875\n",
      "2019-01-26T23:25:49.995425: step 220, loss 0.0243765, acc 0.984375\n",
      "2019-01-26T23:25:50.395978: step 221, loss 0.431712, acc 0.9375\n",
      "2019-01-26T23:25:50.797666: step 222, loss 0.111726, acc 0.96875\n",
      "2019-01-26T23:25:51.198124: step 223, loss 0.0574745, acc 0.96875\n",
      "2019-01-26T23:25:51.621674: step 224, loss 0.0187981, acc 1\n",
      "2019-01-26T23:25:52.020787: step 225, loss 0.131589, acc 0.9375\n",
      "2019-01-26T23:25:52.419189: step 226, loss 0.0498169, acc 0.96875\n",
      "2019-01-26T23:25:52.825890: step 227, loss 0.166486, acc 0.9375\n",
      "2019-01-26T23:25:53.226893: step 228, loss 0.225352, acc 0.953125\n",
      "2019-01-26T23:25:53.633595: step 229, loss 0.0612039, acc 0.96875\n",
      "2019-01-26T23:25:54.026090: step 230, loss 0.141457, acc 0.984375\n",
      "2019-01-26T23:25:54.430610: step 231, loss 0.246585, acc 0.953125\n",
      "2019-01-26T23:25:54.826771: step 232, loss 0.030753, acc 1\n",
      "2019-01-26T23:25:55.229844: step 233, loss 0.121257, acc 0.96875\n",
      "2019-01-26T23:25:55.628936: step 234, loss 0.030862, acc 0.984375\n",
      "2019-01-26T23:25:56.035240: step 235, loss 0.103867, acc 0.984375\n",
      "2019-01-26T23:25:56.433948: step 236, loss 0.0973698, acc 0.953125\n",
      "2019-01-26T23:25:56.614431: step 237, loss 0.041381, acc 1\n",
      "2019-01-26T23:25:57.019132: step 238, loss 0.324405, acc 0.953125\n",
      "2019-01-26T23:25:57.427646: step 239, loss 0.188383, acc 0.953125\n",
      "2019-01-26T23:25:57.830124: step 240, loss 0.190795, acc 0.921875\n",
      "2019-01-26T23:25:58.232875: step 241, loss 0.0938925, acc 0.953125\n",
      "2019-01-26T23:25:58.661818: step 242, loss 0.00807509, acc 1\n",
      "2019-01-26T23:25:59.062122: step 243, loss 0.262958, acc 0.953125\n",
      "2019-01-26T23:25:59.469139: step 244, loss 0.0178217, acc 0.984375\n",
      "2019-01-26T23:25:59.921247: step 245, loss 0.131895, acc 0.953125\n",
      "2019-01-26T23:26:00.339561: step 246, loss 0.0998438, acc 0.984375\n",
      "2019-01-26T23:26:00.761056: step 247, loss 0.0228261, acc 1\n",
      "2019-01-26T23:26:01.183625: step 248, loss 0.0291432, acc 1\n",
      "2019-01-26T23:26:01.610736: step 249, loss 0.0380239, acc 1\n",
      "2019-01-26T23:26:02.036528: step 250, loss 0.074334, acc 0.96875\n",
      "2019-01-26T23:26:02.465954: step 251, loss 0.126834, acc 0.953125\n",
      "2019-01-26T23:26:02.901355: step 252, loss 0.117323, acc 0.953125\n",
      "2019-01-26T23:26:03.321925: step 253, loss 0.0708382, acc 0.96875\n",
      "2019-01-26T23:26:03.750432: step 254, loss 0.103344, acc 0.96875\n",
      "2019-01-26T23:26:04.183084: step 255, loss 0.0412925, acc 0.984375\n",
      "2019-01-26T23:26:04.610175: step 256, loss 0.0563693, acc 0.984375\n",
      "2019-01-26T23:26:05.035104: step 257, loss 0.0683077, acc 0.96875\n",
      "2019-01-26T23:26:05.464991: step 258, loss 0.0832838, acc 0.984375\n",
      "2019-01-26T23:26:05.886947: step 259, loss 0.212261, acc 0.9375\n",
      "2019-01-26T23:26:06.310178: step 260, loss 0.0509409, acc 0.96875\n",
      "2019-01-26T23:26:06.734601: step 261, loss 0.131292, acc 0.9375\n",
      "2019-01-26T23:26:07.156518: step 262, loss 0.0514377, acc 0.96875\n",
      "2019-01-26T23:26:07.565677: step 263, loss 0.0148832, acc 1\n",
      "2019-01-26T23:26:07.989806: step 264, loss 0.026341, acc 0.984375\n",
      "2019-01-26T23:26:08.404902: step 265, loss 0.219623, acc 0.9375\n",
      "2019-01-26T23:26:08.827387: step 266, loss 0.0184483, acc 1\n",
      "2019-01-26T23:26:09.247906: step 267, loss 0.136004, acc 0.984375\n",
      "2019-01-26T23:26:09.655377: step 268, loss 0.0100321, acc 1\n",
      "2019-01-26T23:26:10.059821: step 269, loss 0.0693507, acc 0.96875\n",
      "2019-01-26T23:26:10.463945: step 270, loss 0.125375, acc 0.96875\n",
      "2019-01-26T23:26:10.867187: step 271, loss 0.0615097, acc 0.953125\n",
      "2019-01-26T23:26:11.281930: step 272, loss 0.238181, acc 0.953125\n",
      "2019-01-26T23:26:11.701392: step 273, loss 0.0494297, acc 0.96875\n",
      "2019-01-26T23:26:12.120346: step 274, loss 0.0492462, acc 0.984375\n",
      "2019-01-26T23:26:12.536783: step 275, loss 0.00703269, acc 1\n",
      "2019-01-26T23:26:12.955174: step 276, loss 0.0858849, acc 0.96875\n",
      "2019-01-26T23:26:13.376599: step 277, loss 0.0145486, acc 1\n",
      "2019-01-26T23:26:13.797502: step 278, loss 0.076536, acc 0.984375\n",
      "2019-01-26T23:26:14.218223: step 279, loss 0.178495, acc 0.953125\n",
      "2019-01-26T23:26:14.650867: step 280, loss 0.093826, acc 0.96875\n",
      "2019-01-26T23:26:15.071147: step 281, loss 0.0238706, acc 0.984375\n",
      "2019-01-26T23:26:15.485275: step 282, loss 0.0149273, acc 1\n",
      "2019-01-26T23:26:15.909516: step 283, loss 0.0422842, acc 0.984375\n",
      "2019-01-26T23:26:16.321424: step 284, loss 0.091036, acc 0.96875\n",
      "2019-01-26T23:26:16.728046: step 285, loss 0.0663018, acc 0.96875\n",
      "2019-01-26T23:26:17.138743: step 286, loss 0.0273139, acc 1\n",
      "2019-01-26T23:26:17.543847: step 287, loss 0.150504, acc 0.921875\n",
      "2019-01-26T23:26:17.945343: step 288, loss 0.0873486, acc 0.953125\n",
      "2019-01-26T23:26:18.351022: step 289, loss 0.0873085, acc 0.96875\n",
      "2019-01-26T23:26:18.763873: step 290, loss 0.0214894, acc 0.984375\n",
      "2019-01-26T23:26:19.179800: step 291, loss 0.0330307, acc 0.984375\n",
      "2019-01-26T23:26:19.599878: step 292, loss 0.0927302, acc 0.984375\n",
      "2019-01-26T23:26:20.017203: step 293, loss 0.00683507, acc 1\n",
      "2019-01-26T23:26:20.433549: step 294, loss 0.0931408, acc 0.953125\n",
      "2019-01-26T23:26:20.853245: step 295, loss 0.109115, acc 0.953125\n",
      "2019-01-26T23:26:21.267755: step 296, loss 0.0743648, acc 0.96875\n",
      "2019-01-26T23:26:21.686629: step 297, loss 0.138702, acc 0.96875\n",
      "2019-01-26T23:26:22.105158: step 298, loss 0.0662015, acc 0.96875\n",
      "2019-01-26T23:26:22.517847: step 299, loss 0.0697627, acc 0.96875\n",
      "2019-01-26T23:26:22.946822: step 300, loss 0.0473872, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:26:23.968962: step 300, loss 0.057974, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-300\n",
      "\n",
      "2019-01-26T23:26:24.450130: step 301, loss 0.0479329, acc 0.984375\n",
      "2019-01-26T23:26:24.854098: step 302, loss 0.0805202, acc 0.96875\n",
      "2019-01-26T23:26:25.254846: step 303, loss 0.109251, acc 0.96875\n",
      "2019-01-26T23:26:25.664838: step 304, loss 0.0466911, acc 0.96875\n",
      "2019-01-26T23:26:26.068571: step 305, loss 0.095437, acc 0.9375\n",
      "2019-01-26T23:26:26.468912: step 306, loss 0.242651, acc 0.90625\n",
      "2019-01-26T23:26:26.896507: step 307, loss 0.0277774, acc 1\n",
      "2019-01-26T23:26:27.318767: step 308, loss 0.110466, acc 0.984375\n",
      "2019-01-26T23:26:27.771474: step 309, loss 0.0590283, acc 0.984375\n",
      "2019-01-26T23:26:28.192986: step 310, loss 0.126645, acc 0.984375\n",
      "2019-01-26T23:26:28.612655: step 311, loss 0.0568733, acc 0.984375\n",
      "2019-01-26T23:26:29.030528: step 312, loss 0.0485903, acc 0.984375\n",
      "2019-01-26T23:26:29.450335: step 313, loss 0.149013, acc 0.9375\n",
      "2019-01-26T23:26:29.866888: step 314, loss 0.0368868, acc 0.984375\n",
      "2019-01-26T23:26:30.290852: step 315, loss 0.342558, acc 0.890625\n",
      "2019-01-26T23:26:30.470641: step 316, loss 0.0776769, acc 0.956522\n",
      "2019-01-26T23:26:30.891258: step 317, loss 0.149918, acc 0.953125\n",
      "2019-01-26T23:26:31.305130: step 318, loss 0.0410639, acc 0.984375\n",
      "2019-01-26T23:26:31.722041: step 319, loss 0.0986745, acc 0.96875\n",
      "2019-01-26T23:26:32.142053: step 320, loss 0.20188, acc 0.96875\n",
      "2019-01-26T23:26:32.564513: step 321, loss 0.0409749, acc 0.984375\n",
      "2019-01-26T23:26:32.970452: step 322, loss 0.0637332, acc 0.984375\n",
      "2019-01-26T23:26:33.375060: step 323, loss 0.016132, acc 1\n",
      "2019-01-26T23:26:33.779812: step 324, loss 0.0196739, acc 0.984375\n",
      "2019-01-26T23:26:34.193320: step 325, loss 0.0408926, acc 0.984375\n",
      "2019-01-26T23:26:34.599534: step 326, loss 0.0171721, acc 1\n",
      "2019-01-26T23:26:35.014789: step 327, loss 0.00596698, acc 1\n",
      "2019-01-26T23:26:35.430252: step 328, loss 0.0696009, acc 0.96875\n",
      "2019-01-26T23:26:35.846013: step 329, loss 0.103095, acc 0.96875\n",
      "2019-01-26T23:26:36.272692: step 330, loss 0.0693223, acc 0.96875\n",
      "2019-01-26T23:26:36.689962: step 331, loss 0.0108456, acc 1\n",
      "2019-01-26T23:26:37.109353: step 332, loss 0.0543423, acc 0.984375\n",
      "2019-01-26T23:26:37.528264: step 333, loss 0.0165583, acc 1\n",
      "2019-01-26T23:26:37.948606: step 334, loss 0.0606541, acc 0.984375\n",
      "2019-01-26T23:26:38.366545: step 335, loss 0.0818638, acc 0.96875\n",
      "2019-01-26T23:26:38.792990: step 336, loss 0.101738, acc 0.953125\n",
      "2019-01-26T23:26:39.223541: step 337, loss 0.166607, acc 0.953125\n",
      "2019-01-26T23:26:39.656285: step 338, loss 0.0440688, acc 0.984375\n",
      "2019-01-26T23:26:40.088546: step 339, loss 0.188795, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:26:40.525307: step 340, loss 0.0145829, acc 1\n",
      "2019-01-26T23:26:40.972733: step 341, loss 0.148165, acc 0.9375\n",
      "2019-01-26T23:26:41.415706: step 342, loss 0.025517, acc 0.984375\n",
      "2019-01-26T23:26:41.863339: step 343, loss 0.0533726, acc 0.96875\n",
      "2019-01-26T23:26:42.308125: step 344, loss 0.000995091, acc 1\n",
      "2019-01-26T23:26:42.767536: step 345, loss 0.0473097, acc 0.96875\n",
      "2019-01-26T23:26:43.210691: step 346, loss 0.00182587, acc 1\n",
      "2019-01-26T23:26:43.653393: step 347, loss 0.0360012, acc 0.96875\n",
      "2019-01-26T23:26:44.096995: step 348, loss 0.0652739, acc 0.96875\n",
      "2019-01-26T23:26:44.543475: step 349, loss 0.0100563, acc 1\n",
      "2019-01-26T23:26:44.987888: step 350, loss 0.247142, acc 0.9375\n",
      "2019-01-26T23:26:45.431883: step 351, loss 0.0150099, acc 1\n",
      "2019-01-26T23:26:45.878965: step 352, loss 0.0408462, acc 0.984375\n",
      "2019-01-26T23:26:46.325391: step 353, loss 0.0174304, acc 0.984375\n",
      "2019-01-26T23:26:46.770005: step 354, loss 0.194338, acc 0.96875\n",
      "2019-01-26T23:26:47.221109: step 355, loss 0.00718158, acc 1\n",
      "2019-01-26T23:26:47.704066: step 356, loss 0.121828, acc 0.9375\n",
      "2019-01-26T23:26:48.151193: step 357, loss 0.00396334, acc 1\n",
      "2019-01-26T23:26:48.584867: step 358, loss 0.0185087, acc 1\n",
      "2019-01-26T23:26:49.012111: step 359, loss 0.00339671, acc 1\n",
      "2019-01-26T23:26:49.435110: step 360, loss 0.0555332, acc 0.984375\n",
      "2019-01-26T23:26:49.868942: step 361, loss 0.00835463, acc 1\n",
      "2019-01-26T23:26:50.296819: step 362, loss 0.0865377, acc 0.9375\n",
      "2019-01-26T23:26:50.734703: step 363, loss 0.0114441, acc 1\n",
      "2019-01-26T23:26:51.166795: step 364, loss 0.0129667, acc 1\n",
      "2019-01-26T23:26:51.589238: step 365, loss 0.0267971, acc 0.984375\n",
      "2019-01-26T23:26:52.004654: step 366, loss 0.0287219, acc 0.984375\n",
      "2019-01-26T23:26:52.420260: step 367, loss 0.105329, acc 0.96875\n",
      "2019-01-26T23:26:52.837702: step 368, loss 0.0351678, acc 0.96875\n",
      "2019-01-26T23:26:53.251589: step 369, loss 0.0302332, acc 0.96875\n",
      "2019-01-26T23:26:53.663349: step 370, loss 0.0956077, acc 0.96875\n",
      "2019-01-26T23:26:54.079520: step 371, loss 0.0308156, acc 0.984375\n",
      "2019-01-26T23:26:54.483326: step 372, loss 0.0236158, acc 0.984375\n",
      "2019-01-26T23:26:54.899468: step 373, loss 0.00841515, acc 1\n",
      "2019-01-26T23:26:55.319156: step 374, loss 0.0463244, acc 0.984375\n",
      "2019-01-26T23:26:55.733716: step 375, loss 0.0154355, acc 0.984375\n",
      "2019-01-26T23:26:56.148447: step 376, loss 0.100825, acc 0.953125\n",
      "2019-01-26T23:26:56.573547: step 377, loss 0.0762034, acc 0.953125\n",
      "2019-01-26T23:26:56.987239: step 378, loss 0.0702352, acc 0.96875\n",
      "2019-01-26T23:26:57.402719: step 379, loss 0.15942, acc 0.9375\n",
      "2019-01-26T23:26:57.821463: step 380, loss 0.0251081, acc 0.984375\n",
      "2019-01-26T23:26:58.235998: step 381, loss 0.0176162, acc 1\n",
      "2019-01-26T23:26:58.654554: step 382, loss 0.0228782, acc 0.984375\n",
      "2019-01-26T23:26:59.082725: step 383, loss 0.0138048, acc 1\n",
      "2019-01-26T23:26:59.509049: step 384, loss 0.0153914, acc 1\n",
      "2019-01-26T23:26:59.977113: step 385, loss 0.0563475, acc 0.96875\n",
      "2019-01-26T23:27:00.421885: step 386, loss 0.0361394, acc 0.984375\n",
      "2019-01-26T23:27:00.868076: step 387, loss 0.127171, acc 0.9375\n",
      "2019-01-26T23:27:01.306962: step 388, loss 0.0905494, acc 0.96875\n",
      "2019-01-26T23:27:01.745251: step 389, loss 0.0227635, acc 0.984375\n",
      "2019-01-26T23:27:02.200201: step 390, loss 0.0313781, acc 0.984375\n",
      "2019-01-26T23:27:02.667042: step 391, loss 0.0117077, acc 1\n",
      "2019-01-26T23:27:03.132482: step 392, loss 0.265319, acc 0.9375\n",
      "2019-01-26T23:27:03.578657: step 393, loss 0.105205, acc 0.953125\n",
      "2019-01-26T23:27:04.022180: step 394, loss 0.0316708, acc 0.984375\n",
      "2019-01-26T23:27:04.209930: step 395, loss 0.00340216, acc 1\n",
      "2019-01-26T23:27:04.658149: step 396, loss 0.0117526, acc 1\n",
      "2019-01-26T23:27:05.108881: step 397, loss 0.140477, acc 0.96875\n",
      "2019-01-26T23:27:05.561923: step 398, loss 0.0116244, acc 1\n",
      "2019-01-26T23:27:06.000226: step 399, loss 0.000429466, acc 1\n",
      "2019-01-26T23:27:06.444080: step 400, loss 0.0454098, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:27:07.593350: step 400, loss 0.0660759, acc 0.982047\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-400\n",
      "\n",
      "2019-01-26T23:27:08.111639: step 401, loss 0.159134, acc 0.953125\n",
      "2019-01-26T23:27:08.543233: step 402, loss 0.0048736, acc 1\n",
      "2019-01-26T23:27:08.975566: step 403, loss 0.0977921, acc 0.96875\n",
      "2019-01-26T23:27:09.407868: step 404, loss 0.0232256, acc 1\n",
      "2019-01-26T23:27:09.836767: step 405, loss 0.0186156, acc 1\n",
      "2019-01-26T23:27:10.268331: step 406, loss 0.0404475, acc 0.96875\n",
      "2019-01-26T23:27:10.701201: step 407, loss 0.0083093, acc 1\n",
      "2019-01-26T23:27:11.129998: step 408, loss 0.017457, acc 0.984375\n",
      "2019-01-26T23:27:11.560074: step 409, loss 0.0539426, acc 0.96875\n",
      "2019-01-26T23:27:11.976974: step 410, loss 0.0407693, acc 0.96875\n",
      "2019-01-26T23:27:12.393562: step 411, loss 0.0246868, acc 0.984375\n",
      "2019-01-26T23:27:12.816011: step 412, loss 0.101534, acc 0.96875\n",
      "2019-01-26T23:27:13.231899: step 413, loss 0.020502, acc 1\n",
      "2019-01-26T23:27:13.655715: step 414, loss 0.219108, acc 0.9375\n",
      "2019-01-26T23:27:14.073758: step 415, loss 0.00468989, acc 1\n",
      "2019-01-26T23:27:14.471494: step 416, loss 0.10065, acc 0.96875\n",
      "2019-01-26T23:27:14.876855: step 417, loss 0.0859193, acc 0.96875\n",
      "2019-01-26T23:27:15.284914: step 418, loss 0.0094064, acc 1\n",
      "2019-01-26T23:27:15.691399: step 419, loss 0.177282, acc 0.96875\n",
      "2019-01-26T23:27:16.096622: step 420, loss 0.0155211, acc 1\n",
      "2019-01-26T23:27:16.496148: step 421, loss 0.00556823, acc 1\n",
      "2019-01-26T23:27:16.904742: step 422, loss 0.0890863, acc 0.96875\n",
      "2019-01-26T23:27:17.307220: step 423, loss 0.0787234, acc 0.953125\n",
      "2019-01-26T23:27:17.717508: step 424, loss 0.0526785, acc 0.984375\n",
      "2019-01-26T23:27:18.113513: step 425, loss 0.016102, acc 1\n",
      "2019-01-26T23:27:18.516289: step 426, loss 0.0380872, acc 0.984375\n",
      "2019-01-26T23:27:18.918789: step 427, loss 0.0198206, acc 0.984375\n",
      "2019-01-26T23:27:19.321675: step 428, loss 0.0327488, acc 0.984375\n",
      "2019-01-26T23:27:19.725802: step 429, loss 0.00812576, acc 1\n",
      "2019-01-26T23:27:20.126063: step 430, loss 0.0255513, acc 0.984375\n",
      "2019-01-26T23:27:20.526504: step 431, loss 0.000634866, acc 1\n",
      "2019-01-26T23:27:20.933058: step 432, loss 0.00611218, acc 1\n",
      "2019-01-26T23:27:21.335159: step 433, loss 0.0405109, acc 0.984375\n",
      "2019-01-26T23:27:21.738155: step 434, loss 0.0202098, acc 0.984375\n",
      "2019-01-26T23:27:22.147165: step 435, loss 0.00108684, acc 1\n",
      "2019-01-26T23:27:22.550303: step 436, loss 0.0761316, acc 0.984375\n",
      "2019-01-26T23:27:22.955431: step 437, loss 0.0181201, acc 1\n",
      "2019-01-26T23:27:23.364795: step 438, loss 0.000408183, acc 1\n",
      "2019-01-26T23:27:23.821906: step 439, loss 0.0204492, acc 1\n",
      "2019-01-26T23:27:24.238690: step 440, loss 0.0059997, acc 1\n",
      "2019-01-26T23:27:24.653710: step 441, loss 0.000694187, acc 1\n",
      "2019-01-26T23:27:25.083139: step 442, loss 0.000475001, acc 1\n",
      "2019-01-26T23:27:25.514782: step 443, loss 0.0786137, acc 0.953125\n",
      "2019-01-26T23:27:25.946741: step 444, loss 0.0156724, acc 1\n",
      "2019-01-26T23:27:26.375267: step 445, loss 0.0501628, acc 0.984375\n",
      "2019-01-26T23:27:26.812007: step 446, loss 0.0388476, acc 0.96875\n",
      "2019-01-26T23:27:27.270839: step 447, loss 0.0360419, acc 0.984375\n",
      "2019-01-26T23:27:27.720631: step 448, loss 0.0384409, acc 0.984375\n",
      "2019-01-26T23:27:28.180121: step 449, loss 0.066069, acc 0.96875\n",
      "2019-01-26T23:27:28.649532: step 450, loss 0.0170564, acc 0.984375\n",
      "2019-01-26T23:27:29.113518: step 451, loss 0.0607363, acc 0.984375\n",
      "2019-01-26T23:27:29.590098: step 452, loss 0.0811979, acc 0.984375\n",
      "2019-01-26T23:27:30.071104: step 453, loss 0.0196931, acc 0.984375\n",
      "2019-01-26T23:27:30.556686: step 454, loss 0.0456464, acc 0.984375\n",
      "2019-01-26T23:27:31.037386: step 455, loss 0.0291089, acc 0.984375\n",
      "2019-01-26T23:27:31.533943: step 456, loss 0.0147768, acc 1\n",
      "2019-01-26T23:27:32.026411: step 457, loss 0.0695581, acc 0.96875\n",
      "2019-01-26T23:27:32.503259: step 458, loss 0.0204747, acc 0.984375\n",
      "2019-01-26T23:27:32.991237: step 459, loss 0.0286214, acc 0.984375\n",
      "2019-01-26T23:27:33.473254: step 460, loss 0.0194626, acc 1\n",
      "2019-01-26T23:27:33.968698: step 461, loss 0.0120974, acc 1\n",
      "2019-01-26T23:27:34.455271: step 462, loss 0.00252327, acc 1\n",
      "2019-01-26T23:27:34.923729: step 463, loss 0.00830054, acc 1\n",
      "2019-01-26T23:27:35.386848: step 464, loss 0.120787, acc 0.984375\n",
      "2019-01-26T23:27:35.858234: step 465, loss 0.0133845, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:27:36.326712: step 466, loss 0.148157, acc 0.96875\n",
      "2019-01-26T23:27:36.791222: step 467, loss 0.0161797, acc 0.984375\n",
      "2019-01-26T23:27:37.241242: step 468, loss 0.0639936, acc 0.96875\n",
      "2019-01-26T23:27:37.689775: step 469, loss 0.0764795, acc 0.984375\n",
      "2019-01-26T23:27:38.133934: step 470, loss 0.00156959, acc 1\n",
      "2019-01-26T23:27:38.573885: step 471, loss 0.000756919, acc 1\n",
      "2019-01-26T23:27:39.015378: step 472, loss 0.0107632, acc 1\n",
      "2019-01-26T23:27:39.453442: step 473, loss 0.0438444, acc 0.984375\n",
      "2019-01-26T23:27:39.649849: step 474, loss 0.103802, acc 0.956522\n",
      "2019-01-26T23:27:40.097116: step 475, loss 0.000872729, acc 1\n",
      "2019-01-26T23:27:40.526325: step 476, loss 0.0237954, acc 0.984375\n",
      "2019-01-26T23:27:40.960597: step 477, loss 0.00101331, acc 1\n",
      "2019-01-26T23:27:41.391259: step 478, loss 0.0812446, acc 0.984375\n",
      "2019-01-26T23:27:41.826402: step 479, loss 0.0117772, acc 0.984375\n",
      "2019-01-26T23:27:42.249906: step 480, loss 0.00577734, acc 1\n",
      "2019-01-26T23:27:42.679588: step 481, loss 0.0170382, acc 0.984375\n",
      "2019-01-26T23:27:43.108281: step 482, loss 0.0293208, acc 0.984375\n",
      "2019-01-26T23:27:43.528458: step 483, loss 0.0153437, acc 0.984375\n",
      "2019-01-26T23:27:43.942934: step 484, loss 0.00926645, acc 1\n",
      "2019-01-26T23:27:44.361948: step 485, loss 0.00114793, acc 1\n",
      "2019-01-26T23:27:44.776997: step 486, loss 0.00906547, acc 1\n",
      "2019-01-26T23:27:45.189400: step 487, loss 0.00738378, acc 1\n",
      "2019-01-26T23:27:45.604282: step 488, loss 0.000852294, acc 1\n",
      "2019-01-26T23:27:46.019114: step 489, loss 0.00588563, acc 1\n",
      "2019-01-26T23:27:46.425339: step 490, loss 0.00409789, acc 1\n",
      "2019-01-26T23:27:46.834893: step 491, loss 0.0114623, acc 1\n",
      "2019-01-26T23:27:47.237269: step 492, loss 0.00832662, acc 1\n",
      "2019-01-26T23:27:47.662020: step 493, loss 0.000865472, acc 1\n",
      "2019-01-26T23:27:48.067927: step 494, loss 0.0177859, acc 1\n",
      "2019-01-26T23:27:48.468814: step 495, loss 0.0489957, acc 0.96875\n",
      "2019-01-26T23:27:48.875344: step 496, loss 0.0150424, acc 0.984375\n",
      "2019-01-26T23:27:49.284463: step 497, loss 0.000725122, acc 1\n",
      "2019-01-26T23:27:49.736614: step 498, loss 0.00961803, acc 1\n",
      "2019-01-26T23:27:50.137548: step 499, loss 0.0011561, acc 1\n",
      "2019-01-26T23:27:50.540727: step 500, loss 0.058151, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:27:51.543742: step 500, loss 0.0516012, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-500\n",
      "\n",
      "2019-01-26T23:27:52.025049: step 501, loss 0.067825, acc 0.984375\n",
      "2019-01-26T23:27:52.426456: step 502, loss 0.0125996, acc 1\n",
      "2019-01-26T23:27:52.833657: step 503, loss 0.0393055, acc 0.984375\n",
      "2019-01-26T23:27:53.243048: step 504, loss 0.00705294, acc 1\n",
      "2019-01-26T23:27:53.643182: step 505, loss 0.0202804, acc 0.984375\n",
      "2019-01-26T23:27:54.049578: step 506, loss 0.00722554, acc 1\n",
      "2019-01-26T23:27:54.453860: step 507, loss 0.139871, acc 0.984375\n",
      "2019-01-26T23:27:54.853154: step 508, loss 0.0135503, acc 1\n",
      "2019-01-26T23:27:55.257069: step 509, loss 0.000852527, acc 1\n",
      "2019-01-26T23:27:55.680513: step 510, loss 0.00351933, acc 1\n",
      "2019-01-26T23:27:56.091269: step 511, loss 0.00247127, acc 1\n",
      "2019-01-26T23:27:56.511909: step 512, loss 0.00406881, acc 1\n",
      "2019-01-26T23:27:56.931506: step 513, loss 0.00941957, acc 1\n",
      "2019-01-26T23:27:57.363452: step 514, loss 0.0483077, acc 0.96875\n",
      "2019-01-26T23:27:57.798867: step 515, loss 0.0200213, acc 1\n",
      "2019-01-26T23:27:58.225879: step 516, loss 0.00821946, acc 1\n",
      "2019-01-26T23:27:58.721230: step 517, loss 0.0089536, acc 1\n",
      "2019-01-26T23:27:59.153348: step 518, loss 0.00138101, acc 1\n",
      "2019-01-26T23:27:59.589116: step 519, loss 0.00488069, acc 1\n",
      "2019-01-26T23:28:00.029246: step 520, loss 0.0874077, acc 0.96875\n",
      "2019-01-26T23:28:00.455895: step 521, loss 0.0246171, acc 0.984375\n",
      "2019-01-26T23:28:00.905635: step 522, loss 0.0485666, acc 0.984375\n",
      "2019-01-26T23:28:01.342645: step 523, loss 0.00941096, acc 1\n",
      "2019-01-26T23:28:01.770323: step 524, loss 0.0404655, acc 0.984375\n",
      "2019-01-26T23:28:02.190581: step 525, loss 0.0121689, acc 1\n",
      "2019-01-26T23:28:02.620962: step 526, loss 0.0080582, acc 1\n",
      "2019-01-26T23:28:03.041768: step 527, loss 0.0985898, acc 0.953125\n",
      "2019-01-26T23:28:03.456822: step 528, loss 0.0235384, acc 1\n",
      "2019-01-26T23:28:03.883962: step 529, loss 0.00881612, acc 1\n",
      "2019-01-26T23:28:04.301524: step 530, loss 0.0146997, acc 1\n",
      "2019-01-26T23:28:04.714717: step 531, loss 0.173817, acc 0.921875\n",
      "2019-01-26T23:28:05.127491: step 532, loss 0.026243, acc 0.984375\n",
      "2019-01-26T23:28:05.543262: step 533, loss 0.0685832, acc 0.96875\n",
      "2019-01-26T23:28:05.948744: step 534, loss 0.0291516, acc 0.984375\n",
      "2019-01-26T23:28:06.357148: step 535, loss 0.00669359, acc 1\n",
      "2019-01-26T23:28:06.776266: step 536, loss 0.00113262, acc 1\n",
      "2019-01-26T23:28:07.178488: step 537, loss 0.177447, acc 0.953125\n",
      "2019-01-26T23:28:07.586751: step 538, loss 0.0499818, acc 0.96875\n",
      "2019-01-26T23:28:07.998513: step 539, loss 0.00236741, acc 1\n",
      "2019-01-26T23:28:08.402786: step 540, loss 0.00343888, acc 1\n",
      "2019-01-26T23:28:08.815483: step 541, loss 0.00744561, acc 1\n",
      "2019-01-26T23:28:09.221569: step 542, loss 0.00250087, acc 1\n",
      "2019-01-26T23:28:09.621755: step 543, loss 0.00910687, acc 1\n",
      "2019-01-26T23:28:10.022541: step 544, loss 0.0770853, acc 0.96875\n",
      "2019-01-26T23:28:10.427162: step 545, loss 0.0262727, acc 0.984375\n",
      "2019-01-26T23:28:10.833109: step 546, loss 0.0378997, acc 0.984375\n",
      "2019-01-26T23:28:11.239584: step 547, loss 0.135903, acc 0.984375\n",
      "2019-01-26T23:28:11.692499: step 548, loss 0.0243057, acc 0.984375\n",
      "2019-01-26T23:28:12.098564: step 549, loss 0.00467875, acc 1\n",
      "2019-01-26T23:28:12.506477: step 550, loss 0.0355788, acc 0.984375\n",
      "2019-01-26T23:28:12.918117: step 551, loss 0.0118919, acc 1\n",
      "2019-01-26T23:28:13.321959: step 552, loss 0.0142246, acc 1\n",
      "2019-01-26T23:28:13.490542: step 553, loss 0.0108887, acc 1\n",
      "2019-01-26T23:28:13.902331: step 554, loss 0.00556075, acc 1\n",
      "2019-01-26T23:28:14.303980: step 555, loss 0.00533193, acc 1\n",
      "2019-01-26T23:28:14.713079: step 556, loss 0.000906681, acc 1\n",
      "2019-01-26T23:28:15.119749: step 557, loss 0.0263895, acc 1\n",
      "2019-01-26T23:28:15.519404: step 558, loss 0.0134507, acc 1\n",
      "2019-01-26T23:28:15.940932: step 559, loss 0.0502742, acc 0.984375\n",
      "2019-01-26T23:28:16.359918: step 560, loss 0.0365237, acc 0.984375\n",
      "2019-01-26T23:28:16.781985: step 561, loss 0.0102832, acc 1\n",
      "2019-01-26T23:28:17.202801: step 562, loss 0.0375288, acc 0.984375\n",
      "2019-01-26T23:28:17.636078: step 563, loss 0.0450619, acc 0.984375\n",
      "2019-01-26T23:28:18.068060: step 564, loss 0.0035755, acc 1\n",
      "2019-01-26T23:28:18.507608: step 565, loss 0.0164609, acc 1\n",
      "2019-01-26T23:28:18.943391: step 566, loss 0.00119017, acc 1\n",
      "2019-01-26T23:28:19.381206: step 567, loss 0.000185304, acc 1\n",
      "2019-01-26T23:28:19.810835: step 568, loss 0.00513546, acc 1\n",
      "2019-01-26T23:28:20.242000: step 569, loss 0.0168833, acc 0.984375\n",
      "2019-01-26T23:28:20.680251: step 570, loss 0.0035104, acc 1\n",
      "2019-01-26T23:28:21.102398: step 571, loss 0.160818, acc 0.953125\n",
      "2019-01-26T23:28:21.528130: step 572, loss 0.0179377, acc 1\n",
      "2019-01-26T23:28:21.957756: step 573, loss 0.0751211, acc 0.984375\n",
      "2019-01-26T23:28:22.380467: step 574, loss 0.0802811, acc 0.96875\n",
      "2019-01-26T23:28:22.803038: step 575, loss 0.0972241, acc 0.984375\n",
      "2019-01-26T23:28:23.223693: step 576, loss 0.00269186, acc 1\n",
      "2019-01-26T23:28:23.643026: step 577, loss 0.00327506, acc 1\n",
      "2019-01-26T23:28:24.064730: step 578, loss 0.00107092, acc 1\n",
      "2019-01-26T23:28:24.485634: step 579, loss 0.00539215, acc 1\n",
      "2019-01-26T23:28:24.940794: step 580, loss 0.00423192, acc 1\n",
      "2019-01-26T23:28:25.353884: step 581, loss 0.000921234, acc 1\n",
      "2019-01-26T23:28:25.760027: step 582, loss 0.00945012, acc 1\n",
      "2019-01-26T23:28:26.166565: step 583, loss 0.00180509, acc 1\n",
      "2019-01-26T23:28:26.570359: step 584, loss 0.000616051, acc 1\n",
      "2019-01-26T23:28:27.002983: step 585, loss 0.0116784, acc 1\n",
      "2019-01-26T23:28:27.413024: step 586, loss 0.104658, acc 0.984375\n",
      "2019-01-26T23:28:27.835772: step 587, loss 0.00235507, acc 1\n",
      "2019-01-26T23:28:28.254082: step 588, loss 0.000850068, acc 1\n",
      "2019-01-26T23:28:28.671639: step 589, loss 0.0154693, acc 1\n",
      "2019-01-26T23:28:29.091996: step 590, loss 0.0240125, acc 0.984375\n",
      "2019-01-26T23:28:29.507350: step 591, loss 0.0423969, acc 0.984375\n",
      "2019-01-26T23:28:29.932578: step 592, loss 0.00884822, acc 1\n",
      "2019-01-26T23:28:30.349490: step 593, loss 0.0268896, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:28:30.775986: step 594, loss 0.0806951, acc 0.984375\n",
      "2019-01-26T23:28:31.197341: step 595, loss 0.00871485, acc 1\n",
      "2019-01-26T23:28:31.626062: step 596, loss 0.0280242, acc 0.984375\n",
      "2019-01-26T23:28:32.048504: step 597, loss 0.00198365, acc 1\n",
      "2019-01-26T23:28:32.486245: step 598, loss 0.00346301, acc 1\n",
      "2019-01-26T23:28:32.932626: step 599, loss 0.00615564, acc 1\n",
      "2019-01-26T23:28:33.372607: step 600, loss 0.0169848, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:28:34.531062: step 600, loss 0.052376, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-600\n",
      "\n",
      "2019-01-26T23:28:35.057822: step 601, loss 0.00412048, acc 1\n",
      "2019-01-26T23:28:35.500516: step 602, loss 0.0220837, acc 0.984375\n",
      "2019-01-26T23:28:35.950316: step 603, loss 0.0138881, acc 1\n",
      "2019-01-26T23:28:36.390449: step 604, loss 0.00350131, acc 1\n",
      "2019-01-26T23:28:36.838190: step 605, loss 0.00761912, acc 1\n",
      "2019-01-26T23:28:37.280092: step 606, loss 0.0131383, acc 1\n",
      "2019-01-26T23:28:37.728537: step 607, loss 0.0126753, acc 1\n",
      "2019-01-26T23:28:38.169182: step 608, loss 0.00221429, acc 1\n",
      "2019-01-26T23:28:38.622392: step 609, loss 0.0543156, acc 0.984375\n",
      "2019-01-26T23:28:39.069362: step 610, loss 0.0139534, acc 1\n",
      "2019-01-26T23:28:39.512505: step 611, loss 0.00118158, acc 1\n",
      "2019-01-26T23:28:39.958239: step 612, loss 0.0594831, acc 0.984375\n",
      "2019-01-26T23:28:40.392102: step 613, loss 0.0142958, acc 1\n",
      "2019-01-26T23:28:40.824559: step 614, loss 0.00995022, acc 1\n",
      "2019-01-26T23:28:41.249209: step 615, loss 0.0140055, acc 1\n",
      "2019-01-26T23:28:41.675282: step 616, loss 0.100945, acc 0.984375\n",
      "2019-01-26T23:28:42.102598: step 617, loss 0.00331729, acc 1\n",
      "2019-01-26T23:28:42.532325: step 618, loss 0.000385, acc 1\n",
      "2019-01-26T23:28:42.966214: step 619, loss 0.00339959, acc 1\n",
      "2019-01-26T23:28:43.395085: step 620, loss 0.00498615, acc 1\n",
      "2019-01-26T23:28:43.811956: step 621, loss 0.0115985, acc 1\n",
      "2019-01-26T23:28:44.231170: step 622, loss 0.000260899, acc 1\n",
      "2019-01-26T23:28:44.667113: step 623, loss 0.00891884, acc 1\n",
      "2019-01-26T23:28:45.090306: step 624, loss 0.00602609, acc 1\n",
      "2019-01-26T23:28:45.504762: step 625, loss 0.00381719, acc 1\n",
      "2019-01-26T23:28:45.918348: step 626, loss 0.00349065, acc 1\n",
      "2019-01-26T23:28:46.326903: step 627, loss 0.00131143, acc 1\n",
      "2019-01-26T23:28:46.727985: step 628, loss 0.00490368, acc 1\n",
      "2019-01-26T23:28:47.135572: step 629, loss 0.0238279, acc 0.984375\n",
      "2019-01-26T23:28:47.551355: step 630, loss 0.00257451, acc 1\n",
      "2019-01-26T23:28:47.967370: step 631, loss 0.0559304, acc 0.96875\n",
      "2019-01-26T23:28:48.149673: step 632, loss 0.000300445, acc 1\n",
      "2019-01-26T23:28:48.564017: step 633, loss 0.0432408, acc 0.953125\n",
      "2019-01-26T23:28:48.982379: step 634, loss 0.024853, acc 0.984375\n",
      "2019-01-26T23:28:49.392770: step 635, loss 0.0111118, acc 1\n",
      "2019-01-26T23:28:49.819672: step 636, loss 0.0547116, acc 0.984375\n",
      "2019-01-26T23:28:50.235087: step 637, loss 0.0101024, acc 1\n",
      "2019-01-26T23:28:50.650602: step 638, loss 0.0159904, acc 0.984375\n",
      "2019-01-26T23:28:51.069280: step 639, loss 0.0376477, acc 0.984375\n",
      "2019-01-26T23:28:51.494082: step 640, loss 0.0452023, acc 0.984375\n",
      "2019-01-26T23:28:51.932715: step 641, loss 0.00927804, acc 1\n",
      "2019-01-26T23:28:52.371270: step 642, loss 0.047765, acc 0.984375\n",
      "2019-01-26T23:28:52.820842: step 643, loss 0.0470863, acc 0.984375\n",
      "2019-01-26T23:28:53.262476: step 644, loss 0.00271578, acc 1\n",
      "2019-01-26T23:28:53.715380: step 645, loss 0.0127337, acc 1\n",
      "2019-01-26T23:28:54.158107: step 646, loss 0.00472692, acc 1\n",
      "2019-01-26T23:28:54.608728: step 647, loss 0.0366221, acc 0.984375\n",
      "2019-01-26T23:28:55.057125: step 648, loss 0.00106594, acc 1\n",
      "2019-01-26T23:28:55.503692: step 649, loss 0.00265475, acc 1\n",
      "2019-01-26T23:28:55.947531: step 650, loss 0.000885729, acc 1\n",
      "2019-01-26T23:28:56.386814: step 651, loss 0.120362, acc 0.984375\n",
      "2019-01-26T23:28:56.835491: step 652, loss 0.0381646, acc 0.984375\n",
      "2019-01-26T23:28:57.280544: step 653, loss 0.0101002, acc 1\n",
      "2019-01-26T23:28:57.730101: step 654, loss 0.00680096, acc 1\n",
      "2019-01-26T23:28:58.174142: step 655, loss 0.00706407, acc 1\n",
      "2019-01-26T23:28:58.683050: step 656, loss 0.00123538, acc 1\n",
      "2019-01-26T23:28:59.148737: step 657, loss 0.00144025, acc 1\n",
      "2019-01-26T23:28:59.594907: step 658, loss 0.0211204, acc 0.984375\n",
      "2019-01-26T23:29:00.034955: step 659, loss 0.0313285, acc 0.984375\n",
      "2019-01-26T23:29:00.470312: step 660, loss 0.00423837, acc 1\n",
      "2019-01-26T23:29:00.910474: step 661, loss 0.0885002, acc 0.96875\n",
      "2019-01-26T23:29:01.340345: step 662, loss 0.00754194, acc 1\n",
      "2019-01-26T23:29:01.780104: step 663, loss 0.00114919, acc 1\n",
      "2019-01-26T23:29:02.216900: step 664, loss 0.00807083, acc 1\n",
      "2019-01-26T23:29:02.648853: step 665, loss 0.0193372, acc 1\n",
      "2019-01-26T23:29:03.077615: step 666, loss 0.016484, acc 1\n",
      "2019-01-26T23:29:03.498872: step 667, loss 0.00957737, acc 1\n",
      "2019-01-26T23:29:03.961683: step 668, loss 0.0402834, acc 0.984375\n",
      "2019-01-26T23:29:04.375631: step 669, loss 0.0680155, acc 0.96875\n",
      "2019-01-26T23:29:04.788080: step 670, loss 0.00160726, acc 1\n",
      "2019-01-26T23:29:05.206747: step 671, loss 0.00427608, acc 1\n",
      "2019-01-26T23:29:05.625362: step 672, loss 0.00181841, acc 1\n",
      "2019-01-26T23:29:06.045849: step 673, loss 0.00063254, acc 1\n",
      "2019-01-26T23:29:06.451536: step 674, loss 0.00101404, acc 1\n",
      "2019-01-26T23:29:06.859784: step 675, loss 0.0952106, acc 0.984375\n",
      "2019-01-26T23:29:07.262812: step 676, loss 0.0368779, acc 0.984375\n",
      "2019-01-26T23:29:07.670679: step 677, loss 0.0494507, acc 0.984375\n",
      "2019-01-26T23:29:08.081588: step 678, loss 0.0105979, acc 1\n",
      "2019-01-26T23:29:08.491308: step 679, loss 0.0101435, acc 1\n",
      "2019-01-26T23:29:08.897179: step 680, loss 0.00737043, acc 1\n",
      "2019-01-26T23:29:09.300035: step 681, loss 0.0181427, acc 0.984375\n",
      "2019-01-26T23:29:09.709193: step 682, loss 0.00462586, acc 1\n",
      "2019-01-26T23:29:10.115196: step 683, loss 0.0428388, acc 0.984375\n",
      "2019-01-26T23:29:10.512772: step 684, loss 0.00740416, acc 1\n",
      "2019-01-26T23:29:10.910125: step 685, loss 0.0267968, acc 0.984375\n",
      "2019-01-26T23:29:11.324894: step 686, loss 0.0567133, acc 0.984375\n",
      "2019-01-26T23:29:11.739474: step 687, loss 0.00111747, acc 1\n",
      "2019-01-26T23:29:12.142274: step 688, loss 0.00156611, acc 1\n",
      "2019-01-26T23:29:12.546125: step 689, loss 0.000358571, acc 1\n",
      "2019-01-26T23:29:12.955742: step 690, loss 0.0349581, acc 0.984375\n",
      "2019-01-26T23:29:13.358604: step 691, loss 0.0118953, acc 1\n",
      "2019-01-26T23:29:13.766914: step 692, loss 0.0108161, acc 1\n",
      "2019-01-26T23:29:14.182044: step 693, loss 0.022765, acc 0.984375\n",
      "2019-01-26T23:29:14.597431: step 694, loss 0.0044352, acc 1\n",
      "2019-01-26T23:29:15.000286: step 695, loss 0.0715963, acc 0.984375\n",
      "2019-01-26T23:29:15.401820: step 696, loss 0.0148824, acc 1\n",
      "2019-01-26T23:29:15.822281: step 697, loss 0.0235864, acc 0.984375\n",
      "2019-01-26T23:29:16.236920: step 698, loss 0.0358176, acc 0.984375\n",
      "2019-01-26T23:29:16.654924: step 699, loss 0.0100481, acc 1\n",
      "2019-01-26T23:29:17.077516: step 700, loss 0.0507079, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:29:18.172417: step 700, loss 0.0513741, acc 0.989228\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-700\n",
      "\n",
      "2019-01-26T23:29:18.695756: step 701, loss 0.0172489, acc 0.984375\n",
      "2019-01-26T23:29:19.128945: step 702, loss 0.00666146, acc 1\n",
      "2019-01-26T23:29:19.565306: step 703, loss 0.00575129, acc 1\n",
      "2019-01-26T23:29:20.002186: step 704, loss 0.0327579, acc 0.984375\n",
      "2019-01-26T23:29:20.438844: step 705, loss 0.0152106, acc 1\n",
      "2019-01-26T23:29:20.871277: step 706, loss 0.00520472, acc 1\n",
      "2019-01-26T23:29:21.305705: step 707, loss 0.00155374, acc 1\n",
      "2019-01-26T23:29:21.730213: step 708, loss 0.0114003, acc 1\n",
      "2019-01-26T23:29:22.157399: step 709, loss 0.0299124, acc 0.984375\n",
      "2019-01-26T23:29:22.584592: step 710, loss 0.0059451, acc 1\n",
      "2019-01-26T23:29:22.769065: step 711, loss 0.000590942, acc 1\n",
      "2019-01-26T23:29:23.198348: step 712, loss 0.00172601, acc 1\n",
      "2019-01-26T23:29:23.625256: step 713, loss 0.000211373, acc 1\n",
      "2019-01-26T23:29:24.059908: step 714, loss 0.0229492, acc 0.984375\n",
      "2019-01-26T23:29:24.493799: step 715, loss 0.00638155, acc 1\n",
      "2019-01-26T23:29:24.939553: step 716, loss 0.00106068, acc 1\n",
      "2019-01-26T23:29:25.369188: step 717, loss 0.00292858, acc 1\n",
      "2019-01-26T23:29:25.809606: step 718, loss 0.00911512, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:29:26.251201: step 719, loss 0.0112713, acc 1\n",
      "2019-01-26T23:29:26.698404: step 720, loss 0.0295664, acc 0.984375\n",
      "2019-01-26T23:29:27.137881: step 721, loss 0.0101924, acc 1\n",
      "2019-01-26T23:29:27.566352: step 722, loss 0.001867, acc 1\n",
      "2019-01-26T23:29:27.993462: step 723, loss 0.00875198, acc 1\n",
      "2019-01-26T23:29:28.418678: step 724, loss 0.000907047, acc 1\n",
      "2019-01-26T23:29:28.851723: step 725, loss 0.0369558, acc 0.984375\n",
      "2019-01-26T23:29:29.274002: step 726, loss 0.0130411, acc 1\n",
      "2019-01-26T23:29:29.704377: step 727, loss 0.0182374, acc 1\n",
      "2019-01-26T23:29:30.134510: step 728, loss 0.00331346, acc 1\n",
      "2019-01-26T23:29:30.568491: step 729, loss 0.0160227, acc 1\n",
      "2019-01-26T23:29:30.996343: step 730, loss 0.0146456, acc 1\n",
      "2019-01-26T23:29:31.419952: step 731, loss 0.0006317, acc 1\n",
      "2019-01-26T23:29:31.838778: step 732, loss 0.0128604, acc 0.984375\n",
      "2019-01-26T23:29:32.250427: step 733, loss 0.0620555, acc 0.984375\n",
      "2019-01-26T23:29:32.665804: step 734, loss 0.00851527, acc 1\n",
      "2019-01-26T23:29:33.080855: step 735, loss 0.000730238, acc 1\n",
      "2019-01-26T23:29:33.498093: step 736, loss 0.00343652, acc 1\n",
      "2019-01-26T23:29:33.918886: step 737, loss 0.0246229, acc 0.984375\n",
      "2019-01-26T23:29:34.333173: step 738, loss 0.0292706, acc 0.984375\n",
      "2019-01-26T23:29:34.737227: step 739, loss 0.000679209, acc 1\n",
      "2019-01-26T23:29:35.138177: step 740, loss 0.0256496, acc 0.984375\n",
      "2019-01-26T23:29:35.536552: step 741, loss 0.0134189, acc 0.984375\n",
      "2019-01-26T23:29:35.941971: step 742, loss 0.00478566, acc 1\n",
      "2019-01-26T23:29:36.349467: step 743, loss 0.103868, acc 0.96875\n",
      "2019-01-26T23:29:36.751535: step 744, loss 0.0286629, acc 0.984375\n",
      "2019-01-26T23:29:37.155382: step 745, loss 0.0439624, acc 0.984375\n",
      "2019-01-26T23:29:37.556062: step 746, loss 0.00442777, acc 1\n",
      "2019-01-26T23:29:37.957661: step 747, loss 0.000438566, acc 1\n",
      "2019-01-26T23:29:38.356070: step 748, loss 0.00916121, acc 1\n",
      "2019-01-26T23:29:38.753453: step 749, loss 0.00116446, acc 1\n",
      "2019-01-26T23:29:39.154790: step 750, loss 0.00169733, acc 1\n",
      "2019-01-26T23:29:39.551193: step 751, loss 0.00514223, acc 1\n",
      "2019-01-26T23:29:39.952088: step 752, loss 0.01158, acc 1\n",
      "2019-01-26T23:29:40.341107: step 753, loss 0.000483134, acc 1\n",
      "2019-01-26T23:29:40.740798: step 754, loss 0.0295897, acc 0.984375\n",
      "2019-01-26T23:29:41.141601: step 755, loss 0.00205423, acc 1\n",
      "2019-01-26T23:29:41.530210: step 756, loss 0.00737283, acc 1\n",
      "2019-01-26T23:29:41.929110: step 757, loss 0.0122451, acc 1\n",
      "2019-01-26T23:29:42.322928: step 758, loss 0.00665661, acc 1\n",
      "2019-01-26T23:29:42.720400: step 759, loss 0.0056725, acc 1\n",
      "2019-01-26T23:29:43.117408: step 760, loss 0.000699317, acc 1\n",
      "2019-01-26T23:29:43.513998: step 761, loss 0.0207651, acc 1\n",
      "2019-01-26T23:29:43.916036: step 762, loss 0.0288407, acc 0.984375\n",
      "2019-01-26T23:29:44.314417: step 763, loss 0.0626223, acc 0.984375\n",
      "2019-01-26T23:29:44.713425: step 764, loss 0.0231954, acc 0.984375\n",
      "2019-01-26T23:29:45.110370: step 765, loss 0.0401777, acc 0.984375\n",
      "2019-01-26T23:29:45.504857: step 766, loss 0.0786611, acc 0.984375\n",
      "2019-01-26T23:29:45.903552: step 767, loss 0.00476248, acc 1\n",
      "2019-01-26T23:29:46.294148: step 768, loss 0.022528, acc 0.984375\n",
      "2019-01-26T23:29:46.689403: step 769, loss 0.000417442, acc 1\n",
      "2019-01-26T23:29:47.089913: step 770, loss 0.0462295, acc 0.984375\n",
      "2019-01-26T23:29:47.493346: step 771, loss 0.00124646, acc 1\n",
      "2019-01-26T23:29:47.900059: step 772, loss 0.00180826, acc 1\n",
      "2019-01-26T23:29:48.297703: step 773, loss 0.00322973, acc 1\n",
      "2019-01-26T23:29:48.713034: step 774, loss 0.0128786, acc 1\n",
      "2019-01-26T23:29:49.124510: step 775, loss 0.124386, acc 0.984375\n",
      "2019-01-26T23:29:49.578157: step 776, loss 0.0153038, acc 1\n",
      "2019-01-26T23:29:50.008022: step 777, loss 0.00698139, acc 1\n",
      "2019-01-26T23:29:50.425456: step 778, loss 0.0160056, acc 0.984375\n",
      "2019-01-26T23:29:50.846051: step 779, loss 0.00353271, acc 1\n",
      "2019-01-26T23:29:51.261846: step 780, loss 0.0030968, acc 1\n",
      "2019-01-26T23:29:51.753075: step 781, loss 0.00168706, acc 1\n",
      "2019-01-26T23:29:52.183311: step 782, loss 0.00111124, acc 1\n",
      "2019-01-26T23:29:52.624709: step 783, loss 0.000484681, acc 1\n",
      "2019-01-26T23:29:53.072165: step 784, loss 0.00792775, acc 1\n",
      "2019-01-26T23:29:53.528492: step 785, loss 0.0146719, acc 1\n",
      "2019-01-26T23:29:53.974419: step 786, loss 0.00223803, acc 1\n",
      "2019-01-26T23:29:54.426213: step 787, loss 0.0116421, acc 1\n",
      "2019-01-26T23:29:54.882331: step 788, loss 0.00551151, acc 1\n",
      "2019-01-26T23:29:55.338914: step 789, loss 0.0215203, acc 0.984375\n",
      "2019-01-26T23:29:55.530296: step 790, loss 0.00357897, acc 1\n",
      "2019-01-26T23:29:55.983840: step 791, loss 0.00169997, acc 1\n",
      "2019-01-26T23:29:56.428540: step 792, loss 8.10497e-05, acc 1\n",
      "2019-01-26T23:29:56.878598: step 793, loss 0.000739845, acc 1\n",
      "2019-01-26T23:29:57.327983: step 794, loss 0.000438076, acc 1\n",
      "2019-01-26T23:29:57.818597: step 795, loss 0.0145461, acc 1\n",
      "2019-01-26T23:29:58.304909: step 796, loss 0.00451088, acc 1\n",
      "2019-01-26T23:29:58.758436: step 797, loss 0.00443284, acc 1\n",
      "2019-01-26T23:29:59.219277: step 798, loss 0.0644914, acc 0.984375\n",
      "2019-01-26T23:29:59.663769: step 799, loss 0.0120372, acc 0.984375\n",
      "2019-01-26T23:30:00.105831: step 800, loss 0.0305989, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:30:01.222603: step 800, loss 0.0596458, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-800\n",
      "\n",
      "2019-01-26T23:30:01.736804: step 801, loss 0.0117273, acc 1\n",
      "2019-01-26T23:30:02.170108: step 802, loss 0.00494091, acc 1\n",
      "2019-01-26T23:30:02.602359: step 803, loss 0.00284844, acc 1\n",
      "2019-01-26T23:30:03.036634: step 804, loss 0.0013132, acc 1\n",
      "2019-01-26T23:30:03.470375: step 805, loss 0.00497737, acc 1\n",
      "2019-01-26T23:30:03.903004: step 806, loss 0.0255623, acc 0.984375\n",
      "2019-01-26T23:30:04.330146: step 807, loss 0.00299994, acc 1\n",
      "2019-01-26T23:30:04.759545: step 808, loss 0.00707724, acc 1\n",
      "2019-01-26T23:30:05.203362: step 809, loss 0.00198596, acc 1\n",
      "2019-01-26T23:30:05.654075: step 810, loss 0.00229814, acc 1\n",
      "2019-01-26T23:30:06.104449: step 811, loss 0.00183987, acc 1\n",
      "2019-01-26T23:30:06.550156: step 812, loss 0.00330099, acc 1\n",
      "2019-01-26T23:30:06.990723: step 813, loss 0.00297072, acc 1\n",
      "2019-01-26T23:30:07.420841: step 814, loss 0.00958626, acc 1\n",
      "2019-01-26T23:30:07.856155: step 815, loss 0.150949, acc 0.984375\n",
      "2019-01-26T23:30:08.292351: step 816, loss 0.00453796, acc 1\n",
      "2019-01-26T23:30:08.729590: step 817, loss 0.000841814, acc 1\n",
      "2019-01-26T23:30:09.160638: step 818, loss 0.0169122, acc 0.984375\n",
      "2019-01-26T23:30:09.592314: step 819, loss 0.0343282, acc 0.984375\n",
      "2019-01-26T23:30:10.022936: step 820, loss 0.0037764, acc 1\n",
      "2019-01-26T23:30:10.452053: step 821, loss 0.0154583, acc 1\n",
      "2019-01-26T23:30:10.882781: step 822, loss 0.0308457, acc 0.984375\n",
      "2019-01-26T23:30:11.317702: step 823, loss 0.086313, acc 0.96875\n",
      "2019-01-26T23:30:11.752744: step 824, loss 0.00388697, acc 1\n",
      "2019-01-26T23:30:12.188362: step 825, loss 0.00426226, acc 1\n",
      "2019-01-26T23:30:12.649317: step 826, loss 0.00742279, acc 1\n",
      "2019-01-26T23:30:13.079794: step 827, loss 0.000643912, acc 1\n",
      "2019-01-26T23:30:13.529407: step 828, loss 0.000922301, acc 1\n",
      "2019-01-26T23:30:13.974167: step 829, loss 0.000782538, acc 1\n",
      "2019-01-26T23:30:14.411581: step 830, loss 0.00389119, acc 1\n",
      "2019-01-26T23:30:14.864533: step 831, loss 0.00106498, acc 1\n",
      "2019-01-26T23:30:15.295456: step 832, loss 0.0224272, acc 0.984375\n",
      "2019-01-26T23:30:15.726010: step 833, loss 0.000176436, acc 1\n",
      "2019-01-26T23:30:16.160578: step 834, loss 0.000191246, acc 1\n",
      "2019-01-26T23:30:16.590561: step 835, loss 0.000517385, acc 1\n",
      "2019-01-26T23:30:17.019705: step 836, loss 0.00174498, acc 1\n",
      "2019-01-26T23:30:17.455478: step 837, loss 0.014157, acc 0.984375\n",
      "2019-01-26T23:30:17.891609: step 838, loss 0.0174567, acc 1\n",
      "2019-01-26T23:30:18.322939: step 839, loss 0.00244085, acc 1\n",
      "2019-01-26T23:30:18.758652: step 840, loss 0.0321355, acc 0.984375\n",
      "2019-01-26T23:30:19.192832: step 841, loss 0.000444075, acc 1\n",
      "2019-01-26T23:30:19.611944: step 842, loss 0.000620073, acc 1\n",
      "2019-01-26T23:30:20.029150: step 843, loss 0.00986734, acc 1\n",
      "2019-01-26T23:30:20.445897: step 844, loss 0.00551815, acc 1\n",
      "2019-01-26T23:30:20.866033: step 845, loss 0.00640866, acc 1\n",
      "2019-01-26T23:30:21.280109: step 846, loss 0.00147667, acc 1\n",
      "2019-01-26T23:30:21.699698: step 847, loss 0.00276045, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:30:22.117449: step 848, loss 0.000179255, acc 1\n",
      "2019-01-26T23:30:22.527370: step 849, loss 0.00273674, acc 1\n",
      "2019-01-26T23:30:22.941341: step 850, loss 0.00178996, acc 1\n",
      "2019-01-26T23:30:23.349442: step 851, loss 0.000348957, acc 1\n",
      "2019-01-26T23:30:23.769747: step 852, loss 0.000978712, acc 1\n",
      "2019-01-26T23:30:24.180970: step 853, loss 0.00159914, acc 1\n",
      "2019-01-26T23:30:24.605191: step 854, loss 0.0254401, acc 0.984375\n",
      "2019-01-26T23:30:25.039931: step 855, loss 0.0014381, acc 1\n",
      "2019-01-26T23:30:25.453239: step 856, loss 0.000406233, acc 1\n",
      "2019-01-26T23:30:25.869786: step 857, loss 0.00860088, acc 1\n",
      "2019-01-26T23:30:26.282311: step 858, loss 5.62362e-05, acc 1\n",
      "2019-01-26T23:30:26.701184: step 859, loss 0.0102461, acc 1\n",
      "2019-01-26T23:30:27.126077: step 860, loss 0.00100543, acc 1\n",
      "2019-01-26T23:30:27.554955: step 861, loss 0.00103154, acc 1\n",
      "2019-01-26T23:30:27.978309: step 862, loss 0.00627989, acc 1\n",
      "2019-01-26T23:30:28.409982: step 863, loss 0.00379285, acc 1\n",
      "2019-01-26T23:30:28.850412: step 864, loss 0.000609146, acc 1\n",
      "2019-01-26T23:30:29.289102: step 865, loss 0.0243495, acc 0.984375\n",
      "2019-01-26T23:30:29.730393: step 866, loss 0.00129838, acc 1\n",
      "2019-01-26T23:30:30.182750: step 867, loss 0.115656, acc 0.96875\n",
      "2019-01-26T23:30:30.634935: step 868, loss 0.000452886, acc 1\n",
      "2019-01-26T23:30:30.837665: step 869, loss 0.0052761, acc 1\n",
      "2019-01-26T23:30:31.289272: step 870, loss 0.00177771, acc 1\n",
      "2019-01-26T23:30:31.729711: step 871, loss 0.00112978, acc 1\n",
      "2019-01-26T23:30:32.165644: step 872, loss 0.0113846, acc 1\n",
      "2019-01-26T23:30:32.614224: step 873, loss 0.0825817, acc 0.984375\n",
      "2019-01-26T23:30:33.053610: step 874, loss 0.0180143, acc 1\n",
      "2019-01-26T23:30:33.495351: step 875, loss 0.00739279, acc 1\n",
      "2019-01-26T23:30:33.946065: step 876, loss 0.00860508, acc 1\n",
      "2019-01-26T23:30:34.389462: step 877, loss 0.0137634, acc 1\n",
      "2019-01-26T23:30:34.834539: step 878, loss 0.0263337, acc 0.984375\n",
      "2019-01-26T23:30:35.278531: step 879, loss 0.0107175, acc 1\n",
      "2019-01-26T23:30:35.726784: step 880, loss 0.00293791, acc 1\n",
      "2019-01-26T23:30:36.172212: step 881, loss 0.00129703, acc 1\n",
      "2019-01-26T23:30:36.608478: step 882, loss 0.0240484, acc 0.984375\n",
      "2019-01-26T23:30:37.034553: step 883, loss 0.023101, acc 0.984375\n",
      "2019-01-26T23:30:37.457972: step 884, loss 0.000930149, acc 1\n",
      "2019-01-26T23:30:37.883751: step 885, loss 0.00103084, acc 1\n",
      "2019-01-26T23:30:38.314485: step 886, loss 0.000611545, acc 1\n",
      "2019-01-26T23:30:38.746532: step 887, loss 0.00200121, acc 1\n",
      "2019-01-26T23:30:39.185281: step 888, loss 0.000938939, acc 1\n",
      "2019-01-26T23:30:39.612516: step 889, loss 0.000710827, acc 1\n",
      "2019-01-26T23:30:40.047939: step 890, loss 0.00424012, acc 1\n",
      "2019-01-26T23:30:40.476886: step 891, loss 0.000795935, acc 1\n",
      "2019-01-26T23:30:40.924105: step 892, loss 0.000198128, acc 1\n",
      "2019-01-26T23:30:41.369429: step 893, loss 0.00458032, acc 1\n",
      "2019-01-26T23:30:41.812612: step 894, loss 0.000272599, acc 1\n",
      "2019-01-26T23:30:42.261499: step 895, loss 0.084733, acc 0.984375\n",
      "2019-01-26T23:30:42.701599: step 896, loss 0.0191884, acc 0.984375\n",
      "2019-01-26T23:30:43.133947: step 897, loss 0.00130059, acc 1\n",
      "2019-01-26T23:30:43.572154: step 898, loss 0.00106959, acc 1\n",
      "2019-01-26T23:30:44.004241: step 899, loss 0.0534399, acc 0.984375\n",
      "2019-01-26T23:30:44.437330: step 900, loss 0.0112268, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:30:45.560699: step 900, loss 0.0626094, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-900\n",
      "\n",
      "2019-01-26T23:30:46.073829: step 901, loss 0.0515345, acc 0.984375\n",
      "2019-01-26T23:30:46.502050: step 902, loss 0.00331801, acc 1\n",
      "2019-01-26T23:30:46.938369: step 903, loss 0.000479501, acc 1\n",
      "2019-01-26T23:30:47.358345: step 904, loss 0.00322355, acc 1\n",
      "2019-01-26T23:30:47.777886: step 905, loss 0.00711803, acc 1\n",
      "2019-01-26T23:30:48.196763: step 906, loss 0.0108562, acc 1\n",
      "2019-01-26T23:30:48.620058: step 907, loss 0.00442447, acc 1\n",
      "2019-01-26T23:30:49.037284: step 908, loss 0.013992, acc 1\n",
      "2019-01-26T23:30:49.452356: step 909, loss 0.000618906, acc 1\n",
      "2019-01-26T23:30:49.871710: step 910, loss 0.0106785, acc 1\n",
      "2019-01-26T23:30:50.279992: step 911, loss 0.0170249, acc 1\n",
      "2019-01-26T23:30:50.687297: step 912, loss 0.00458619, acc 1\n",
      "2019-01-26T23:30:51.092168: step 913, loss 0.00238405, acc 1\n",
      "2019-01-26T23:30:51.500997: step 914, loss 0.0165082, acc 0.984375\n",
      "2019-01-26T23:30:51.928712: step 915, loss 0.0411025, acc 0.984375\n",
      "2019-01-26T23:30:52.343265: step 916, loss 0.00143839, acc 1\n",
      "2019-01-26T23:30:52.768730: step 917, loss 0.00710245, acc 1\n",
      "2019-01-26T23:30:53.183459: step 918, loss 0.0146065, acc 0.984375\n",
      "2019-01-26T23:30:53.599992: step 919, loss 0.0318653, acc 0.984375\n",
      "2019-01-26T23:30:54.008858: step 920, loss 0.00119066, acc 1\n",
      "2019-01-26T23:30:54.424602: step 921, loss 0.0485546, acc 0.984375\n",
      "2019-01-26T23:30:54.844612: step 922, loss 0.0154203, acc 0.984375\n",
      "2019-01-26T23:30:55.268308: step 923, loss 0.0032269, acc 1\n",
      "2019-01-26T23:30:55.683738: step 924, loss 0.00165814, acc 1\n",
      "2019-01-26T23:30:56.096916: step 925, loss 0.00232272, acc 1\n",
      "2019-01-26T23:30:56.511151: step 926, loss 0.0124262, acc 1\n",
      "2019-01-26T23:30:56.923997: step 927, loss 0.00178664, acc 1\n",
      "2019-01-26T23:30:57.334354: step 928, loss 0.00628059, acc 1\n",
      "2019-01-26T23:30:57.740126: step 929, loss 0.0744252, acc 0.984375\n",
      "2019-01-26T23:30:58.157517: step 930, loss 0.00201507, acc 1\n",
      "2019-01-26T23:30:58.568123: step 931, loss 0.00144795, acc 1\n",
      "2019-01-26T23:30:58.978392: step 932, loss 0.014345, acc 1\n",
      "2019-01-26T23:30:59.398355: step 933, loss 0.000745262, acc 1\n",
      "2019-01-26T23:30:59.813805: step 934, loss 0.00588056, acc 1\n",
      "2019-01-26T23:31:00.232106: step 935, loss 0.000174504, acc 1\n",
      "2019-01-26T23:31:00.652371: step 936, loss 0.00422473, acc 1\n",
      "2019-01-26T23:31:01.071714: step 937, loss 0.0476533, acc 0.984375\n",
      "2019-01-26T23:31:01.480419: step 938, loss 0.00739937, acc 1\n",
      "2019-01-26T23:31:01.898551: step 939, loss 0.00272763, acc 1\n",
      "2019-01-26T23:31:02.312952: step 940, loss 0.00160944, acc 1\n",
      "2019-01-26T23:31:02.721863: step 941, loss 0.0107755, acc 1\n",
      "2019-01-26T23:31:03.144448: step 942, loss 0.0205827, acc 0.984375\n",
      "2019-01-26T23:31:03.580210: step 943, loss 0.000586236, acc 1\n",
      "2019-01-26T23:31:04.011910: step 944, loss 0.0120534, acc 1\n",
      "2019-01-26T23:31:04.450763: step 945, loss 0.000692875, acc 1\n",
      "2019-01-26T23:31:04.901781: step 946, loss 0.0273844, acc 0.96875\n",
      "2019-01-26T23:31:05.345211: step 947, loss 0.118525, acc 0.953125\n",
      "2019-01-26T23:31:05.540519: step 948, loss 0.000107263, acc 1\n",
      "2019-01-26T23:31:05.991448: step 949, loss 0.000454909, acc 1\n",
      "2019-01-26T23:31:06.448089: step 950, loss 0.000315728, acc 1\n",
      "2019-01-26T23:31:06.917087: step 951, loss 0.000639609, acc 1\n",
      "2019-01-26T23:31:07.365677: step 952, loss 0.0177865, acc 0.984375\n",
      "2019-01-26T23:31:07.820457: step 953, loss 0.004986, acc 1\n",
      "2019-01-26T23:31:08.271900: step 954, loss 0.00230198, acc 1\n",
      "2019-01-26T23:31:08.717890: step 955, loss 0.00330141, acc 1\n",
      "2019-01-26T23:31:09.165982: step 956, loss 0.00236695, acc 1\n",
      "2019-01-26T23:31:09.614130: step 957, loss 0.0626658, acc 0.96875\n",
      "2019-01-26T23:31:10.062449: step 958, loss 0.000662827, acc 1\n",
      "2019-01-26T23:31:10.509664: step 959, loss 0.00183522, acc 1\n",
      "2019-01-26T23:31:10.958021: step 960, loss 0.000167631, acc 1\n",
      "2019-01-26T23:31:11.419253: step 961, loss 0.00153201, acc 1\n",
      "2019-01-26T23:31:11.866076: step 962, loss 0.00229181, acc 1\n",
      "2019-01-26T23:31:12.309459: step 963, loss 0.00275694, acc 1\n",
      "2019-01-26T23:31:12.755525: step 964, loss 0.000786389, acc 1\n",
      "2019-01-26T23:31:13.193132: step 965, loss 0.000467717, acc 1\n",
      "2019-01-26T23:31:13.627594: step 966, loss 0.00272802, acc 1\n",
      "2019-01-26T23:31:14.056631: step 967, loss 0.000842899, acc 1\n",
      "2019-01-26T23:31:14.488882: step 968, loss 0.00429596, acc 1\n",
      "2019-01-26T23:31:14.922329: step 969, loss 0.0432311, acc 0.96875\n",
      "2019-01-26T23:31:15.354381: step 970, loss 0.0104522, acc 1\n",
      "2019-01-26T23:31:15.789029: step 971, loss 0.0126597, acc 1\n",
      "2019-01-26T23:31:16.217355: step 972, loss 0.000701268, acc 1\n",
      "2019-01-26T23:31:16.662673: step 973, loss 0.00200711, acc 1\n",
      "2019-01-26T23:31:17.105382: step 974, loss 0.0724638, acc 0.984375\n",
      "2019-01-26T23:31:17.557285: step 975, loss 0.0026781, acc 1\n",
      "2019-01-26T23:31:18.024332: step 976, loss 0.0182791, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:31:18.472882: step 977, loss 0.000923148, acc 1\n",
      "2019-01-26T23:31:18.916908: step 978, loss 0.00260456, acc 1\n",
      "2019-01-26T23:31:19.340879: step 979, loss 0.00382464, acc 1\n",
      "2019-01-26T23:31:19.773093: step 980, loss 0.000455166, acc 1\n",
      "2019-01-26T23:31:20.195800: step 981, loss 0.00731582, acc 1\n",
      "2019-01-26T23:31:20.624053: step 982, loss 0.0168624, acc 0.984375\n",
      "2019-01-26T23:31:21.055660: step 983, loss 0.00176919, acc 1\n",
      "2019-01-26T23:31:21.483020: step 984, loss 0.00171807, acc 1\n",
      "2019-01-26T23:31:21.918270: step 985, loss 0.00382357, acc 1\n",
      "2019-01-26T23:31:22.344691: step 986, loss 0.00900711, acc 1\n",
      "2019-01-26T23:31:22.771099: step 987, loss 0.0126106, acc 1\n",
      "2019-01-26T23:31:23.200431: step 988, loss 0.000425892, acc 1\n",
      "2019-01-26T23:31:23.632351: step 989, loss 0.0157971, acc 0.984375\n",
      "2019-01-26T23:31:24.056747: step 990, loss 0.0137189, acc 0.984375\n",
      "2019-01-26T23:31:24.479557: step 991, loss 0.00827686, acc 1\n",
      "2019-01-26T23:31:24.908428: step 992, loss 0.000707252, acc 1\n",
      "2019-01-26T23:31:25.338144: step 993, loss 0.00748354, acc 1\n",
      "2019-01-26T23:31:25.782008: step 994, loss 0.00274003, acc 1\n",
      "2019-01-26T23:31:26.221068: step 995, loss 0.00190645, acc 1\n",
      "2019-01-26T23:31:26.664327: step 996, loss 0.0142231, acc 0.984375\n",
      "2019-01-26T23:31:27.119494: step 997, loss 0.00168987, acc 1\n",
      "2019-01-26T23:31:27.556795: step 998, loss 0.000779171, acc 1\n",
      "2019-01-26T23:31:27.983527: step 999, loss 0.000541739, acc 1\n",
      "2019-01-26T23:31:28.407841: step 1000, loss 0.00367643, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:31:29.474925: step 1000, loss 0.0562752, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1000\n",
      "\n",
      "2019-01-26T23:31:29.985293: step 1001, loss 0.000109719, acc 1\n",
      "2019-01-26T23:31:30.414234: step 1002, loss 0.00202787, acc 1\n",
      "2019-01-26T23:31:30.847434: step 1003, loss 0.00279289, acc 1\n",
      "2019-01-26T23:31:31.277094: step 1004, loss 0.00107918, acc 1\n",
      "2019-01-26T23:31:31.696116: step 1005, loss 0.011892, acc 1\n",
      "2019-01-26T23:31:32.112677: step 1006, loss 0.00208877, acc 1\n",
      "2019-01-26T23:31:32.525006: step 1007, loss 0.00564231, acc 1\n",
      "2019-01-26T23:31:32.943954: step 1008, loss 0.00110874, acc 1\n",
      "2019-01-26T23:31:33.354920: step 1009, loss 0.000531166, acc 1\n",
      "2019-01-26T23:31:33.777238: step 1010, loss 0.0048418, acc 1\n",
      "2019-01-26T23:31:34.186809: step 1011, loss 0.0324586, acc 0.984375\n",
      "2019-01-26T23:31:34.592988: step 1012, loss 0.0188361, acc 0.984375\n",
      "2019-01-26T23:31:35.002520: step 1013, loss 0.00431315, acc 1\n",
      "2019-01-26T23:31:35.417450: step 1014, loss 0.00359382, acc 1\n",
      "2019-01-26T23:31:35.832421: step 1015, loss 0.0270922, acc 0.984375\n",
      "2019-01-26T23:31:36.245927: step 1016, loss 0.0063066, acc 1\n",
      "2019-01-26T23:31:36.694849: step 1017, loss 0.0182938, acc 1\n",
      "2019-01-26T23:31:37.111403: step 1018, loss 0.0567279, acc 0.984375\n",
      "2019-01-26T23:31:37.518513: step 1019, loss 0.000816691, acc 1\n",
      "2019-01-26T23:31:37.934966: step 1020, loss 0.0157399, acc 0.984375\n",
      "2019-01-26T23:31:38.355510: step 1021, loss 0.00699638, acc 1\n",
      "2019-01-26T23:31:38.774141: step 1022, loss 0.00184763, acc 1\n",
      "2019-01-26T23:31:39.197561: step 1023, loss 0.00107918, acc 1\n",
      "2019-01-26T23:31:39.632747: step 1024, loss 0.00216489, acc 1\n",
      "2019-01-26T23:31:40.060433: step 1025, loss 0.000467659, acc 1\n",
      "2019-01-26T23:31:40.493124: step 1026, loss 0.000475591, acc 1\n",
      "2019-01-26T23:31:40.689181: step 1027, loss 0.00402198, acc 1\n",
      "2019-01-26T23:31:41.138107: step 1028, loss 0.000352712, acc 1\n",
      "2019-01-26T23:31:41.584680: step 1029, loss 0.00298323, acc 1\n",
      "2019-01-26T23:31:42.029472: step 1030, loss 0.00385042, acc 1\n",
      "2019-01-26T23:31:42.476764: step 1031, loss 0.0018489, acc 1\n",
      "2019-01-26T23:31:42.932064: step 1032, loss 0.00144716, acc 1\n",
      "2019-01-26T23:31:43.377890: step 1033, loss 0.00310272, acc 1\n",
      "2019-01-26T23:31:43.828949: step 1034, loss 0.000449681, acc 1\n",
      "2019-01-26T23:31:44.270364: step 1035, loss 0.0212242, acc 0.984375\n",
      "2019-01-26T23:31:44.721412: step 1036, loss 0.000199181, acc 1\n",
      "2019-01-26T23:31:45.170132: step 1037, loss 0.000404507, acc 1\n",
      "2019-01-26T23:31:45.608948: step 1038, loss 0.0125478, acc 0.984375\n",
      "2019-01-26T23:31:46.055790: step 1039, loss 0.00383336, acc 1\n",
      "2019-01-26T23:31:46.501451: step 1040, loss 0.00482959, acc 1\n",
      "2019-01-26T23:31:46.944479: step 1041, loss 0.00265879, acc 1\n",
      "2019-01-26T23:31:47.393362: step 1042, loss 0.00750046, acc 1\n",
      "2019-01-26T23:31:47.835999: step 1043, loss 0.0107597, acc 1\n",
      "2019-01-26T23:31:48.278979: step 1044, loss 0.000462706, acc 1\n",
      "2019-01-26T23:31:48.713042: step 1045, loss 0.00252565, acc 1\n",
      "2019-01-26T23:31:49.139922: step 1046, loss 0.000423393, acc 1\n",
      "2019-01-26T23:31:49.613513: step 1047, loss 0.00198758, acc 1\n",
      "2019-01-26T23:31:50.055156: step 1048, loss 0.000348001, acc 1\n",
      "2019-01-26T23:31:50.483937: step 1049, loss 0.00428606, acc 1\n",
      "2019-01-26T23:31:50.948617: step 1050, loss 0.00420537, acc 1\n",
      "2019-01-26T23:31:51.381999: step 1051, loss 0.00341064, acc 1\n",
      "2019-01-26T23:31:51.832395: step 1052, loss 0.0100884, acc 1\n",
      "2019-01-26T23:31:52.250505: step 1053, loss 0.0011007, acc 1\n",
      "2019-01-26T23:31:52.667913: step 1054, loss 0.000101954, acc 1\n",
      "2019-01-26T23:31:53.079543: step 1055, loss 0.00129672, acc 1\n",
      "2019-01-26T23:31:53.497924: step 1056, loss 0.00215784, acc 1\n",
      "2019-01-26T23:31:53.920002: step 1057, loss 0.00385565, acc 1\n",
      "2019-01-26T23:31:54.335908: step 1058, loss 0.00114199, acc 1\n",
      "2019-01-26T23:31:54.750346: step 1059, loss 0.00275325, acc 1\n",
      "2019-01-26T23:31:55.166596: step 1060, loss 0.00219474, acc 1\n",
      "2019-01-26T23:31:55.582858: step 1061, loss 0.00120343, acc 1\n",
      "2019-01-26T23:31:56.002138: step 1062, loss 0.00262154, acc 1\n",
      "2019-01-26T23:31:56.414167: step 1063, loss 0.00223816, acc 1\n",
      "2019-01-26T23:31:56.827551: step 1064, loss 0.000100017, acc 1\n",
      "2019-01-26T23:31:57.240387: step 1065, loss 0.00301544, acc 1\n",
      "2019-01-26T23:31:57.659141: step 1066, loss 0.00457223, acc 1\n",
      "2019-01-26T23:31:58.075551: step 1067, loss 0.000773153, acc 1\n",
      "2019-01-26T23:31:58.493834: step 1068, loss 0.000208951, acc 1\n",
      "2019-01-26T23:31:58.912521: step 1069, loss 0.00303762, acc 1\n",
      "2019-01-26T23:31:59.339344: step 1070, loss 0.0201978, acc 0.984375\n",
      "2019-01-26T23:31:59.788556: step 1071, loss 0.0236934, acc 0.984375\n",
      "2019-01-26T23:32:00.226157: step 1072, loss 0.00251667, acc 1\n",
      "2019-01-26T23:32:00.676489: step 1073, loss 0.000943885, acc 1\n",
      "2019-01-26T23:32:01.128462: step 1074, loss 0.0039439, acc 1\n",
      "2019-01-26T23:32:01.574138: step 1075, loss 0.000444765, acc 1\n",
      "2019-01-26T23:32:02.024922: step 1076, loss 0.00265057, acc 1\n",
      "2019-01-26T23:32:02.474432: step 1077, loss 0.00283025, acc 1\n",
      "2019-01-26T23:32:02.945287: step 1078, loss 0.00906277, acc 1\n",
      "2019-01-26T23:32:03.399711: step 1079, loss 0.012452, acc 1\n",
      "2019-01-26T23:32:03.849221: step 1080, loss 0.00186136, acc 1\n",
      "2019-01-26T23:32:04.294557: step 1081, loss 0.00117056, acc 1\n",
      "2019-01-26T23:32:04.747927: step 1082, loss 0.000303839, acc 1\n",
      "2019-01-26T23:32:05.190984: step 1083, loss 0.00044967, acc 1\n",
      "2019-01-26T23:32:05.638384: step 1084, loss 0.00309714, acc 1\n",
      "2019-01-26T23:32:06.087742: step 1085, loss 0.000365777, acc 1\n",
      "2019-01-26T23:32:06.527522: step 1086, loss 0.0135927, acc 0.984375\n",
      "2019-01-26T23:32:06.976947: step 1087, loss 0.000530086, acc 1\n",
      "2019-01-26T23:32:07.432644: step 1088, loss 0.00608423, acc 1\n",
      "2019-01-26T23:32:07.915815: step 1089, loss 0.00819335, acc 1\n",
      "2019-01-26T23:32:08.363765: step 1090, loss 0.000578098, acc 1\n",
      "2019-01-26T23:32:08.791387: step 1091, loss 0.00123648, acc 1\n",
      "2019-01-26T23:32:09.225232: step 1092, loss 0.000823761, acc 1\n",
      "2019-01-26T23:32:09.662561: step 1093, loss 0.000754029, acc 1\n",
      "2019-01-26T23:32:10.094073: step 1094, loss 0.000463625, acc 1\n",
      "2019-01-26T23:32:10.527649: step 1095, loss 0.0121123, acc 1\n",
      "2019-01-26T23:32:10.968466: step 1096, loss 0.00010984, acc 1\n",
      "2019-01-26T23:32:11.412507: step 1097, loss 0.00179845, acc 1\n",
      "2019-01-26T23:32:11.835966: step 1098, loss 0.0165447, acc 1\n",
      "2019-01-26T23:32:12.254477: step 1099, loss 0.000359932, acc 1\n",
      "2019-01-26T23:32:12.674396: step 1100, loss 0.0469116, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:32:13.703738: step 1100, loss 0.0644461, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1100\n",
      "\n",
      "2019-01-26T23:32:14.202454: step 1101, loss 0.0187403, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:32:14.626412: step 1102, loss 0.0112017, acc 1\n",
      "2019-01-26T23:32:15.035158: step 1103, loss 0.000782358, acc 1\n",
      "2019-01-26T23:32:15.447480: step 1104, loss 0.00423621, acc 1\n",
      "2019-01-26T23:32:15.859441: step 1105, loss 0.000105937, acc 1\n",
      "2019-01-26T23:32:16.038040: step 1106, loss 0.00084976, acc 1\n",
      "2019-01-26T23:32:16.456250: step 1107, loss 0.000282172, acc 1\n",
      "2019-01-26T23:32:16.873027: step 1108, loss 0.00416555, acc 1\n",
      "2019-01-26T23:32:17.287435: step 1109, loss 0.00178902, acc 1\n",
      "2019-01-26T23:32:17.700965: step 1110, loss 0.000370994, acc 1\n",
      "2019-01-26T23:32:18.109496: step 1111, loss 0.00423837, acc 1\n",
      "2019-01-26T23:32:18.517532: step 1112, loss 0.012111, acc 1\n",
      "2019-01-26T23:32:18.938187: step 1113, loss 0.0271939, acc 0.984375\n",
      "2019-01-26T23:32:19.357187: step 1114, loss 0.000304385, acc 1\n",
      "2019-01-26T23:32:19.788574: step 1115, loss 0.000206141, acc 1\n",
      "2019-01-26T23:32:20.215561: step 1116, loss 0.0110276, acc 1\n",
      "2019-01-26T23:32:20.647941: step 1117, loss 0.00084847, acc 1\n",
      "2019-01-26T23:32:21.092725: step 1118, loss 0.000383377, acc 1\n",
      "2019-01-26T23:32:21.537754: step 1119, loss 0.00270265, acc 1\n",
      "2019-01-26T23:32:21.984813: step 1120, loss 0.00580012, acc 1\n",
      "2019-01-26T23:32:22.428649: step 1121, loss 0.00279883, acc 1\n",
      "2019-01-26T23:32:22.890582: step 1122, loss 0.000933285, acc 1\n",
      "2019-01-26T23:32:23.345944: step 1123, loss 0.00131469, acc 1\n",
      "2019-01-26T23:32:23.796972: step 1124, loss 0.00206489, acc 1\n",
      "2019-01-26T23:32:24.241729: step 1125, loss 0.0420188, acc 0.984375\n",
      "2019-01-26T23:32:24.684013: step 1126, loss 0.000116763, acc 1\n",
      "2019-01-26T23:32:25.128203: step 1127, loss 0.0331163, acc 0.984375\n",
      "2019-01-26T23:32:25.574827: step 1128, loss 0.00154053, acc 1\n",
      "2019-01-26T23:32:26.023236: step 1129, loss 0.0579734, acc 0.96875\n",
      "2019-01-26T23:32:26.471032: step 1130, loss 0.00956888, acc 1\n",
      "2019-01-26T23:32:26.917434: step 1131, loss 0.00136415, acc 1\n",
      "2019-01-26T23:32:27.371899: step 1132, loss 0.00385489, acc 1\n",
      "2019-01-26T23:32:27.822319: step 1133, loss 0.0009411, acc 1\n",
      "2019-01-26T23:32:28.259036: step 1134, loss 0.0170699, acc 0.984375\n",
      "2019-01-26T23:32:28.721508: step 1135, loss 0.00280799, acc 1\n",
      "2019-01-26T23:32:29.179184: step 1136, loss 0.00645179, acc 1\n",
      "2019-01-26T23:32:29.605433: step 1137, loss 0.00146218, acc 1\n",
      "2019-01-26T23:32:30.030090: step 1138, loss 0.00478861, acc 1\n",
      "2019-01-26T23:32:30.457762: step 1139, loss 0.00260893, acc 1\n",
      "2019-01-26T23:32:30.889799: step 1140, loss 0.00109697, acc 1\n",
      "2019-01-26T23:32:31.315200: step 1141, loss 0.000231513, acc 1\n",
      "2019-01-26T23:32:31.740721: step 1142, loss 0.000572492, acc 1\n",
      "2019-01-26T23:32:32.158791: step 1143, loss 0.00241807, acc 1\n",
      "2019-01-26T23:32:32.570392: step 1144, loss 0.000128503, acc 1\n",
      "2019-01-26T23:32:32.982100: step 1145, loss 0.00166888, acc 1\n",
      "2019-01-26T23:32:33.398502: step 1146, loss 0.00791008, acc 1\n",
      "2019-01-26T23:32:33.824295: step 1147, loss 0.00147543, acc 1\n",
      "2019-01-26T23:32:34.245530: step 1148, loss 0.000576772, acc 1\n",
      "2019-01-26T23:32:34.651318: step 1149, loss 0.00820394, acc 1\n",
      "2019-01-26T23:32:35.059345: step 1150, loss 0.00089321, acc 1\n",
      "2019-01-26T23:32:35.469768: step 1151, loss 0.000485458, acc 1\n",
      "2019-01-26T23:32:35.892158: step 1152, loss 0.00110086, acc 1\n",
      "2019-01-26T23:32:36.308633: step 1153, loss 0.00288407, acc 1\n",
      "2019-01-26T23:32:36.730256: step 1154, loss 3.59933e-05, acc 1\n",
      "2019-01-26T23:32:37.146966: step 1155, loss 0.00552843, acc 1\n",
      "2019-01-26T23:32:37.566455: step 1156, loss 0.0712845, acc 0.984375\n",
      "2019-01-26T23:32:37.984325: step 1157, loss 0.032081, acc 0.984375\n",
      "2019-01-26T23:32:38.401468: step 1158, loss 0.00104156, acc 1\n",
      "2019-01-26T23:32:38.815032: step 1159, loss 0.000620438, acc 1\n",
      "2019-01-26T23:32:39.227437: step 1160, loss 0.00931238, acc 1\n",
      "2019-01-26T23:32:39.657659: step 1161, loss 0.000820275, acc 1\n",
      "2019-01-26T23:32:40.088277: step 1162, loss 0.000707058, acc 1\n",
      "2019-01-26T23:32:40.519676: step 1163, loss 0.00373807, acc 1\n",
      "2019-01-26T23:32:40.975053: step 1164, loss 0.0251939, acc 0.984375\n",
      "2019-01-26T23:32:41.418941: step 1165, loss 0.00266351, acc 1\n",
      "2019-01-26T23:32:41.875950: step 1166, loss 0.00061958, acc 1\n",
      "2019-01-26T23:32:42.321942: step 1167, loss 0.0565019, acc 0.984375\n",
      "2019-01-26T23:32:42.771514: step 1168, loss 0.003107, acc 1\n",
      "2019-01-26T23:32:43.229642: step 1169, loss 0.000428702, acc 1\n",
      "2019-01-26T23:32:43.672616: step 1170, loss 0.000623066, acc 1\n",
      "2019-01-26T23:32:44.117888: step 1171, loss 0.000180733, acc 1\n",
      "2019-01-26T23:32:44.568809: step 1172, loss 0.000100569, acc 1\n",
      "2019-01-26T23:32:45.015524: step 1173, loss 0.000929768, acc 1\n",
      "2019-01-26T23:32:45.460919: step 1174, loss 0.000506999, acc 1\n",
      "2019-01-26T23:32:45.903372: step 1175, loss 9.06429e-05, acc 1\n",
      "2019-01-26T23:32:46.350794: step 1176, loss 0.003331, acc 1\n",
      "2019-01-26T23:32:46.795065: step 1177, loss 0.0494854, acc 0.96875\n",
      "2019-01-26T23:32:47.236067: step 1178, loss 0.0011051, acc 1\n",
      "2019-01-26T23:32:47.681697: step 1179, loss 0.0354625, acc 0.984375\n",
      "2019-01-26T23:32:48.127321: step 1180, loss 0.0147978, acc 1\n",
      "2019-01-26T23:32:48.576776: step 1181, loss 0.000150312, acc 1\n",
      "2019-01-26T23:32:49.029171: step 1182, loss 0.0425423, acc 0.96875\n",
      "2019-01-26T23:32:49.457591: step 1183, loss 9.04646e-05, acc 1\n",
      "2019-01-26T23:32:49.887151: step 1184, loss 0.000548908, acc 1\n",
      "2019-01-26T23:32:50.070081: step 1185, loss 3.16087e-05, acc 1\n",
      "2019-01-26T23:32:50.502367: step 1186, loss 0.000174948, acc 1\n",
      "2019-01-26T23:32:50.935662: step 1187, loss 0.00832055, acc 1\n",
      "2019-01-26T23:32:51.365989: step 1188, loss 0.000261195, acc 1\n",
      "2019-01-26T23:32:51.786660: step 1189, loss 0.000235477, acc 1\n",
      "2019-01-26T23:32:52.202586: step 1190, loss 0.00076786, acc 1\n",
      "2019-01-26T23:32:52.616036: step 1191, loss 0.00117402, acc 1\n",
      "2019-01-26T23:32:53.032115: step 1192, loss 0.00689566, acc 1\n",
      "2019-01-26T23:32:53.443485: step 1193, loss 0.00255002, acc 1\n",
      "2019-01-26T23:32:53.866332: step 1194, loss 0.00280492, acc 1\n",
      "2019-01-26T23:32:54.286653: step 1195, loss 0.0312474, acc 0.984375\n",
      "2019-01-26T23:32:54.710161: step 1196, loss 0.0016286, acc 1\n",
      "2019-01-26T23:32:55.120729: step 1197, loss 0.000809976, acc 1\n",
      "2019-01-26T23:32:55.542077: step 1198, loss 0.000997942, acc 1\n",
      "2019-01-26T23:32:55.953982: step 1199, loss 0.0126611, acc 1\n",
      "2019-01-26T23:32:56.374198: step 1200, loss 0.00778606, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:32:57.401554: step 1200, loss 0.0572145, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1200\n",
      "\n",
      "2019-01-26T23:32:57.900258: step 1201, loss 0.00141587, acc 1\n",
      "2019-01-26T23:32:58.316577: step 1202, loss 0.00512126, acc 1\n",
      "2019-01-26T23:32:58.734825: step 1203, loss 0.00849647, acc 1\n",
      "2019-01-26T23:32:59.157815: step 1204, loss 0.00984742, acc 1\n",
      "2019-01-26T23:32:59.590712: step 1205, loss 0.0132364, acc 1\n",
      "2019-01-26T23:33:00.026408: step 1206, loss 0.0303334, acc 0.984375\n",
      "2019-01-26T23:33:00.466264: step 1207, loss 0.000751282, acc 1\n",
      "2019-01-26T23:33:00.920028: step 1208, loss 0.00013973, acc 1\n",
      "2019-01-26T23:33:01.363501: step 1209, loss 0.000459548, acc 1\n",
      "2019-01-26T23:33:01.813117: step 1210, loss 0.000818283, acc 1\n",
      "2019-01-26T23:33:02.261753: step 1211, loss 0.00849584, acc 1\n",
      "2019-01-26T23:33:02.722283: step 1212, loss 0.0146544, acc 0.984375\n",
      "2019-01-26T23:33:03.188631: step 1213, loss 0.000125565, acc 1\n",
      "2019-01-26T23:33:03.638558: step 1214, loss 0.000957483, acc 1\n",
      "2019-01-26T23:33:04.091858: step 1215, loss 0.00133076, acc 1\n",
      "2019-01-26T23:33:04.531186: step 1216, loss 0.00095006, acc 1\n",
      "2019-01-26T23:33:04.982122: step 1217, loss 0.0151549, acc 1\n",
      "2019-01-26T23:33:05.432363: step 1218, loss 0.0046587, acc 1\n",
      "2019-01-26T23:33:05.925290: step 1219, loss 0.00357807, acc 1\n",
      "2019-01-26T23:33:06.378613: step 1220, loss 0.000192601, acc 1\n",
      "2019-01-26T23:33:06.828631: step 1221, loss 0.00192116, acc 1\n",
      "2019-01-26T23:33:07.275386: step 1222, loss 0.00127703, acc 1\n",
      "2019-01-26T23:33:07.723531: step 1223, loss 0.00139449, acc 1\n",
      "2019-01-26T23:33:08.184006: step 1224, loss 0.0120697, acc 1\n",
      "2019-01-26T23:33:08.622611: step 1225, loss 0.000412184, acc 1\n",
      "2019-01-26T23:33:09.057344: step 1226, loss 0.00162099, acc 1\n",
      "2019-01-26T23:33:09.490052: step 1227, loss 0.000643651, acc 1\n",
      "2019-01-26T23:33:09.933359: step 1228, loss 0.00380012, acc 1\n",
      "2019-01-26T23:33:10.365667: step 1229, loss 0.000350475, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:33:10.792778: step 1230, loss 0.00061467, acc 1\n",
      "2019-01-26T23:33:11.227136: step 1231, loss 0.00181334, acc 1\n",
      "2019-01-26T23:33:11.677255: step 1232, loss 0.00993488, acc 1\n",
      "2019-01-26T23:33:12.090868: step 1233, loss 0.000751327, acc 1\n",
      "2019-01-26T23:33:12.506333: step 1234, loss 0.00337405, acc 1\n",
      "2019-01-26T23:33:12.921915: step 1235, loss 0.0036255, acc 1\n",
      "2019-01-26T23:33:13.330143: step 1236, loss 0.000690285, acc 1\n",
      "2019-01-26T23:33:13.742363: step 1237, loss 0.00169637, acc 1\n",
      "2019-01-26T23:33:14.153468: step 1238, loss 0.0175887, acc 0.984375\n",
      "2019-01-26T23:33:14.565999: step 1239, loss 0.000267326, acc 1\n",
      "2019-01-26T23:33:14.971184: step 1240, loss 0.000503553, acc 1\n",
      "2019-01-26T23:33:15.381127: step 1241, loss 0.00258594, acc 1\n",
      "2019-01-26T23:33:15.794138: step 1242, loss 0.000351346, acc 1\n",
      "2019-01-26T23:33:16.207374: step 1243, loss 0.0116674, acc 0.984375\n",
      "2019-01-26T23:33:16.620347: step 1244, loss 0.000468122, acc 1\n",
      "2019-01-26T23:33:17.035703: step 1245, loss 8.11462e-05, acc 1\n",
      "2019-01-26T23:33:17.447846: step 1246, loss 8.0045e-05, acc 1\n",
      "2019-01-26T23:33:17.867328: step 1247, loss 0.00408586, acc 1\n",
      "2019-01-26T23:33:18.282695: step 1248, loss 6.89917e-05, acc 1\n",
      "2019-01-26T23:33:18.702075: step 1249, loss 0.000293122, acc 1\n",
      "2019-01-26T23:33:19.117495: step 1250, loss 0.00180942, acc 1\n",
      "2019-01-26T23:33:19.538346: step 1251, loss 0.00422504, acc 1\n",
      "2019-01-26T23:33:19.964658: step 1252, loss 0.00127003, acc 1\n",
      "2019-01-26T23:33:20.391647: step 1253, loss 0.000441122, acc 1\n",
      "2019-01-26T23:33:20.832814: step 1254, loss 6.25491e-05, acc 1\n",
      "2019-01-26T23:33:21.274292: step 1255, loss 0.000434258, acc 1\n",
      "2019-01-26T23:33:21.724214: step 1256, loss 0.00101681, acc 1\n",
      "2019-01-26T23:33:22.166789: step 1257, loss 0.0049195, acc 1\n",
      "2019-01-26T23:33:22.612427: step 1258, loss 0.00012976, acc 1\n",
      "2019-01-26T23:33:23.078501: step 1259, loss 0.0013075, acc 1\n",
      "2019-01-26T23:33:23.531397: step 1260, loss 0.010503, acc 1\n",
      "2019-01-26T23:33:23.977931: step 1261, loss 0.034293, acc 0.984375\n",
      "2019-01-26T23:33:24.429733: step 1262, loss 5.7545e-05, acc 1\n",
      "2019-01-26T23:33:24.875145: step 1263, loss 0.000719616, acc 1\n",
      "2019-01-26T23:33:25.064958: step 1264, loss 0.000253168, acc 1\n",
      "2019-01-26T23:33:25.508776: step 1265, loss 2.27334e-05, acc 1\n",
      "2019-01-26T23:33:25.960587: step 1266, loss 0.00135886, acc 1\n",
      "2019-01-26T23:33:26.399402: step 1267, loss 0.00087439, acc 1\n",
      "2019-01-26T23:33:26.849318: step 1268, loss 0.0194469, acc 0.984375\n",
      "2019-01-26T23:33:27.309281: step 1269, loss 0.000534519, acc 1\n",
      "2019-01-26T23:33:27.757336: step 1270, loss 0.00791696, acc 1\n",
      "2019-01-26T23:33:28.206448: step 1271, loss 0.00301693, acc 1\n",
      "2019-01-26T23:33:28.647027: step 1272, loss 0.00832931, acc 1\n",
      "2019-01-26T23:33:29.076698: step 1273, loss 3.49747e-05, acc 1\n",
      "2019-01-26T23:33:29.514062: step 1274, loss 0.000615888, acc 1\n",
      "2019-01-26T23:33:29.949489: step 1275, loss 0.000803949, acc 1\n",
      "2019-01-26T23:33:30.376314: step 1276, loss 0.000508453, acc 1\n",
      "2019-01-26T23:33:30.807284: step 1277, loss 0.0015991, acc 1\n",
      "2019-01-26T23:33:31.238140: step 1278, loss 0.000483349, acc 1\n",
      "2019-01-26T23:33:31.660932: step 1279, loss 0.00259608, acc 1\n",
      "2019-01-26T23:33:32.076862: step 1280, loss 0.000240966, acc 1\n",
      "2019-01-26T23:33:32.493891: step 1281, loss 0.00512691, acc 1\n",
      "2019-01-26T23:33:32.915530: step 1282, loss 0.00943827, acc 1\n",
      "2019-01-26T23:33:33.332145: step 1283, loss 0.000981444, acc 1\n",
      "2019-01-26T23:33:33.742614: step 1284, loss 0.00337755, acc 1\n",
      "2019-01-26T23:33:34.154233: step 1285, loss 0.00107761, acc 1\n",
      "2019-01-26T23:33:34.574364: step 1286, loss 0.00056696, acc 1\n",
      "2019-01-26T23:33:34.990265: step 1287, loss 0.00047422, acc 1\n",
      "2019-01-26T23:33:35.394666: step 1288, loss 0.00100393, acc 1\n",
      "2019-01-26T23:33:35.832267: step 1289, loss 0.000408743, acc 1\n",
      "2019-01-26T23:33:36.234855: step 1290, loss 0.00232954, acc 1\n",
      "2019-01-26T23:33:36.637714: step 1291, loss 0.000547812, acc 1\n",
      "2019-01-26T23:33:37.042808: step 1292, loss 5.55326e-05, acc 1\n",
      "2019-01-26T23:33:37.439027: step 1293, loss 8.78506e-05, acc 1\n",
      "2019-01-26T23:33:37.843451: step 1294, loss 0.000103064, acc 1\n",
      "2019-01-26T23:33:38.247516: step 1295, loss 0.000161759, acc 1\n",
      "2019-01-26T23:33:38.649345: step 1296, loss 0.000612007, acc 1\n",
      "2019-01-26T23:33:39.045605: step 1297, loss 0.00332962, acc 1\n",
      "2019-01-26T23:33:39.445430: step 1298, loss 0.000655475, acc 1\n",
      "2019-01-26T23:33:39.850357: step 1299, loss 0.0161586, acc 0.984375\n",
      "2019-01-26T23:33:40.257944: step 1300, loss 2.85386e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:33:41.245778: step 1300, loss 0.0642768, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1300\n",
      "\n",
      "2019-01-26T23:33:41.729638: step 1301, loss 0.000508672, acc 1\n",
      "2019-01-26T23:33:42.136350: step 1302, loss 7.87209e-05, acc 1\n",
      "2019-01-26T23:33:42.538554: step 1303, loss 0.00221409, acc 1\n",
      "2019-01-26T23:33:42.945519: step 1304, loss 0.000306978, acc 1\n",
      "2019-01-26T23:33:43.347992: step 1305, loss 0.00635322, acc 1\n",
      "2019-01-26T23:33:43.754610: step 1306, loss 0.00199962, acc 1\n",
      "2019-01-26T23:33:44.174498: step 1307, loss 0.000135245, acc 1\n",
      "2019-01-26T23:33:44.592131: step 1308, loss 0.00201413, acc 1\n",
      "2019-01-26T23:33:45.016549: step 1309, loss 0.012548, acc 0.984375\n",
      "2019-01-26T23:33:45.441448: step 1310, loss 0.00132025, acc 1\n",
      "2019-01-26T23:33:45.873414: step 1311, loss 0.00519132, acc 1\n",
      "2019-01-26T23:33:46.302771: step 1312, loss 0.00106646, acc 1\n",
      "2019-01-26T23:33:46.739291: step 1313, loss 0.000260068, acc 1\n",
      "2019-01-26T23:33:47.167243: step 1314, loss 0.000526, acc 1\n",
      "2019-01-26T23:33:47.614861: step 1315, loss 0.00201217, acc 1\n",
      "2019-01-26T23:33:48.060280: step 1316, loss 0.0069772, acc 1\n",
      "2019-01-26T23:33:48.515031: step 1317, loss 4.12403e-05, acc 1\n",
      "2019-01-26T23:33:48.991931: step 1318, loss 0.00228993, acc 1\n",
      "2019-01-26T23:33:49.484769: step 1319, loss 0.000257417, acc 1\n",
      "2019-01-26T23:33:49.979702: step 1320, loss 0.00905089, acc 1\n",
      "2019-01-26T23:33:50.459758: step 1321, loss 0.00396695, acc 1\n",
      "2019-01-26T23:33:50.948706: step 1322, loss 0.00264615, acc 1\n",
      "2019-01-26T23:33:51.431691: step 1323, loss 0.00731345, acc 1\n",
      "2019-01-26T23:33:51.953765: step 1324, loss 0.00227028, acc 1\n",
      "2019-01-26T23:33:52.434768: step 1325, loss 0.000348718, acc 1\n",
      "2019-01-26T23:33:52.926635: step 1326, loss 0.00404616, acc 1\n",
      "2019-01-26T23:33:53.415220: step 1327, loss 0.000128546, acc 1\n",
      "2019-01-26T23:33:53.901271: step 1328, loss 0.000142202, acc 1\n",
      "2019-01-26T23:33:54.382800: step 1329, loss 0.000166242, acc 1\n",
      "2019-01-26T23:33:54.863216: step 1330, loss 0.00257914, acc 1\n",
      "2019-01-26T23:33:55.323673: step 1331, loss 0.00142211, acc 1\n",
      "2019-01-26T23:33:55.792391: step 1332, loss 0.00151711, acc 1\n",
      "2019-01-26T23:33:56.256525: step 1333, loss 0.0188914, acc 0.984375\n",
      "2019-01-26T23:33:56.722676: step 1334, loss 0.000576344, acc 1\n",
      "2019-01-26T23:33:57.188579: step 1335, loss 0.000317575, acc 1\n",
      "2019-01-26T23:33:57.657917: step 1336, loss 0.000351836, acc 1\n",
      "2019-01-26T23:33:58.114982: step 1337, loss 0.000126424, acc 1\n",
      "2019-01-26T23:33:58.577502: step 1338, loss 0.000442259, acc 1\n",
      "2019-01-26T23:33:59.094038: step 1339, loss 0.000487086, acc 1\n",
      "2019-01-26T23:33:59.566417: step 1340, loss 0.00733176, acc 1\n",
      "2019-01-26T23:34:00.047463: step 1341, loss 0.063119, acc 0.984375\n",
      "2019-01-26T23:34:00.487538: step 1342, loss 0.000710639, acc 1\n",
      "2019-01-26T23:34:00.714594: step 1343, loss 0.000495703, acc 1\n",
      "2019-01-26T23:34:01.155369: step 1344, loss 0.00018674, acc 1\n",
      "2019-01-26T23:34:01.585308: step 1345, loss 0.00505628, acc 1\n",
      "2019-01-26T23:34:02.015807: step 1346, loss 0.000175762, acc 1\n",
      "2019-01-26T23:34:02.439476: step 1347, loss 0.000750849, acc 1\n",
      "2019-01-26T23:34:02.872155: step 1348, loss 0.000300098, acc 1\n",
      "2019-01-26T23:34:03.303077: step 1349, loss 0.000411094, acc 1\n",
      "2019-01-26T23:34:03.730356: step 1350, loss 0.000181952, acc 1\n",
      "2019-01-26T23:34:04.151437: step 1351, loss 7.89537e-05, acc 1\n",
      "2019-01-26T23:34:04.569843: step 1352, loss 0.000109099, acc 1\n",
      "2019-01-26T23:34:04.987559: step 1353, loss 0.00395508, acc 1\n",
      "2019-01-26T23:34:05.409412: step 1354, loss 0.00243198, acc 1\n",
      "2019-01-26T23:34:05.828203: step 1355, loss 0.000106944, acc 1\n",
      "2019-01-26T23:34:06.243429: step 1356, loss 0.00137849, acc 1\n",
      "2019-01-26T23:34:06.653711: step 1357, loss 0.000794009, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:34:07.062827: step 1358, loss 0.00299798, acc 1\n",
      "2019-01-26T23:34:07.471058: step 1359, loss 0.0179647, acc 0.984375\n",
      "2019-01-26T23:34:07.886113: step 1360, loss 0.00203174, acc 1\n",
      "2019-01-26T23:34:08.304434: step 1361, loss 0.000585323, acc 1\n",
      "2019-01-26T23:34:08.721565: step 1362, loss 0.00911998, acc 1\n",
      "2019-01-26T23:34:09.128045: step 1363, loss 2.05739e-05, acc 1\n",
      "2019-01-26T23:34:09.540099: step 1364, loss 0.00132853, acc 1\n",
      "2019-01-26T23:34:09.964730: step 1365, loss 0.000641133, acc 1\n",
      "2019-01-26T23:34:10.377701: step 1366, loss 0.000124274, acc 1\n",
      "2019-01-26T23:34:10.796803: step 1367, loss 0.0488207, acc 0.984375\n",
      "2019-01-26T23:34:11.211640: step 1368, loss 0.00444828, acc 1\n",
      "2019-01-26T23:34:11.647983: step 1369, loss 0.000110629, acc 1\n",
      "2019-01-26T23:34:12.063807: step 1370, loss 0.00193527, acc 1\n",
      "2019-01-26T23:34:12.473542: step 1371, loss 0.0532472, acc 0.984375\n",
      "2019-01-26T23:34:12.892807: step 1372, loss 0.000182463, acc 1\n",
      "2019-01-26T23:34:13.292973: step 1373, loss 0.00364027, acc 1\n",
      "2019-01-26T23:34:13.698057: step 1374, loss 0.00025325, acc 1\n",
      "2019-01-26T23:34:14.097654: step 1375, loss 0.00189752, acc 1\n",
      "2019-01-26T23:34:14.497004: step 1376, loss 0.000460712, acc 1\n",
      "2019-01-26T23:34:14.904523: step 1377, loss 0.000366981, acc 1\n",
      "2019-01-26T23:34:15.304747: step 1378, loss 0.000278796, acc 1\n",
      "2019-01-26T23:34:15.721936: step 1379, loss 0.00111331, acc 1\n",
      "2019-01-26T23:34:16.138682: step 1380, loss 0.00602526, acc 1\n",
      "2019-01-26T23:34:16.546281: step 1381, loss 0.00239033, acc 1\n",
      "2019-01-26T23:34:16.963834: step 1382, loss 0.00495551, acc 1\n",
      "2019-01-26T23:34:17.377437: step 1383, loss 0.0020434, acc 1\n",
      "2019-01-26T23:34:17.797483: step 1384, loss 0.00156374, acc 1\n",
      "2019-01-26T23:34:18.206015: step 1385, loss 0.000459022, acc 1\n",
      "2019-01-26T23:34:18.659112: step 1386, loss 0.00720604, acc 1\n",
      "2019-01-26T23:34:19.070091: step 1387, loss 0.00304965, acc 1\n",
      "2019-01-26T23:34:19.494658: step 1388, loss 0.00630579, acc 1\n",
      "2019-01-26T23:34:19.925625: step 1389, loss 0.00288179, acc 1\n",
      "2019-01-26T23:34:20.358247: step 1390, loss 0.0059144, acc 1\n",
      "2019-01-26T23:34:20.800040: step 1391, loss 0.000904249, acc 1\n",
      "2019-01-26T23:34:21.242701: step 1392, loss 0.00433892, acc 1\n",
      "2019-01-26T23:34:21.690859: step 1393, loss 0.000124215, acc 1\n",
      "2019-01-26T23:34:22.138623: step 1394, loss 0.000959109, acc 1\n",
      "2019-01-26T23:34:22.582185: step 1395, loss 0.0151987, acc 1\n",
      "2019-01-26T23:34:23.041460: step 1396, loss 0.0154713, acc 0.984375\n",
      "2019-01-26T23:34:23.488393: step 1397, loss 0.0800624, acc 0.984375\n",
      "2019-01-26T23:34:23.936979: step 1398, loss 0.00173532, acc 1\n",
      "2019-01-26T23:34:24.416255: step 1399, loss 0.0013586, acc 1\n",
      "2019-01-26T23:34:24.910455: step 1400, loss 0.000342869, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:34:26.052856: step 1400, loss 0.0674053, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1400\n",
      "\n",
      "2019-01-26T23:34:26.580524: step 1401, loss 0.000101692, acc 1\n",
      "2019-01-26T23:34:27.040742: step 1402, loss 0.00621607, acc 1\n",
      "2019-01-26T23:34:27.481023: step 1403, loss 0.000140217, acc 1\n",
      "2019-01-26T23:34:27.935896: step 1404, loss 0.0257752, acc 0.984375\n",
      "2019-01-26T23:34:28.381193: step 1405, loss 0.0303468, acc 0.984375\n",
      "2019-01-26T23:34:28.811977: step 1406, loss 0.0371341, acc 0.984375\n",
      "2019-01-26T23:34:29.241436: step 1407, loss 0.000431521, acc 1\n",
      "2019-01-26T23:34:29.702858: step 1408, loss 0.0199156, acc 0.984375\n",
      "2019-01-26T23:34:30.136438: step 1409, loss 0.000208538, acc 1\n",
      "2019-01-26T23:34:30.567924: step 1410, loss 0.00090432, acc 1\n",
      "2019-01-26T23:34:31.003059: step 1411, loss 0.00109659, acc 1\n",
      "2019-01-26T23:34:31.430581: step 1412, loss 0.000973095, acc 1\n",
      "2019-01-26T23:34:31.854946: step 1413, loss 0.000624718, acc 1\n",
      "2019-01-26T23:34:32.270688: step 1414, loss 0.0051614, acc 1\n",
      "2019-01-26T23:34:32.691450: step 1415, loss 0.00522271, acc 1\n",
      "2019-01-26T23:34:33.106414: step 1416, loss 5.72701e-05, acc 1\n",
      "2019-01-26T23:34:33.525290: step 1417, loss 0.000342845, acc 1\n",
      "2019-01-26T23:34:33.941545: step 1418, loss 0.0382193, acc 0.984375\n",
      "2019-01-26T23:34:34.354238: step 1419, loss 0.000445911, acc 1\n",
      "2019-01-26T23:34:34.777651: step 1420, loss 0.00130606, acc 1\n",
      "2019-01-26T23:34:35.186595: step 1421, loss 0.00127763, acc 1\n",
      "2019-01-26T23:34:35.366863: step 1422, loss 0.000995951, acc 1\n",
      "2019-01-26T23:34:35.784091: step 1423, loss 0.00130974, acc 1\n",
      "2019-01-26T23:34:36.203716: step 1424, loss 0.000300941, acc 1\n",
      "2019-01-26T23:34:36.629924: step 1425, loss 0.00159277, acc 1\n",
      "2019-01-26T23:34:37.047330: step 1426, loss 0.000341561, acc 1\n",
      "2019-01-26T23:34:37.460627: step 1427, loss 0.00148705, acc 1\n",
      "2019-01-26T23:34:37.879832: step 1428, loss 0.00130551, acc 1\n",
      "2019-01-26T23:34:38.293838: step 1429, loss 0.00290054, acc 1\n",
      "2019-01-26T23:34:38.709383: step 1430, loss 0.000548839, acc 1\n",
      "2019-01-26T23:34:39.123832: step 1431, loss 0.00641474, acc 1\n",
      "2019-01-26T23:34:39.542670: step 1432, loss 0.00743971, acc 1\n",
      "2019-01-26T23:34:39.983004: step 1433, loss 0.0047636, acc 1\n",
      "2019-01-26T23:34:40.409784: step 1434, loss 0.000720103, acc 1\n",
      "2019-01-26T23:34:40.849571: step 1435, loss 0.0054143, acc 1\n",
      "2019-01-26T23:34:41.296278: step 1436, loss 0.000100258, acc 1\n",
      "2019-01-26T23:34:41.749943: step 1437, loss 0.000733538, acc 1\n",
      "2019-01-26T23:34:42.200200: step 1438, loss 2.74583e-05, acc 1\n",
      "2019-01-26T23:34:42.640746: step 1439, loss 0.00112107, acc 1\n",
      "2019-01-26T23:34:43.103587: step 1440, loss 0.00613671, acc 1\n",
      "2019-01-26T23:34:43.557826: step 1441, loss 0.00831018, acc 1\n",
      "2019-01-26T23:34:44.021948: step 1442, loss 0.000486832, acc 1\n",
      "2019-01-26T23:34:44.476006: step 1443, loss 0.000141786, acc 1\n",
      "2019-01-26T23:34:44.927073: step 1444, loss 0.000132214, acc 1\n",
      "2019-01-26T23:34:45.376714: step 1445, loss 0.000488647, acc 1\n",
      "2019-01-26T23:34:45.824990: step 1446, loss 0.00643336, acc 1\n",
      "2019-01-26T23:34:46.276533: step 1447, loss 0.000200153, acc 1\n",
      "2019-01-26T23:34:46.752277: step 1448, loss 0.00396361, acc 1\n",
      "2019-01-26T23:34:47.199911: step 1449, loss 0.000236545, acc 1\n",
      "2019-01-26T23:34:47.642408: step 1450, loss 0.000300157, acc 1\n",
      "2019-01-26T23:34:48.090521: step 1451, loss 0.000206625, acc 1\n",
      "2019-01-26T23:34:48.533255: step 1452, loss 0.00570255, acc 1\n",
      "2019-01-26T23:34:48.972622: step 1453, loss 0.00485971, acc 1\n",
      "2019-01-26T23:34:49.398126: step 1454, loss 0.0184575, acc 0.984375\n",
      "2019-01-26T23:34:49.832396: step 1455, loss 0.00636415, acc 1\n",
      "2019-01-26T23:34:50.265034: step 1456, loss 0.000299272, acc 1\n",
      "2019-01-26T23:34:50.691239: step 1457, loss 0.000103243, acc 1\n",
      "2019-01-26T23:34:51.118828: step 1458, loss 0.00100635, acc 1\n",
      "2019-01-26T23:34:51.548614: step 1459, loss 0.000289689, acc 1\n",
      "2019-01-26T23:34:51.976232: step 1460, loss 0.000186671, acc 1\n",
      "2019-01-26T23:34:52.390045: step 1461, loss 0.00136991, acc 1\n",
      "2019-01-26T23:34:52.805523: step 1462, loss 0.000853742, acc 1\n",
      "2019-01-26T23:34:53.220934: step 1463, loss 0.000281608, acc 1\n",
      "2019-01-26T23:34:53.636917: step 1464, loss 0.000108219, acc 1\n",
      "2019-01-26T23:34:54.048151: step 1465, loss 4.3573e-05, acc 1\n",
      "2019-01-26T23:34:54.466969: step 1466, loss 0.00200924, acc 1\n",
      "2019-01-26T23:34:54.879166: step 1467, loss 0.00372043, acc 1\n",
      "2019-01-26T23:34:55.293360: step 1468, loss 0.000767704, acc 1\n",
      "2019-01-26T23:34:55.711671: step 1469, loss 0.000830895, acc 1\n",
      "2019-01-26T23:34:56.131128: step 1470, loss 0.00495801, acc 1\n",
      "2019-01-26T23:34:56.548111: step 1471, loss 0.000591283, acc 1\n",
      "2019-01-26T23:34:56.969705: step 1472, loss 6.48197e-05, acc 1\n",
      "2019-01-26T23:34:57.384952: step 1473, loss 0.00121249, acc 1\n",
      "2019-01-26T23:34:57.812767: step 1474, loss 0.0112203, acc 0.984375\n",
      "2019-01-26T23:34:58.229192: step 1475, loss 0.000104493, acc 1\n",
      "2019-01-26T23:34:58.639871: step 1476, loss 0.00105991, acc 1\n",
      "2019-01-26T23:34:59.056984: step 1477, loss 5.34292e-05, acc 1\n",
      "2019-01-26T23:34:59.478681: step 1478, loss 0.000113916, acc 1\n",
      "2019-01-26T23:34:59.904780: step 1479, loss 0.00010784, acc 1\n",
      "2019-01-26T23:35:00.328227: step 1480, loss 0.000694667, acc 1\n",
      "2019-01-26T23:35:00.749125: step 1481, loss 0.00100091, acc 1\n",
      "2019-01-26T23:35:01.168331: step 1482, loss 0.0019198, acc 1\n",
      "2019-01-26T23:35:01.579875: step 1483, loss 0.000343318, acc 1\n",
      "2019-01-26T23:35:01.989387: step 1484, loss 0.000516062, acc 1\n",
      "2019-01-26T23:35:02.394743: step 1485, loss 0.00037326, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:35:02.804841: step 1486, loss 4.79068e-05, acc 1\n",
      "2019-01-26T23:35:03.216296: step 1487, loss 0.000149114, acc 1\n",
      "2019-01-26T23:35:03.630290: step 1488, loss 0.000980795, acc 1\n",
      "2019-01-26T23:35:04.042618: step 1489, loss 0.000399728, acc 1\n",
      "2019-01-26T23:35:04.455783: step 1490, loss 0.00760056, acc 1\n",
      "2019-01-26T23:35:04.873453: step 1491, loss 0.000126609, acc 1\n",
      "2019-01-26T23:35:05.282798: step 1492, loss 0.00509383, acc 1\n",
      "2019-01-26T23:35:05.697424: step 1493, loss 0.000329278, acc 1\n",
      "2019-01-26T23:35:06.118591: step 1494, loss 0.000179541, acc 1\n",
      "2019-01-26T23:35:06.534114: step 1495, loss 0.00145062, acc 1\n",
      "2019-01-26T23:35:06.985360: step 1496, loss 0.000283585, acc 1\n",
      "2019-01-26T23:35:07.399333: step 1497, loss 0.00118648, acc 1\n",
      "2019-01-26T23:35:07.831889: step 1498, loss 0.00621094, acc 1\n",
      "2019-01-26T23:35:08.265442: step 1499, loss 0.0011185, acc 1\n",
      "2019-01-26T23:35:08.700179: step 1500, loss 0.000813845, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:35:09.868068: step 1500, loss 0.0644235, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1500\n",
      "\n",
      "2019-01-26T23:35:10.156557: step 1501, loss 0.0002453, acc 1\n",
      "2019-01-26T23:35:10.602827: step 1502, loss 0.000536995, acc 1\n",
      "2019-01-26T23:35:11.041323: step 1503, loss 0.0276255, acc 0.984375\n",
      "2019-01-26T23:35:11.493177: step 1504, loss 0.000128632, acc 1\n",
      "2019-01-26T23:35:11.943016: step 1505, loss 0.000446567, acc 1\n",
      "2019-01-26T23:35:12.385048: step 1506, loss 0.00771116, acc 1\n",
      "2019-01-26T23:35:12.829658: step 1507, loss 0.00279212, acc 1\n",
      "2019-01-26T23:35:13.275359: step 1508, loss 0.000252553, acc 1\n",
      "2019-01-26T23:35:13.727424: step 1509, loss 0.00113096, acc 1\n",
      "2019-01-26T23:35:14.172422: step 1510, loss 0.00391068, acc 1\n",
      "2019-01-26T23:35:14.611588: step 1511, loss 9.34405e-05, acc 1\n",
      "2019-01-26T23:35:15.055994: step 1512, loss 0.00158538, acc 1\n",
      "2019-01-26T23:35:15.500476: step 1513, loss 0.00138687, acc 1\n",
      "2019-01-26T23:35:15.950738: step 1514, loss 0.0120215, acc 1\n",
      "2019-01-26T23:35:16.401596: step 1515, loss 0.00115439, acc 1\n",
      "2019-01-26T23:35:16.840796: step 1516, loss 0.000298644, acc 1\n",
      "2019-01-26T23:35:17.270438: step 1517, loss 0.000892021, acc 1\n",
      "2019-01-26T23:35:17.724923: step 1518, loss 0.000531383, acc 1\n",
      "2019-01-26T23:35:18.161736: step 1519, loss 0.00444485, acc 1\n",
      "2019-01-26T23:35:18.594575: step 1520, loss 0.00188367, acc 1\n",
      "2019-01-26T23:35:19.022738: step 1521, loss 0.00214294, acc 1\n",
      "2019-01-26T23:35:19.458654: step 1522, loss 0.00458667, acc 1\n",
      "2019-01-26T23:35:19.879122: step 1523, loss 0.00047924, acc 1\n",
      "2019-01-26T23:35:20.291801: step 1524, loss 0.000186589, acc 1\n",
      "2019-01-26T23:35:20.705239: step 1525, loss 0.00039524, acc 1\n",
      "2019-01-26T23:35:21.122673: step 1526, loss 0.0104589, acc 1\n",
      "2019-01-26T23:35:21.534549: step 1527, loss 0.000775736, acc 1\n",
      "2019-01-26T23:35:21.955423: step 1528, loss 0.00147131, acc 1\n",
      "2019-01-26T23:35:22.370540: step 1529, loss 0.00123549, acc 1\n",
      "2019-01-26T23:35:22.785271: step 1530, loss 0.0400184, acc 0.984375\n",
      "2019-01-26T23:35:23.192282: step 1531, loss 0.000520943, acc 1\n",
      "2019-01-26T23:35:23.610191: step 1532, loss 0.00475162, acc 1\n",
      "2019-01-26T23:35:24.028933: step 1533, loss 4.42318e-05, acc 1\n",
      "2019-01-26T23:35:24.441817: step 1534, loss 0.00170881, acc 1\n",
      "2019-01-26T23:35:24.904115: step 1535, loss 0.0118379, acc 0.984375\n",
      "2019-01-26T23:35:25.321708: step 1536, loss 0.000585666, acc 1\n",
      "2019-01-26T23:35:25.745604: step 1537, loss 0.0624123, acc 0.984375\n",
      "2019-01-26T23:35:26.166174: step 1538, loss 0.0243998, acc 0.984375\n",
      "2019-01-26T23:35:26.581485: step 1539, loss 0.000372653, acc 1\n",
      "2019-01-26T23:35:27.014720: step 1540, loss 0.000740504, acc 1\n",
      "2019-01-26T23:35:27.432344: step 1541, loss 3.99776e-05, acc 1\n",
      "2019-01-26T23:35:27.867566: step 1542, loss 0.000247687, acc 1\n",
      "2019-01-26T23:35:28.296749: step 1543, loss 0.000119455, acc 1\n",
      "2019-01-26T23:35:28.733325: step 1544, loss 8.94334e-05, acc 1\n",
      "2019-01-26T23:35:29.176509: step 1545, loss 0.05878, acc 0.984375\n",
      "2019-01-26T23:35:29.623397: step 1546, loss 8.95188e-05, acc 1\n",
      "2019-01-26T23:35:30.068868: step 1547, loss 0.0232347, acc 0.984375\n",
      "2019-01-26T23:35:30.511333: step 1548, loss 0.000105493, acc 1\n",
      "2019-01-26T23:35:30.967319: step 1549, loss 0.00673678, acc 1\n",
      "2019-01-26T23:35:31.420861: step 1550, loss 0.00129117, acc 1\n",
      "2019-01-26T23:35:31.872740: step 1551, loss 0.00200966, acc 1\n",
      "2019-01-26T23:35:32.323453: step 1552, loss 0.00933668, acc 1\n",
      "2019-01-26T23:35:32.781464: step 1553, loss 0.00045115, acc 1\n",
      "2019-01-26T23:35:33.219156: step 1554, loss 0.000153585, acc 1\n",
      "2019-01-26T23:35:33.675751: step 1555, loss 0.000760073, acc 1\n",
      "2019-01-26T23:35:34.118036: step 1556, loss 0.000212574, acc 1\n",
      "2019-01-26T23:35:34.566993: step 1557, loss 0.000149073, acc 1\n",
      "2019-01-26T23:35:35.023665: step 1558, loss 0.00326903, acc 1\n",
      "2019-01-26T23:35:35.468249: step 1559, loss 3.33855e-05, acc 1\n",
      "2019-01-26T23:35:35.917690: step 1560, loss 0.00250332, acc 1\n",
      "2019-01-26T23:35:36.368044: step 1561, loss 0.000162905, acc 1\n",
      "2019-01-26T23:35:36.808715: step 1562, loss 5.96655e-05, acc 1\n",
      "2019-01-26T23:35:37.242232: step 1563, loss 0.00202843, acc 1\n",
      "2019-01-26T23:35:37.678255: step 1564, loss 0.000152943, acc 1\n",
      "2019-01-26T23:35:38.111238: step 1565, loss 0.00145259, acc 1\n",
      "2019-01-26T23:35:38.541394: step 1566, loss 0.00187189, acc 1\n",
      "2019-01-26T23:35:38.970941: step 1567, loss 0.00142567, acc 1\n",
      "2019-01-26T23:35:39.398631: step 1568, loss 2.87514e-05, acc 1\n",
      "2019-01-26T23:35:39.867886: step 1569, loss 3.12816e-05, acc 1\n",
      "2019-01-26T23:35:40.297474: step 1570, loss 0.00138571, acc 1\n",
      "2019-01-26T23:35:40.732396: step 1571, loss 0.010405, acc 1\n",
      "2019-01-26T23:35:41.164719: step 1572, loss 9.09699e-05, acc 1\n",
      "2019-01-26T23:35:41.609547: step 1573, loss 4.06809e-05, acc 1\n",
      "2019-01-26T23:35:42.053074: step 1574, loss 3.5387e-05, acc 1\n",
      "2019-01-26T23:35:42.491999: step 1575, loss 0.000365966, acc 1\n",
      "2019-01-26T23:35:42.940662: step 1576, loss 0.00145138, acc 1\n",
      "2019-01-26T23:35:43.374994: step 1577, loss 0.00138293, acc 1\n",
      "2019-01-26T23:35:43.806732: step 1578, loss 8.19796e-05, acc 1\n",
      "2019-01-26T23:35:44.242092: step 1579, loss 0.000799757, acc 1\n",
      "2019-01-26T23:35:44.430185: step 1580, loss 0.00215701, acc 1\n",
      "2019-01-26T23:35:44.861408: step 1581, loss 5.95513e-05, acc 1\n",
      "2019-01-26T23:35:45.298201: step 1582, loss 0.00220096, acc 1\n",
      "2019-01-26T23:35:45.727268: step 1583, loss 0.00012501, acc 1\n",
      "2019-01-26T23:35:46.158228: step 1584, loss 6.59129e-05, acc 1\n",
      "2019-01-26T23:35:46.590785: step 1585, loss 0.0198303, acc 0.984375\n",
      "2019-01-26T23:35:47.020996: step 1586, loss 0.0142312, acc 0.984375\n",
      "2019-01-26T23:35:47.450683: step 1587, loss 0.00103275, acc 1\n",
      "2019-01-26T23:35:47.888638: step 1588, loss 0.000129375, acc 1\n",
      "2019-01-26T23:35:48.317146: step 1589, loss 0.00047644, acc 1\n",
      "2019-01-26T23:35:48.754330: step 1590, loss 0.000304148, acc 1\n",
      "2019-01-26T23:35:49.190411: step 1591, loss 0.00472181, acc 1\n",
      "2019-01-26T23:35:49.660216: step 1592, loss 0.0110532, acc 1\n",
      "2019-01-26T23:35:50.091525: step 1593, loss 0.000336254, acc 1\n",
      "2019-01-26T23:35:50.531285: step 1594, loss 0.000310788, acc 1\n",
      "2019-01-26T23:35:50.983367: step 1595, loss 0.000158034, acc 1\n",
      "2019-01-26T23:35:51.421236: step 1596, loss 0.000139066, acc 1\n",
      "2019-01-26T23:35:51.888102: step 1597, loss 0.00138494, acc 1\n",
      "2019-01-26T23:35:52.316969: step 1598, loss 0.000979059, acc 1\n",
      "2019-01-26T23:35:52.750228: step 1599, loss 0.00110052, acc 1\n",
      "2019-01-26T23:35:53.180180: step 1600, loss 9.46681e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:35:54.267634: step 1600, loss 0.0615121, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1600\n",
      "\n",
      "2019-01-26T23:35:54.786432: step 1601, loss 0.0010418, acc 1\n",
      "2019-01-26T23:35:55.222308: step 1602, loss 0.000542027, acc 1\n",
      "2019-01-26T23:35:55.652207: step 1603, loss 0.00131853, acc 1\n",
      "2019-01-26T23:35:56.064854: step 1604, loss 0.000286537, acc 1\n",
      "2019-01-26T23:35:56.483406: step 1605, loss 0.000309433, acc 1\n",
      "2019-01-26T23:35:56.905189: step 1606, loss 0.000355833, acc 1\n",
      "2019-01-26T23:35:57.323560: step 1607, loss 0.000600495, acc 1\n",
      "2019-01-26T23:35:57.756234: step 1608, loss 0.00749989, acc 1\n",
      "2019-01-26T23:35:58.170278: step 1609, loss 0.0023874, acc 1\n",
      "2019-01-26T23:35:58.578334: step 1610, loss 0.0129922, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:35:58.983090: step 1611, loss 0.000289305, acc 1\n",
      "2019-01-26T23:35:59.394080: step 1612, loss 0.00474761, acc 1\n",
      "2019-01-26T23:35:59.810139: step 1613, loss 0.00200968, acc 1\n",
      "2019-01-26T23:36:00.229004: step 1614, loss 0.000344169, acc 1\n",
      "2019-01-26T23:36:00.644076: step 1615, loss 6.5806e-05, acc 1\n",
      "2019-01-26T23:36:01.061487: step 1616, loss 0.000537165, acc 1\n",
      "2019-01-26T23:36:01.470569: step 1617, loss 0.000138413, acc 1\n",
      "2019-01-26T23:36:01.891077: step 1618, loss 0.000403204, acc 1\n",
      "2019-01-26T23:36:02.306339: step 1619, loss 0.00655637, acc 1\n",
      "2019-01-26T23:36:02.720616: step 1620, loss 3.07103e-05, acc 1\n",
      "2019-01-26T23:36:03.129294: step 1621, loss 0.000199347, acc 1\n",
      "2019-01-26T23:36:03.552380: step 1622, loss 0.000248046, acc 1\n",
      "2019-01-26T23:36:03.980881: step 1623, loss 0.000122614, acc 1\n",
      "2019-01-26T23:36:04.434038: step 1624, loss 8.25515e-05, acc 1\n",
      "2019-01-26T23:36:04.867676: step 1625, loss 0.0022173, acc 1\n",
      "2019-01-26T23:36:05.310157: step 1626, loss 0.00051147, acc 1\n",
      "2019-01-26T23:36:05.761077: step 1627, loss 6.80847e-05, acc 1\n",
      "2019-01-26T23:36:06.203574: step 1628, loss 0.000903502, acc 1\n",
      "2019-01-26T23:36:06.675604: step 1629, loss 0.000186078, acc 1\n",
      "2019-01-26T23:36:07.133025: step 1630, loss 0.000117993, acc 1\n",
      "2019-01-26T23:36:07.578548: step 1631, loss 0.00104969, acc 1\n",
      "2019-01-26T23:36:08.028989: step 1632, loss 0.0048955, acc 1\n",
      "2019-01-26T23:36:08.473097: step 1633, loss 0.00350795, acc 1\n",
      "2019-01-26T23:36:08.919148: step 1634, loss 0.000674269, acc 1\n",
      "2019-01-26T23:36:09.362111: step 1635, loss 0.0310175, acc 0.984375\n",
      "2019-01-26T23:36:09.819723: step 1636, loss 0.000651283, acc 1\n",
      "2019-01-26T23:36:10.264820: step 1637, loss 0.000140781, acc 1\n",
      "2019-01-26T23:36:10.710756: step 1638, loss 0.00018594, acc 1\n",
      "2019-01-26T23:36:11.160689: step 1639, loss 0.000581937, acc 1\n",
      "2019-01-26T23:36:11.617930: step 1640, loss 0.000168542, acc 1\n",
      "2019-01-26T23:36:12.064223: step 1641, loss 0.00028896, acc 1\n",
      "2019-01-26T23:36:12.510656: step 1642, loss 8.2796e-05, acc 1\n",
      "2019-01-26T23:36:12.947022: step 1643, loss 9.94399e-05, acc 1\n",
      "2019-01-26T23:36:13.375626: step 1644, loss 0.000785623, acc 1\n",
      "2019-01-26T23:36:13.803445: step 1645, loss 0.000474961, acc 1\n",
      "2019-01-26T23:36:14.228898: step 1646, loss 0.0119033, acc 0.984375\n",
      "2019-01-26T23:36:14.661222: step 1647, loss 0.0111953, acc 1\n",
      "2019-01-26T23:36:15.091237: step 1648, loss 0.00417887, acc 1\n",
      "2019-01-26T23:36:15.521849: step 1649, loss 0.00124093, acc 1\n",
      "2019-01-26T23:36:15.952834: step 1650, loss 0.000684206, acc 1\n",
      "2019-01-26T23:36:16.370178: step 1651, loss 0.00115183, acc 1\n",
      "2019-01-26T23:36:16.787851: step 1652, loss 0.000103093, acc 1\n",
      "2019-01-26T23:36:17.197553: step 1653, loss 0.00126238, acc 1\n",
      "2019-01-26T23:36:17.610827: step 1654, loss 4.47528e-05, acc 1\n",
      "2019-01-26T23:36:18.024372: step 1655, loss 0.0233903, acc 0.984375\n",
      "2019-01-26T23:36:18.440333: step 1656, loss 0.00190786, acc 1\n",
      "2019-01-26T23:36:18.858779: step 1657, loss 0.00117706, acc 1\n",
      "2019-01-26T23:36:19.270256: step 1658, loss 0.00017938, acc 1\n",
      "2019-01-26T23:36:19.452382: step 1659, loss 0.000103043, acc 1\n",
      "2019-01-26T23:36:19.867986: step 1660, loss 0.000223539, acc 1\n",
      "2019-01-26T23:36:20.280957: step 1661, loss 0.000604904, acc 1\n",
      "2019-01-26T23:36:20.701451: step 1662, loss 0.0161787, acc 0.984375\n",
      "2019-01-26T23:36:21.122231: step 1663, loss 0.000126904, acc 1\n",
      "2019-01-26T23:36:21.531378: step 1664, loss 0.000653752, acc 1\n",
      "2019-01-26T23:36:21.947468: step 1665, loss 0.00134129, acc 1\n",
      "2019-01-26T23:36:22.359956: step 1666, loss 0.00219859, acc 1\n",
      "2019-01-26T23:36:22.782520: step 1667, loss 0.00208341, acc 1\n",
      "2019-01-26T23:36:23.191180: step 1668, loss 0.0028841, acc 1\n",
      "2019-01-26T23:36:23.609963: step 1669, loss 0.00546625, acc 1\n",
      "2019-01-26T23:36:24.043337: step 1670, loss 0.00165822, acc 1\n",
      "2019-01-26T23:36:24.471556: step 1671, loss 0.00173061, acc 1\n",
      "2019-01-26T23:36:24.909954: step 1672, loss 0.0574308, acc 0.984375\n",
      "2019-01-26T23:36:25.359166: step 1673, loss 0.0183828, acc 0.984375\n",
      "2019-01-26T23:36:25.805191: step 1674, loss 0.0128098, acc 0.984375\n",
      "2019-01-26T23:36:26.256313: step 1675, loss 0.00555045, acc 1\n",
      "2019-01-26T23:36:26.699645: step 1676, loss 0.00055484, acc 1\n",
      "2019-01-26T23:36:27.166948: step 1677, loss 7.8557e-05, acc 1\n",
      "2019-01-26T23:36:27.636835: step 1678, loss 0.00973444, acc 1\n",
      "2019-01-26T23:36:28.120684: step 1679, loss 0.000261537, acc 1\n",
      "2019-01-26T23:36:28.568119: step 1680, loss 7.01155e-05, acc 1\n",
      "2019-01-26T23:36:29.017708: step 1681, loss 5.81936e-05, acc 1\n",
      "2019-01-26T23:36:29.459598: step 1682, loss 0.00707242, acc 1\n",
      "2019-01-26T23:36:29.907708: step 1683, loss 0.00393602, acc 1\n",
      "2019-01-26T23:36:30.353047: step 1684, loss 0.00117151, acc 1\n",
      "2019-01-26T23:36:30.799684: step 1685, loss 2.90473e-05, acc 1\n",
      "2019-01-26T23:36:31.248325: step 1686, loss 0.000454187, acc 1\n",
      "2019-01-26T23:36:31.697211: step 1687, loss 0.0181439, acc 0.984375\n",
      "2019-01-26T23:36:32.143089: step 1688, loss 0.000234703, acc 1\n",
      "2019-01-26T23:36:32.588320: step 1689, loss 0.00100225, acc 1\n",
      "2019-01-26T23:36:33.033739: step 1690, loss 0.000229379, acc 1\n",
      "2019-01-26T23:36:33.464587: step 1691, loss 0.0103028, acc 1\n",
      "2019-01-26T23:36:33.894985: step 1692, loss 0.00335802, acc 1\n",
      "2019-01-26T23:36:34.327428: step 1693, loss 0.000731363, acc 1\n",
      "2019-01-26T23:36:34.760461: step 1694, loss 7.15008e-05, acc 1\n",
      "2019-01-26T23:36:35.186274: step 1695, loss 0.000864773, acc 1\n",
      "2019-01-26T23:36:35.621038: step 1696, loss 0.0002727, acc 1\n",
      "2019-01-26T23:36:36.050242: step 1697, loss 0.00188292, acc 1\n",
      "2019-01-26T23:36:36.481849: step 1698, loss 0.000742931, acc 1\n",
      "2019-01-26T23:36:36.904918: step 1699, loss 0.00114664, acc 1\n",
      "2019-01-26T23:36:37.322225: step 1700, loss 0.000103713, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:36:38.356390: step 1700, loss 0.0661369, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1700\n",
      "\n",
      "2019-01-26T23:36:38.853426: step 1701, loss 0.000390336, acc 1\n",
      "2019-01-26T23:36:39.272048: step 1702, loss 0.000616642, acc 1\n",
      "2019-01-26T23:36:39.690417: step 1703, loss 0.0153875, acc 0.984375\n",
      "2019-01-26T23:36:40.094189: step 1704, loss 0.000135242, acc 1\n",
      "2019-01-26T23:36:40.497264: step 1705, loss 0.00320243, acc 1\n",
      "2019-01-26T23:36:40.904477: step 1706, loss 0.000375387, acc 1\n",
      "2019-01-26T23:36:41.314431: step 1707, loss 0.00123933, acc 1\n",
      "2019-01-26T23:36:41.719076: step 1708, loss 0.000378192, acc 1\n",
      "2019-01-26T23:36:42.134477: step 1709, loss 0.000252635, acc 1\n",
      "2019-01-26T23:36:42.551841: step 1710, loss 0.00439478, acc 1\n",
      "2019-01-26T23:36:43.009630: step 1711, loss 0.00157636, acc 1\n",
      "2019-01-26T23:36:43.425053: step 1712, loss 0.00170257, acc 1\n",
      "2019-01-26T23:36:43.841300: step 1713, loss 0.00330294, acc 1\n",
      "2019-01-26T23:36:44.260387: step 1714, loss 0.000236919, acc 1\n",
      "2019-01-26T23:36:44.677701: step 1715, loss 4.12012e-05, acc 1\n",
      "2019-01-26T23:36:45.098673: step 1716, loss 0.00024536, acc 1\n",
      "2019-01-26T23:36:45.517502: step 1717, loss 0.000103965, acc 1\n",
      "2019-01-26T23:36:45.946375: step 1718, loss 0.00152267, acc 1\n",
      "2019-01-26T23:36:46.377849: step 1719, loss 6.05541e-05, acc 1\n",
      "2019-01-26T23:36:46.809587: step 1720, loss 0.000603124, acc 1\n",
      "2019-01-26T23:36:47.254237: step 1721, loss 0.000294451, acc 1\n",
      "2019-01-26T23:36:47.709692: step 1722, loss 0.00964501, acc 1\n",
      "2019-01-26T23:36:48.163529: step 1723, loss 7.2571e-05, acc 1\n",
      "2019-01-26T23:36:48.618337: step 1724, loss 0.000983421, acc 1\n",
      "2019-01-26T23:36:49.071383: step 1725, loss 0.0014321, acc 1\n",
      "2019-01-26T23:36:49.528363: step 1726, loss 0.00096648, acc 1\n",
      "2019-01-26T23:36:50.000136: step 1727, loss 0.000252267, acc 1\n",
      "2019-01-26T23:36:50.452552: step 1728, loss 4.15711e-05, acc 1\n",
      "2019-01-26T23:36:50.911613: step 1729, loss 6.75068e-05, acc 1\n",
      "2019-01-26T23:36:51.360927: step 1730, loss 0.00110157, acc 1\n",
      "2019-01-26T23:36:51.811305: step 1731, loss 0.000225284, acc 1\n",
      "2019-01-26T23:36:52.265087: step 1732, loss 0.000120017, acc 1\n",
      "2019-01-26T23:36:52.800890: step 1733, loss 0.000821613, acc 1\n",
      "2019-01-26T23:36:53.273777: step 1734, loss 0.0027708, acc 1\n",
      "2019-01-26T23:36:53.728491: step 1735, loss 8.58664e-05, acc 1\n",
      "2019-01-26T23:36:54.176575: step 1736, loss 9.76849e-06, acc 1\n",
      "2019-01-26T23:36:54.628563: step 1737, loss 0.000237117, acc 1\n",
      "2019-01-26T23:36:54.819711: step 1738, loss 0.0295725, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:36:55.255599: step 1739, loss 0.000141229, acc 1\n",
      "2019-01-26T23:36:55.683879: step 1740, loss 1.42257e-05, acc 1\n",
      "2019-01-26T23:36:56.109231: step 1741, loss 0.000273862, acc 1\n",
      "2019-01-26T23:36:56.538425: step 1742, loss 0.000723389, acc 1\n",
      "2019-01-26T23:36:56.967110: step 1743, loss 8.62915e-05, acc 1\n",
      "2019-01-26T23:36:57.399458: step 1744, loss 9.52267e-05, acc 1\n",
      "2019-01-26T23:36:57.818927: step 1745, loss 0.000146868, acc 1\n",
      "2019-01-26T23:36:58.249867: step 1746, loss 0.000816327, acc 1\n",
      "2019-01-26T23:36:58.678014: step 1747, loss 0.00144547, acc 1\n",
      "2019-01-26T23:36:59.100774: step 1748, loss 0.010069, acc 1\n",
      "2019-01-26T23:36:59.539092: step 1749, loss 0.000893333, acc 1\n",
      "2019-01-26T23:36:59.985501: step 1750, loss 5.6433e-05, acc 1\n",
      "2019-01-26T23:37:00.424767: step 1751, loss 0.000295906, acc 1\n",
      "2019-01-26T23:37:00.906750: step 1752, loss 0.00019965, acc 1\n",
      "2019-01-26T23:37:01.347685: step 1753, loss 9.44887e-05, acc 1\n",
      "2019-01-26T23:37:01.792895: step 1754, loss 0.000113585, acc 1\n",
      "2019-01-26T23:37:02.227336: step 1755, loss 7.72747e-05, acc 1\n",
      "2019-01-26T23:37:02.656620: step 1756, loss 0.000289749, acc 1\n",
      "2019-01-26T23:37:03.077723: step 1757, loss 0.00231315, acc 1\n",
      "2019-01-26T23:37:03.503209: step 1758, loss 2.6985e-05, acc 1\n",
      "2019-01-26T23:37:03.934781: step 1759, loss 1.34661e-05, acc 1\n",
      "2019-01-26T23:37:04.365936: step 1760, loss 0.000132563, acc 1\n",
      "2019-01-26T23:37:04.804262: step 1761, loss 7.64021e-05, acc 1\n",
      "2019-01-26T23:37:05.238135: step 1762, loss 7.31482e-05, acc 1\n",
      "2019-01-26T23:37:05.672700: step 1763, loss 0.000707521, acc 1\n",
      "2019-01-26T23:37:06.093882: step 1764, loss 0.000891896, acc 1\n",
      "2019-01-26T23:37:06.517545: step 1765, loss 8.0549e-05, acc 1\n",
      "2019-01-26T23:37:06.934323: step 1766, loss 0.000255952, acc 1\n",
      "2019-01-26T23:37:07.349138: step 1767, loss 0.000255478, acc 1\n",
      "2019-01-26T23:37:07.762895: step 1768, loss 0.000485237, acc 1\n",
      "2019-01-26T23:37:08.189406: step 1769, loss 0.0525661, acc 0.984375\n",
      "2019-01-26T23:37:08.609320: step 1770, loss 3.47132e-05, acc 1\n",
      "2019-01-26T23:37:09.020985: step 1771, loss 0.000287216, acc 1\n",
      "2019-01-26T23:37:09.423985: step 1772, loss 0.000488508, acc 1\n",
      "2019-01-26T23:37:09.834989: step 1773, loss 0.000356563, acc 1\n",
      "2019-01-26T23:37:10.257955: step 1774, loss 9.11586e-05, acc 1\n",
      "2019-01-26T23:37:10.675233: step 1775, loss 0.00312729, acc 1\n",
      "2019-01-26T23:37:11.084147: step 1776, loss 9.99805e-05, acc 1\n",
      "2019-01-26T23:37:11.511071: step 1777, loss 8.13519e-05, acc 1\n",
      "2019-01-26T23:37:11.929990: step 1778, loss 0.000225361, acc 1\n",
      "2019-01-26T23:37:12.342413: step 1779, loss 3.46992e-05, acc 1\n",
      "2019-01-26T23:37:12.759425: step 1780, loss 2.13301e-05, acc 1\n",
      "2019-01-26T23:37:13.181194: step 1781, loss 0.00509509, acc 1\n",
      "2019-01-26T23:37:13.600356: step 1782, loss 0.00129284, acc 1\n",
      "2019-01-26T23:37:14.023491: step 1783, loss 0.000865574, acc 1\n",
      "2019-01-26T23:37:14.450336: step 1784, loss 0.0301251, acc 0.984375\n",
      "2019-01-26T23:37:14.881420: step 1785, loss 9.4092e-05, acc 1\n",
      "2019-01-26T23:37:15.321283: step 1786, loss 0.000757959, acc 1\n",
      "2019-01-26T23:37:15.769612: step 1787, loss 0.000515493, acc 1\n",
      "2019-01-26T23:37:16.211140: step 1788, loss 0.00124195, acc 1\n",
      "2019-01-26T23:37:16.660912: step 1789, loss 0.000918304, acc 1\n",
      "2019-01-26T23:37:17.113635: step 1790, loss 0.00011371, acc 1\n",
      "2019-01-26T23:37:17.573189: step 1791, loss 2.66624e-05, acc 1\n",
      "2019-01-26T23:37:18.036796: step 1792, loss 0.000520681, acc 1\n",
      "2019-01-26T23:37:18.480752: step 1793, loss 0.000173028, acc 1\n",
      "2019-01-26T23:37:18.930165: step 1794, loss 0.000165777, acc 1\n",
      "2019-01-26T23:37:19.371520: step 1795, loss 0.000305252, acc 1\n",
      "2019-01-26T23:37:19.818528: step 1796, loss 0.000462288, acc 1\n",
      "2019-01-26T23:37:20.269359: step 1797, loss 0.000338762, acc 1\n",
      "2019-01-26T23:37:20.714930: step 1798, loss 4.84713e-05, acc 1\n",
      "2019-01-26T23:37:21.161917: step 1799, loss 0.00897235, acc 1\n",
      "2019-01-26T23:37:21.612993: step 1800, loss 0.000154155, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:37:22.725814: step 1800, loss 0.0711612, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1800\n",
      "\n",
      "2019-01-26T23:37:23.248802: step 1801, loss 0.000377719, acc 1\n",
      "2019-01-26T23:37:23.724544: step 1802, loss 0.000176306, acc 1\n",
      "2019-01-26T23:37:24.152137: step 1803, loss 2.00394e-05, acc 1\n",
      "2019-01-26T23:37:24.581504: step 1804, loss 3.29306e-05, acc 1\n",
      "2019-01-26T23:37:25.004436: step 1805, loss 3.48298e-05, acc 1\n",
      "2019-01-26T23:37:25.428662: step 1806, loss 0.0016594, acc 1\n",
      "2019-01-26T23:37:25.862557: step 1807, loss 0.000206495, acc 1\n",
      "2019-01-26T23:37:26.292615: step 1808, loss 0.0262658, acc 0.984375\n",
      "2019-01-26T23:37:26.718159: step 1809, loss 5.42572e-05, acc 1\n",
      "2019-01-26T23:37:27.164983: step 1810, loss 5.39575e-05, acc 1\n",
      "2019-01-26T23:37:27.605100: step 1811, loss 0.000114671, acc 1\n",
      "2019-01-26T23:37:28.053156: step 1812, loss 0.000207675, acc 1\n",
      "2019-01-26T23:37:28.494278: step 1813, loss 0.000474543, acc 1\n",
      "2019-01-26T23:37:28.939948: step 1814, loss 0.000163504, acc 1\n",
      "2019-01-26T23:37:29.385725: step 1815, loss 0.00143031, acc 1\n",
      "2019-01-26T23:37:29.835218: step 1816, loss 0.000243521, acc 1\n",
      "2019-01-26T23:37:30.016802: step 1817, loss 0.000133612, acc 1\n",
      "2019-01-26T23:37:30.445974: step 1818, loss 1.31271e-05, acc 1\n",
      "2019-01-26T23:37:30.878832: step 1819, loss 1.05264e-05, acc 1\n",
      "2019-01-26T23:37:31.303511: step 1820, loss 0.000128731, acc 1\n",
      "2019-01-26T23:37:31.742179: step 1821, loss 0.000290025, acc 1\n",
      "2019-01-26T23:37:32.167262: step 1822, loss 0.000132882, acc 1\n",
      "2019-01-26T23:37:32.600079: step 1823, loss 0.000939258, acc 1\n",
      "2019-01-26T23:37:33.035218: step 1824, loss 6.52765e-05, acc 1\n",
      "2019-01-26T23:37:33.465610: step 1825, loss 0.000907516, acc 1\n",
      "2019-01-26T23:37:33.898814: step 1826, loss 6.35906e-05, acc 1\n",
      "2019-01-26T23:37:34.333383: step 1827, loss 0.000204383, acc 1\n",
      "2019-01-26T23:37:34.764510: step 1828, loss 0.00054364, acc 1\n",
      "2019-01-26T23:37:35.199830: step 1829, loss 0.000250777, acc 1\n",
      "2019-01-26T23:37:35.631460: step 1830, loss 0.044342, acc 0.984375\n",
      "2019-01-26T23:37:36.066138: step 1831, loss 3.30975e-05, acc 1\n",
      "2019-01-26T23:37:36.504318: step 1832, loss 0.000192472, acc 1\n",
      "2019-01-26T23:37:36.959045: step 1833, loss 0.00194056, acc 1\n",
      "2019-01-26T23:37:37.403764: step 1834, loss 0.000103369, acc 1\n",
      "2019-01-26T23:37:37.884500: step 1835, loss 4.48999e-05, acc 1\n",
      "2019-01-26T23:37:38.315227: step 1836, loss 0.0721374, acc 0.984375\n",
      "2019-01-26T23:37:38.758935: step 1837, loss 5.58357e-05, acc 1\n",
      "2019-01-26T23:37:39.186856: step 1838, loss 0.000537315, acc 1\n",
      "2019-01-26T23:37:39.623554: step 1839, loss 0.00308573, acc 1\n",
      "2019-01-26T23:37:40.060170: step 1840, loss 8.72894e-05, acc 1\n",
      "2019-01-26T23:37:40.495516: step 1841, loss 7.70353e-05, acc 1\n",
      "2019-01-26T23:37:40.933080: step 1842, loss 5.2608e-05, acc 1\n",
      "2019-01-26T23:37:41.362957: step 1843, loss 7.88645e-05, acc 1\n",
      "2019-01-26T23:37:41.798080: step 1844, loss 5.55985e-06, acc 1\n",
      "2019-01-26T23:37:42.214613: step 1845, loss 0.00016837, acc 1\n",
      "2019-01-26T23:37:42.635500: step 1846, loss 0.000106023, acc 1\n",
      "2019-01-26T23:37:43.063453: step 1847, loss 0.00252596, acc 1\n",
      "2019-01-26T23:37:43.478629: step 1848, loss 0.000158654, acc 1\n",
      "2019-01-26T23:37:43.895003: step 1849, loss 6.5711e-05, acc 1\n",
      "2019-01-26T23:37:44.311448: step 1850, loss 7.47057e-05, acc 1\n",
      "2019-01-26T23:37:44.732252: step 1851, loss 0.00211287, acc 1\n",
      "2019-01-26T23:37:45.151490: step 1852, loss 0.00267003, acc 1\n",
      "2019-01-26T23:37:45.562155: step 1853, loss 8.32993e-05, acc 1\n",
      "2019-01-26T23:37:45.969895: step 1854, loss 0.00226034, acc 1\n",
      "2019-01-26T23:37:46.384096: step 1855, loss 0.000140948, acc 1\n",
      "2019-01-26T23:37:46.809616: step 1856, loss 0.000262572, acc 1\n",
      "2019-01-26T23:37:47.226983: step 1857, loss 0.000928862, acc 1\n",
      "2019-01-26T23:37:47.650989: step 1858, loss 0.000312704, acc 1\n",
      "2019-01-26T23:37:48.066354: step 1859, loss 0.0100916, acc 1\n",
      "2019-01-26T23:37:48.481407: step 1860, loss 0.000465728, acc 1\n",
      "2019-01-26T23:37:48.905237: step 1861, loss 5.65832e-05, acc 1\n",
      "2019-01-26T23:37:49.323763: step 1862, loss 0.000184256, acc 1\n",
      "2019-01-26T23:37:49.793605: step 1863, loss 4.7172e-05, acc 1\n",
      "2019-01-26T23:37:50.215624: step 1864, loss 0.00170098, acc 1\n",
      "2019-01-26T23:37:50.654166: step 1865, loss 0.000756246, acc 1\n",
      "2019-01-26T23:37:51.095396: step 1866, loss 0.000122049, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:37:51.547735: step 1867, loss 5.35873e-05, acc 1\n",
      "2019-01-26T23:37:52.000854: step 1868, loss 0.000115628, acc 1\n",
      "2019-01-26T23:37:52.439989: step 1869, loss 9.03893e-05, acc 1\n",
      "2019-01-26T23:37:52.883489: step 1870, loss 0.00723556, acc 1\n",
      "2019-01-26T23:37:53.336165: step 1871, loss 0.000233715, acc 1\n",
      "2019-01-26T23:37:53.793859: step 1872, loss 0.000276142, acc 1\n",
      "2019-01-26T23:37:54.244967: step 1873, loss 0.00012129, acc 1\n",
      "2019-01-26T23:37:54.698010: step 1874, loss 0.000212121, acc 1\n",
      "2019-01-26T23:37:55.141601: step 1875, loss 0.000250996, acc 1\n",
      "2019-01-26T23:37:55.580254: step 1876, loss 4.34444e-05, acc 1\n",
      "2019-01-26T23:37:56.016973: step 1877, loss 0.000123108, acc 1\n",
      "2019-01-26T23:37:56.465413: step 1878, loss 3.97463e-05, acc 1\n",
      "2019-01-26T23:37:56.910994: step 1879, loss 0.00332944, acc 1\n",
      "2019-01-26T23:37:57.350832: step 1880, loss 7.31067e-06, acc 1\n",
      "2019-01-26T23:37:57.798565: step 1881, loss 1.78016e-05, acc 1\n",
      "2019-01-26T23:37:58.243201: step 1882, loss 0.000610899, acc 1\n",
      "2019-01-26T23:37:58.689328: step 1883, loss 0.000278088, acc 1\n",
      "2019-01-26T23:37:59.130924: step 1884, loss 0.000175162, acc 1\n",
      "2019-01-26T23:37:59.569064: step 1885, loss 0.00528773, acc 1\n",
      "2019-01-26T23:37:59.998350: step 1886, loss 3.87342e-05, acc 1\n",
      "2019-01-26T23:38:00.426349: step 1887, loss 0.000203981, acc 1\n",
      "2019-01-26T23:38:00.856635: step 1888, loss 0.000134727, acc 1\n",
      "2019-01-26T23:38:01.282116: step 1889, loss 0.000260063, acc 1\n",
      "2019-01-26T23:38:01.712548: step 1890, loss 2.47753e-05, acc 1\n",
      "2019-01-26T23:38:02.140604: step 1891, loss 3.63059e-05, acc 1\n",
      "2019-01-26T23:38:02.572110: step 1892, loss 0.0220399, acc 0.984375\n",
      "2019-01-26T23:38:03.036232: step 1893, loss 7.07737e-05, acc 1\n",
      "2019-01-26T23:38:03.452324: step 1894, loss 1.73371e-05, acc 1\n",
      "2019-01-26T23:38:03.870707: step 1895, loss 0.00601247, acc 1\n",
      "2019-01-26T23:38:04.049769: step 1896, loss 0.000423042, acc 1\n",
      "2019-01-26T23:38:04.463991: step 1897, loss 0.000156533, acc 1\n",
      "2019-01-26T23:38:04.877507: step 1898, loss 0.000861736, acc 1\n",
      "2019-01-26T23:38:05.291805: step 1899, loss 0.00102724, acc 1\n",
      "2019-01-26T23:38:05.703129: step 1900, loss 0.00078808, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:38:06.698842: step 1900, loss 0.0706199, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-1900\n",
      "\n",
      "2019-01-26T23:38:07.179928: step 1901, loss 0.000111962, acc 1\n",
      "2019-01-26T23:38:07.579589: step 1902, loss 0.00295076, acc 1\n",
      "2019-01-26T23:38:07.985165: step 1903, loss 0.000392373, acc 1\n",
      "2019-01-26T23:38:08.381852: step 1904, loss 0.00787774, acc 1\n",
      "2019-01-26T23:38:08.785888: step 1905, loss 0.0119432, acc 0.984375\n",
      "2019-01-26T23:38:09.183650: step 1906, loss 0.00268963, acc 1\n",
      "2019-01-26T23:38:09.586642: step 1907, loss 0.00119513, acc 1\n",
      "2019-01-26T23:38:09.989699: step 1908, loss 6.04332e-05, acc 1\n",
      "2019-01-26T23:38:10.395742: step 1909, loss 0.000980802, acc 1\n",
      "2019-01-26T23:38:10.797283: step 1910, loss 0.000260419, acc 1\n",
      "2019-01-26T23:38:11.196609: step 1911, loss 8.03025e-05, acc 1\n",
      "2019-01-26T23:38:11.613557: step 1912, loss 2.41923e-05, acc 1\n",
      "2019-01-26T23:38:12.012858: step 1913, loss 5.25623e-06, acc 1\n",
      "2019-01-26T23:38:12.417108: step 1914, loss 0.000327803, acc 1\n",
      "2019-01-26T23:38:12.824047: step 1915, loss 6.8925e-05, acc 1\n",
      "2019-01-26T23:38:13.227628: step 1916, loss 5.15989e-05, acc 1\n",
      "2019-01-26T23:38:13.637427: step 1917, loss 0.00151055, acc 1\n",
      "2019-01-26T23:38:14.041454: step 1918, loss 0.0198448, acc 0.984375\n",
      "2019-01-26T23:38:14.454253: step 1919, loss 0.0172198, acc 0.984375\n",
      "2019-01-26T23:38:14.870154: step 1920, loss 5.04056e-05, acc 1\n",
      "2019-01-26T23:38:15.282225: step 1921, loss 7.80257e-05, acc 1\n",
      "2019-01-26T23:38:15.710702: step 1922, loss 0.000502341, acc 1\n",
      "2019-01-26T23:38:16.139863: step 1923, loss 0.00211417, acc 1\n",
      "2019-01-26T23:38:16.580372: step 1924, loss 0.000672096, acc 1\n",
      "2019-01-26T23:38:17.034894: step 1925, loss 0.00134273, acc 1\n",
      "2019-01-26T23:38:17.468932: step 1926, loss 2.42685e-05, acc 1\n",
      "2019-01-26T23:38:17.901489: step 1927, loss 4.52949e-05, acc 1\n",
      "2019-01-26T23:38:18.339799: step 1928, loss 7.78033e-05, acc 1\n",
      "2019-01-26T23:38:18.789716: step 1929, loss 0.00091741, acc 1\n",
      "2019-01-26T23:38:19.251343: step 1930, loss 0.0289905, acc 0.984375\n",
      "2019-01-26T23:38:19.721855: step 1931, loss 0.000178215, acc 1\n",
      "2019-01-26T23:38:20.182774: step 1932, loss 4.59485e-05, acc 1\n",
      "2019-01-26T23:38:20.661005: step 1933, loss 0.000419604, acc 1\n",
      "2019-01-26T23:38:21.143941: step 1934, loss 0.000169283, acc 1\n",
      "2019-01-26T23:38:21.624587: step 1935, loss 0.000166514, acc 1\n",
      "2019-01-26T23:38:22.115718: step 1936, loss 0.000238906, acc 1\n",
      "2019-01-26T23:38:22.601844: step 1937, loss 0.000698287, acc 1\n",
      "2019-01-26T23:38:23.088288: step 1938, loss 0.000271056, acc 1\n",
      "2019-01-26T23:38:23.565146: step 1939, loss 0.000526552, acc 1\n",
      "2019-01-26T23:38:24.050487: step 1940, loss 3.83724e-05, acc 1\n",
      "2019-01-26T23:38:24.537254: step 1941, loss 0.00245151, acc 1\n",
      "2019-01-26T23:38:25.021735: step 1942, loss 0.000113437, acc 1\n",
      "2019-01-26T23:38:25.504626: step 1943, loss 0.00552823, acc 1\n",
      "2019-01-26T23:38:25.979089: step 1944, loss 0.00220313, acc 1\n",
      "2019-01-26T23:38:26.439671: step 1945, loss 0.000157355, acc 1\n",
      "2019-01-26T23:38:26.901643: step 1946, loss 3.00349e-05, acc 1\n",
      "2019-01-26T23:38:27.388618: step 1947, loss 0.00116624, acc 1\n",
      "2019-01-26T23:38:27.864006: step 1948, loss 0.000161182, acc 1\n",
      "2019-01-26T23:38:28.333797: step 1949, loss 0.000265271, acc 1\n",
      "2019-01-26T23:38:28.798964: step 1950, loss 9.69866e-05, acc 1\n",
      "2019-01-26T23:38:29.248981: step 1951, loss 0.000329327, acc 1\n",
      "2019-01-26T23:38:29.698709: step 1952, loss 0.00114361, acc 1\n",
      "2019-01-26T23:38:30.148847: step 1953, loss 0.00104857, acc 1\n",
      "2019-01-26T23:38:30.611592: step 1954, loss 0.000841374, acc 1\n",
      "2019-01-26T23:38:31.069834: step 1955, loss 6.08154e-05, acc 1\n",
      "2019-01-26T23:38:31.516641: step 1956, loss 0.00120555, acc 1\n",
      "2019-01-26T23:38:31.953943: step 1957, loss 4.34213e-05, acc 1\n",
      "2019-01-26T23:38:32.387315: step 1958, loss 8.50442e-05, acc 1\n",
      "2019-01-26T23:38:32.823909: step 1959, loss 4.77033e-05, acc 1\n",
      "2019-01-26T23:38:33.253200: step 1960, loss 0.000648028, acc 1\n",
      "2019-01-26T23:38:33.688281: step 1961, loss 0.000309404, acc 1\n",
      "2019-01-26T23:38:34.121813: step 1962, loss 7.95615e-05, acc 1\n",
      "2019-01-26T23:38:34.552624: step 1963, loss 0.000181172, acc 1\n",
      "2019-01-26T23:38:34.984140: step 1964, loss 8.66872e-05, acc 1\n",
      "2019-01-26T23:38:35.421575: step 1965, loss 0.00043491, acc 1\n",
      "2019-01-26T23:38:35.868775: step 1966, loss 0.000487206, acc 1\n",
      "2019-01-26T23:38:36.322543: step 1967, loss 8.38952e-05, acc 1\n",
      "2019-01-26T23:38:36.767881: step 1968, loss 0.000259097, acc 1\n",
      "2019-01-26T23:38:37.207227: step 1969, loss 0.00025857, acc 1\n",
      "2019-01-26T23:38:37.658003: step 1970, loss 0.000174, acc 1\n",
      "2019-01-26T23:38:38.103526: step 1971, loss 8.65217e-06, acc 1\n",
      "2019-01-26T23:38:38.554196: step 1972, loss 9.59917e-05, acc 1\n",
      "2019-01-26T23:38:39.008547: step 1973, loss 0.0192833, acc 0.984375\n",
      "2019-01-26T23:38:39.469203: step 1974, loss 2.85693e-05, acc 1\n",
      "2019-01-26T23:38:39.670289: step 1975, loss 1.44181e-05, acc 1\n",
      "2019-01-26T23:38:40.130940: step 1976, loss 0.000928694, acc 1\n",
      "2019-01-26T23:38:40.612238: step 1977, loss 0.000300071, acc 1\n",
      "2019-01-26T23:38:41.103389: step 1978, loss 5.08294e-06, acc 1\n",
      "2019-01-26T23:38:41.586605: step 1979, loss 0.000976363, acc 1\n",
      "2019-01-26T23:38:42.071040: step 1980, loss 6.31154e-05, acc 1\n",
      "2019-01-26T23:38:42.555665: step 1981, loss 0.000124365, acc 1\n",
      "2019-01-26T23:38:43.037914: step 1982, loss 4.16814e-05, acc 1\n",
      "2019-01-26T23:38:43.530105: step 1983, loss 0.001054, acc 1\n",
      "2019-01-26T23:38:44.017942: step 1984, loss 0.000187696, acc 1\n",
      "2019-01-26T23:38:44.498182: step 1985, loss 0.000143752, acc 1\n",
      "2019-01-26T23:38:44.985597: step 1986, loss 0.0106438, acc 1\n",
      "2019-01-26T23:38:45.469705: step 1987, loss 0.000343593, acc 1\n",
      "2019-01-26T23:38:45.944453: step 1988, loss 5.69154e-05, acc 1\n",
      "2019-01-26T23:38:46.412299: step 1989, loss 9.31575e-05, acc 1\n",
      "2019-01-26T23:38:46.884109: step 1990, loss 0.000312121, acc 1\n",
      "2019-01-26T23:38:47.347118: step 1991, loss 4.03727e-05, acc 1\n",
      "2019-01-26T23:38:47.812872: step 1992, loss 0.000110448, acc 1\n",
      "2019-01-26T23:38:48.278147: step 1993, loss 7.76468e-05, acc 1\n",
      "2019-01-26T23:38:48.738983: step 1994, loss 0.00496545, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:38:49.195033: step 1995, loss 9.61148e-05, acc 1\n",
      "2019-01-26T23:38:49.645093: step 1996, loss 0.000396607, acc 1\n",
      "2019-01-26T23:38:50.090468: step 1997, loss 5.95928e-05, acc 1\n",
      "2019-01-26T23:38:50.537292: step 1998, loss 0.000551402, acc 1\n",
      "2019-01-26T23:38:51.027136: step 1999, loss 7.03104e-05, acc 1\n",
      "2019-01-26T23:38:51.467819: step 2000, loss 3.87395e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:38:52.534576: step 2000, loss 0.0750271, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2000\n",
      "\n",
      "2019-01-26T23:38:53.045947: step 2001, loss 1.58814e-05, acc 1\n",
      "2019-01-26T23:38:53.478797: step 2002, loss 0.000789504, acc 1\n",
      "2019-01-26T23:38:53.899809: step 2003, loss 4.26161e-06, acc 1\n",
      "2019-01-26T23:38:54.328500: step 2004, loss 0.00703007, acc 1\n",
      "2019-01-26T23:38:54.753266: step 2005, loss 4.18305e-05, acc 1\n",
      "2019-01-26T23:38:55.167263: step 2006, loss 0.0027153, acc 1\n",
      "2019-01-26T23:38:55.577101: step 2007, loss 8.53235e-05, acc 1\n",
      "2019-01-26T23:38:55.986919: step 2008, loss 0.000138654, acc 1\n",
      "2019-01-26T23:38:56.393994: step 2009, loss 5.34728e-05, acc 1\n",
      "2019-01-26T23:38:56.809259: step 2010, loss 0.0144404, acc 0.984375\n",
      "2019-01-26T23:38:57.223237: step 2011, loss 0.0404434, acc 0.984375\n",
      "2019-01-26T23:38:57.644427: step 2012, loss 0.0121312, acc 0.984375\n",
      "2019-01-26T23:38:58.052288: step 2013, loss 6.30683e-05, acc 1\n",
      "2019-01-26T23:38:58.468602: step 2014, loss 7.01683e-05, acc 1\n",
      "2019-01-26T23:38:58.907765: step 2015, loss 4.15909e-05, acc 1\n",
      "2019-01-26T23:38:59.383414: step 2016, loss 0.000704314, acc 1\n",
      "2019-01-26T23:38:59.796812: step 2017, loss 0.00245874, acc 1\n",
      "2019-01-26T23:39:00.210948: step 2018, loss 0.000214326, acc 1\n",
      "2019-01-26T23:39:00.621555: step 2019, loss 1.9162e-05, acc 1\n",
      "2019-01-26T23:39:01.028187: step 2020, loss 0.00105554, acc 1\n",
      "2019-01-26T23:39:01.456124: step 2021, loss 8.69587e-05, acc 1\n",
      "2019-01-26T23:39:01.924450: step 2022, loss 0.000107832, acc 1\n",
      "2019-01-26T23:39:02.346727: step 2023, loss 0.0013658, acc 1\n",
      "2019-01-26T23:39:02.778671: step 2024, loss 0.00358619, acc 1\n",
      "2019-01-26T23:39:03.208564: step 2025, loss 0.000653098, acc 1\n",
      "2019-01-26T23:39:03.654246: step 2026, loss 0.000322256, acc 1\n",
      "2019-01-26T23:39:04.095965: step 2027, loss 0.00255041, acc 1\n",
      "2019-01-26T23:39:04.541026: step 2028, loss 0.000797605, acc 1\n",
      "2019-01-26T23:39:04.985405: step 2029, loss 9.71954e-05, acc 1\n",
      "2019-01-26T23:39:05.435097: step 2030, loss 0.000115224, acc 1\n",
      "2019-01-26T23:39:05.894922: step 2031, loss 0.000253568, acc 1\n",
      "2019-01-26T23:39:06.349935: step 2032, loss 0.000371901, acc 1\n",
      "2019-01-26T23:39:06.801109: step 2033, loss 0.000747487, acc 1\n",
      "2019-01-26T23:39:07.252259: step 2034, loss 0.000352927, acc 1\n",
      "2019-01-26T23:39:07.727506: step 2035, loss 0.00780949, acc 1\n",
      "2019-01-26T23:39:08.178599: step 2036, loss 0.000638719, acc 1\n",
      "2019-01-26T23:39:08.622903: step 2037, loss 0.000261599, acc 1\n",
      "2019-01-26T23:39:09.069438: step 2038, loss 8.58926e-05, acc 1\n",
      "2019-01-26T23:39:09.510371: step 2039, loss 5.3588e-05, acc 1\n",
      "2019-01-26T23:39:09.963705: step 2040, loss 0.000166952, acc 1\n",
      "2019-01-26T23:39:10.404627: step 2041, loss 0.000520166, acc 1\n",
      "2019-01-26T23:39:10.850402: step 2042, loss 0.000360814, acc 1\n",
      "2019-01-26T23:39:11.295622: step 2043, loss 0.0102731, acc 1\n",
      "2019-01-26T23:39:11.751063: step 2044, loss 0.000134623, acc 1\n",
      "2019-01-26T23:39:12.178944: step 2045, loss 0.000266437, acc 1\n",
      "2019-01-26T23:39:12.619578: step 2046, loss 0.00041375, acc 1\n",
      "2019-01-26T23:39:13.050353: step 2047, loss 0.000564492, acc 1\n",
      "2019-01-26T23:39:13.484241: step 2048, loss 3.03001e-05, acc 1\n",
      "2019-01-26T23:39:13.914055: step 2049, loss 3.41435e-05, acc 1\n",
      "2019-01-26T23:39:14.343514: step 2050, loss 0.00225516, acc 1\n",
      "2019-01-26T23:39:14.774625: step 2051, loss 0.00132004, acc 1\n",
      "2019-01-26T23:39:15.196726: step 2052, loss 0.00311739, acc 1\n",
      "2019-01-26T23:39:15.630597: step 2053, loss 6.97324e-05, acc 1\n",
      "2019-01-26T23:39:15.818526: step 2054, loss 0.000744434, acc 1\n",
      "2019-01-26T23:39:16.251739: step 2055, loss 7.02235e-05, acc 1\n",
      "2019-01-26T23:39:16.690837: step 2056, loss 0.000136108, acc 1\n",
      "2019-01-26T23:39:17.134488: step 2057, loss 0.0732691, acc 0.984375\n",
      "2019-01-26T23:39:17.576614: step 2058, loss 0.00034218, acc 1\n",
      "2019-01-26T23:39:18.027537: step 2059, loss 0.00164115, acc 1\n",
      "2019-01-26T23:39:18.478424: step 2060, loss 7.49293e-06, acc 1\n",
      "2019-01-26T23:39:18.929794: step 2061, loss 0.000128815, acc 1\n",
      "2019-01-26T23:39:19.390810: step 2062, loss 0.000743964, acc 1\n",
      "2019-01-26T23:39:19.865475: step 2063, loss 2.1807e-05, acc 1\n",
      "2019-01-26T23:39:20.335796: step 2064, loss 0.00176289, acc 1\n",
      "2019-01-26T23:39:20.825519: step 2065, loss 7.58931e-05, acc 1\n",
      "2019-01-26T23:39:21.316240: step 2066, loss 0.000957334, acc 1\n",
      "2019-01-26T23:39:21.799994: step 2067, loss 6.71788e-05, acc 1\n",
      "2019-01-26T23:39:22.277835: step 2068, loss 0.000506712, acc 1\n",
      "2019-01-26T23:39:22.765819: step 2069, loss 4.89252e-05, acc 1\n",
      "2019-01-26T23:39:23.249184: step 2070, loss 0.000225241, acc 1\n",
      "2019-01-26T23:39:23.730316: step 2071, loss 0.000308974, acc 1\n",
      "2019-01-26T23:39:24.216860: step 2072, loss 0.000197509, acc 1\n",
      "2019-01-26T23:39:24.751096: step 2073, loss 0.00303504, acc 1\n",
      "2019-01-26T23:39:25.229416: step 2074, loss 0.000659307, acc 1\n",
      "2019-01-26T23:39:25.723967: step 2075, loss 0.00251329, acc 1\n",
      "2019-01-26T23:39:26.191384: step 2076, loss 0.0022164, acc 1\n",
      "2019-01-26T23:39:26.660846: step 2077, loss 0.000395685, acc 1\n",
      "2019-01-26T23:39:27.133746: step 2078, loss 0.000246268, acc 1\n",
      "2019-01-26T23:39:27.593226: step 2079, loss 0.000113943, acc 1\n",
      "2019-01-26T23:39:28.057644: step 2080, loss 0.00143247, acc 1\n",
      "2019-01-26T23:39:28.513342: step 2081, loss 0.00243867, acc 1\n",
      "2019-01-26T23:39:28.963700: step 2082, loss 0.000624201, acc 1\n",
      "2019-01-26T23:39:29.411211: step 2083, loss 0.00058004, acc 1\n",
      "2019-01-26T23:39:29.868755: step 2084, loss 0.0001385, acc 1\n",
      "2019-01-26T23:39:30.314226: step 2085, loss 0.00388944, acc 1\n",
      "2019-01-26T23:39:30.762142: step 2086, loss 0.00396113, acc 1\n",
      "2019-01-26T23:39:31.209107: step 2087, loss 5.29866e-05, acc 1\n",
      "2019-01-26T23:39:31.651711: step 2088, loss 0.00247033, acc 1\n",
      "2019-01-26T23:39:32.092884: step 2089, loss 5.54835e-05, acc 1\n",
      "2019-01-26T23:39:32.524885: step 2090, loss 0.026143, acc 0.984375\n",
      "2019-01-26T23:39:32.960773: step 2091, loss 6.64489e-05, acc 1\n",
      "2019-01-26T23:39:33.388598: step 2092, loss 0.000273978, acc 1\n",
      "2019-01-26T23:39:33.826485: step 2093, loss 7.33281e-06, acc 1\n",
      "2019-01-26T23:39:34.258138: step 2094, loss 7.69013e-05, acc 1\n",
      "2019-01-26T23:39:34.679143: step 2095, loss 0.00108103, acc 1\n",
      "2019-01-26T23:39:35.095901: step 2096, loss 0.000598518, acc 1\n",
      "2019-01-26T23:39:35.517365: step 2097, loss 0.00077887, acc 1\n",
      "2019-01-26T23:39:35.936331: step 2098, loss 0.000967534, acc 1\n",
      "2019-01-26T23:39:36.351770: step 2099, loss 0.000113554, acc 1\n",
      "2019-01-26T23:39:36.779043: step 2100, loss 0.00106001, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:39:37.841806: step 2100, loss 0.0787388, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2100\n",
      "\n",
      "2019-01-26T23:39:38.332381: step 2101, loss 1.93725e-05, acc 1\n",
      "2019-01-26T23:39:38.750660: step 2102, loss 7.57055e-05, acc 1\n",
      "2019-01-26T23:39:39.165571: step 2103, loss 0.000493954, acc 1\n",
      "2019-01-26T23:39:39.583210: step 2104, loss 7.7421e-05, acc 1\n",
      "2019-01-26T23:39:40.003826: step 2105, loss 0.000376001, acc 1\n",
      "2019-01-26T23:39:40.420848: step 2106, loss 0.00175797, acc 1\n",
      "2019-01-26T23:39:40.870087: step 2107, loss 2.49355e-05, acc 1\n",
      "2019-01-26T23:39:41.287121: step 2108, loss 0.000107365, acc 1\n",
      "2019-01-26T23:39:41.708949: step 2109, loss 0.0129094, acc 1\n",
      "2019-01-26T23:39:42.134533: step 2110, loss 7.59637e-05, acc 1\n",
      "2019-01-26T23:39:42.559993: step 2111, loss 0.00156519, acc 1\n",
      "2019-01-26T23:39:42.993255: step 2112, loss 9.8283e-06, acc 1\n",
      "2019-01-26T23:39:43.423958: step 2113, loss 0.000332701, acc 1\n",
      "2019-01-26T23:39:43.876952: step 2114, loss 0.00243163, acc 1\n",
      "2019-01-26T23:39:44.319010: step 2115, loss 0.00151443, acc 1\n",
      "2019-01-26T23:39:44.769440: step 2116, loss 0.000301139, acc 1\n",
      "2019-01-26T23:39:45.217727: step 2117, loss 7.32728e-06, acc 1\n",
      "2019-01-26T23:39:45.677272: step 2118, loss 8.91414e-05, acc 1\n",
      "2019-01-26T23:39:46.145513: step 2119, loss 0.000792414, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:39:46.593012: step 2120, loss 0.000226896, acc 1\n",
      "2019-01-26T23:39:47.045261: step 2121, loss 0.000120566, acc 1\n",
      "2019-01-26T23:39:47.478748: step 2122, loss 0.00696008, acc 1\n",
      "2019-01-26T23:39:47.922834: step 2123, loss 0.000110139, acc 1\n",
      "2019-01-26T23:39:48.361608: step 2124, loss 0.000256485, acc 1\n",
      "2019-01-26T23:39:48.807506: step 2125, loss 0.000897654, acc 1\n",
      "2019-01-26T23:39:49.250350: step 2126, loss 0.000958368, acc 1\n",
      "2019-01-26T23:39:49.761103: step 2127, loss 0.00117456, acc 1\n",
      "2019-01-26T23:39:50.215267: step 2128, loss 0.000743528, acc 1\n",
      "2019-01-26T23:39:50.662335: step 2129, loss 0.000327167, acc 1\n",
      "2019-01-26T23:39:51.106078: step 2130, loss 0.000152457, acc 1\n",
      "2019-01-26T23:39:51.546697: step 2131, loss 0.00305856, acc 1\n",
      "2019-01-26T23:39:52.006261: step 2132, loss 0.000244619, acc 1\n",
      "2019-01-26T23:39:52.194649: step 2133, loss 0.00625765, acc 1\n",
      "2019-01-26T23:39:52.626326: step 2134, loss 0.000720008, acc 1\n",
      "2019-01-26T23:39:53.052969: step 2135, loss 0.000131397, acc 1\n",
      "2019-01-26T23:39:53.480292: step 2136, loss 0.000137198, acc 1\n",
      "2019-01-26T23:39:53.917311: step 2137, loss 5.2615e-05, acc 1\n",
      "2019-01-26T23:39:54.350104: step 2138, loss 0.000127577, acc 1\n",
      "2019-01-26T23:39:54.769359: step 2139, loss 0.00016411, acc 1\n",
      "2019-01-26T23:39:55.177740: step 2140, loss 4.48872e-05, acc 1\n",
      "2019-01-26T23:39:55.592929: step 2141, loss 1.54025e-05, acc 1\n",
      "2019-01-26T23:39:56.009074: step 2142, loss 7.31737e-05, acc 1\n",
      "2019-01-26T23:39:56.422940: step 2143, loss 0.0121804, acc 0.984375\n",
      "2019-01-26T23:39:56.837565: step 2144, loss 3.94295e-05, acc 1\n",
      "2019-01-26T23:39:57.256317: step 2145, loss 1.37782e-05, acc 1\n",
      "2019-01-26T23:39:57.672271: step 2146, loss 1.8215e-05, acc 1\n",
      "2019-01-26T23:39:58.084834: step 2147, loss 8.59185e-05, acc 1\n",
      "2019-01-26T23:39:58.499122: step 2148, loss 0.00265196, acc 1\n",
      "2019-01-26T23:39:58.917511: step 2149, loss 0.0016369, acc 1\n",
      "2019-01-26T23:39:59.332949: step 2150, loss 2.52789e-05, acc 1\n",
      "2019-01-26T23:39:59.751785: step 2151, loss 7.89141e-05, acc 1\n",
      "2019-01-26T23:40:00.175067: step 2152, loss 0.00560752, acc 1\n",
      "2019-01-26T23:40:00.597992: step 2153, loss 0.000649072, acc 1\n",
      "2019-01-26T23:40:01.014961: step 2154, loss 0.000531789, acc 1\n",
      "2019-01-26T23:40:01.428334: step 2155, loss 8.49362e-05, acc 1\n",
      "2019-01-26T23:40:01.846155: step 2156, loss 0.000158837, acc 1\n",
      "2019-01-26T23:40:02.273383: step 2157, loss 0.00814537, acc 1\n",
      "2019-01-26T23:40:02.724603: step 2158, loss 1.74439e-05, acc 1\n",
      "2019-01-26T23:40:03.171952: step 2159, loss 3.76911e-05, acc 1\n",
      "2019-01-26T23:40:03.622715: step 2160, loss 0.000190474, acc 1\n",
      "2019-01-26T23:40:04.066431: step 2161, loss 0.000166286, acc 1\n",
      "2019-01-26T23:40:04.518207: step 2162, loss 1.00517e-05, acc 1\n",
      "2019-01-26T23:40:04.965939: step 2163, loss 0.000279144, acc 1\n",
      "2019-01-26T23:40:05.409230: step 2164, loss 8.88631e-05, acc 1\n",
      "2019-01-26T23:40:05.880109: step 2165, loss 4.58876e-05, acc 1\n",
      "2019-01-26T23:40:06.337664: step 2166, loss 0.000259805, acc 1\n",
      "2019-01-26T23:40:06.787277: step 2167, loss 3.37583e-05, acc 1\n",
      "2019-01-26T23:40:07.226278: step 2168, loss 0.000541472, acc 1\n",
      "2019-01-26T23:40:07.671275: step 2169, loss 3.32838e-05, acc 1\n",
      "2019-01-26T23:40:08.108433: step 2170, loss 0.000154707, acc 1\n",
      "2019-01-26T23:40:08.553507: step 2171, loss 9.71131e-06, acc 1\n",
      "2019-01-26T23:40:08.999889: step 2172, loss 3.6317e-05, acc 1\n",
      "2019-01-26T23:40:09.446027: step 2173, loss 0.00128234, acc 1\n",
      "2019-01-26T23:40:09.893506: step 2174, loss 0.000112432, acc 1\n",
      "2019-01-26T23:40:10.337122: step 2175, loss 0.000119726, acc 1\n",
      "2019-01-26T23:40:10.787565: step 2176, loss 0.000190178, acc 1\n",
      "2019-01-26T23:40:11.233540: step 2177, loss 1.20352e-05, acc 1\n",
      "2019-01-26T23:40:11.684958: step 2178, loss 2.24576e-05, acc 1\n",
      "2019-01-26T23:40:12.108714: step 2179, loss 0.00010501, acc 1\n",
      "2019-01-26T23:40:12.534477: step 2180, loss 0.000302273, acc 1\n",
      "2019-01-26T23:40:12.966792: step 2181, loss 0.000237049, acc 1\n",
      "2019-01-26T23:40:13.393782: step 2182, loss 0.000360146, acc 1\n",
      "2019-01-26T23:40:13.820960: step 2183, loss 3.87292e-05, acc 1\n",
      "2019-01-26T23:40:14.247723: step 2184, loss 2.81977e-05, acc 1\n",
      "2019-01-26T23:40:14.677184: step 2185, loss 0.000302574, acc 1\n",
      "2019-01-26T23:40:15.094148: step 2186, loss 0.00117634, acc 1\n",
      "2019-01-26T23:40:15.516177: step 2187, loss 0.0142905, acc 0.984375\n",
      "2019-01-26T23:40:15.935308: step 2188, loss 6.51404e-05, acc 1\n",
      "2019-01-26T23:40:16.353663: step 2189, loss 0.000748868, acc 1\n",
      "2019-01-26T23:40:16.770010: step 2190, loss 0.000328729, acc 1\n",
      "2019-01-26T23:40:17.188403: step 2191, loss 0.000102733, acc 1\n",
      "2019-01-26T23:40:17.604870: step 2192, loss 0.000147051, acc 1\n",
      "2019-01-26T23:40:18.017397: step 2193, loss 3.78246e-05, acc 1\n",
      "2019-01-26T23:40:18.429902: step 2194, loss 1.06308e-05, acc 1\n",
      "2019-01-26T23:40:18.850468: step 2195, loss 0.000349189, acc 1\n",
      "2019-01-26T23:40:19.263510: step 2196, loss 4.45573e-05, acc 1\n",
      "2019-01-26T23:40:19.677994: step 2197, loss 0.000318942, acc 1\n",
      "2019-01-26T23:40:20.089347: step 2198, loss 0.000119481, acc 1\n",
      "2019-01-26T23:40:20.507811: step 2199, loss 0.00271454, acc 1\n",
      "2019-01-26T23:40:20.960814: step 2200, loss 0.000120279, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:40:22.001817: step 2200, loss 0.0820155, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2200\n",
      "\n",
      "2019-01-26T23:40:22.508812: step 2201, loss 2.28589e-05, acc 1\n",
      "2019-01-26T23:40:22.941474: step 2202, loss 8.44832e-05, acc 1\n",
      "2019-01-26T23:40:23.371733: step 2203, loss 4.43476e-06, acc 1\n",
      "2019-01-26T23:40:23.822501: step 2204, loss 5.85817e-05, acc 1\n",
      "2019-01-26T23:40:24.271814: step 2205, loss 1.84011e-05, acc 1\n",
      "2019-01-26T23:40:24.719650: step 2206, loss 0.00102436, acc 1\n",
      "2019-01-26T23:40:25.167906: step 2207, loss 0.000925757, acc 1\n",
      "2019-01-26T23:40:25.620891: step 2208, loss 4.56976e-05, acc 1\n",
      "2019-01-26T23:40:26.079114: step 2209, loss 0.000122365, acc 1\n",
      "2019-01-26T23:40:26.524634: step 2210, loss 3.83058e-05, acc 1\n",
      "2019-01-26T23:40:26.997365: step 2211, loss 0.00131664, acc 1\n",
      "2019-01-26T23:40:27.192846: step 2212, loss 0.000711562, acc 1\n",
      "2019-01-26T23:40:27.643481: step 2213, loss 4.32256e-05, acc 1\n",
      "2019-01-26T23:40:28.098476: step 2214, loss 4.05892e-05, acc 1\n",
      "2019-01-26T23:40:28.551517: step 2215, loss 0.0111929, acc 0.984375\n",
      "2019-01-26T23:40:28.995725: step 2216, loss 0.000213076, acc 1\n",
      "2019-01-26T23:40:29.434797: step 2217, loss 6.23143e-05, acc 1\n",
      "2019-01-26T23:40:29.884601: step 2218, loss 0.000321559, acc 1\n",
      "2019-01-26T23:40:30.325033: step 2219, loss 5.43596e-05, acc 1\n",
      "2019-01-26T23:40:30.775948: step 2220, loss 8.19994e-05, acc 1\n",
      "2019-01-26T23:40:31.219379: step 2221, loss 2.92278e-05, acc 1\n",
      "2019-01-26T23:40:31.648940: step 2222, loss 0.000304329, acc 1\n",
      "2019-01-26T23:40:32.078860: step 2223, loss 0.000128118, acc 1\n",
      "2019-01-26T23:40:32.517192: step 2224, loss 2.34424e-05, acc 1\n",
      "2019-01-26T23:40:32.951659: step 2225, loss 2.11516e-05, acc 1\n",
      "2019-01-26T23:40:33.379191: step 2226, loss 0.000246753, acc 1\n",
      "2019-01-26T23:40:33.818173: step 2227, loss 0.000131076, acc 1\n",
      "2019-01-26T23:40:34.249390: step 2228, loss 0.000298439, acc 1\n",
      "2019-01-26T23:40:34.680606: step 2229, loss 0.00125275, acc 1\n",
      "2019-01-26T23:40:35.095729: step 2230, loss 0.000139046, acc 1\n",
      "2019-01-26T23:40:35.517462: step 2231, loss 0.00489808, acc 1\n",
      "2019-01-26T23:40:35.943761: step 2232, loss 6.52535e-05, acc 1\n",
      "2019-01-26T23:40:36.359700: step 2233, loss 2.26112e-05, acc 1\n",
      "2019-01-26T23:40:36.785830: step 2234, loss 5.02724e-05, acc 1\n",
      "2019-01-26T23:40:37.204031: step 2235, loss 3.47564e-05, acc 1\n",
      "2019-01-26T23:40:37.621830: step 2236, loss 7.66204e-05, acc 1\n",
      "2019-01-26T23:40:38.035515: step 2237, loss 2.58271e-05, acc 1\n",
      "2019-01-26T23:40:38.449738: step 2238, loss 5.86211e-05, acc 1\n",
      "2019-01-26T23:40:38.875566: step 2239, loss 0.000104417, acc 1\n",
      "2019-01-26T23:40:39.294190: step 2240, loss 2.7963e-05, acc 1\n",
      "2019-01-26T23:40:39.714885: step 2241, loss 0.000534245, acc 1\n",
      "2019-01-26T23:40:40.138500: step 2242, loss 0.000334232, acc 1\n",
      "2019-01-26T23:40:40.566516: step 2243, loss 6.91652e-05, acc 1\n",
      "2019-01-26T23:40:41.015496: step 2244, loss 3.6978e-05, acc 1\n",
      "2019-01-26T23:40:41.427742: step 2245, loss 2.66272e-05, acc 1\n",
      "2019-01-26T23:40:41.847772: step 2246, loss 0.000115849, acc 1\n",
      "2019-01-26T23:40:42.273518: step 2247, loss 3.55642e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:40:42.703261: step 2248, loss 0.000216297, acc 1\n",
      "2019-01-26T23:40:43.137746: step 2249, loss 0.00107631, acc 1\n",
      "2019-01-26T23:40:43.571474: step 2250, loss 0.00144522, acc 1\n",
      "2019-01-26T23:40:44.015173: step 2251, loss 1.23503e-05, acc 1\n",
      "2019-01-26T23:40:44.458382: step 2252, loss 0.00270546, acc 1\n",
      "2019-01-26T23:40:44.898898: step 2253, loss 2.7865e-05, acc 1\n",
      "2019-01-26T23:40:45.343983: step 2254, loss 7.30475e-05, acc 1\n",
      "2019-01-26T23:40:45.810086: step 2255, loss 5.01015e-05, acc 1\n",
      "2019-01-26T23:40:46.268031: step 2256, loss 0.000690662, acc 1\n",
      "2019-01-26T23:40:46.713639: step 2257, loss 0.0333295, acc 0.984375\n",
      "2019-01-26T23:40:47.152938: step 2258, loss 3.48588e-05, acc 1\n",
      "2019-01-26T23:40:47.598651: step 2259, loss 0.000215718, acc 1\n",
      "2019-01-26T23:40:48.047255: step 2260, loss 0.000509427, acc 1\n",
      "2019-01-26T23:40:48.494763: step 2261, loss 3.70553e-05, acc 1\n",
      "2019-01-26T23:40:48.943108: step 2262, loss 0.0027551, acc 1\n",
      "2019-01-26T23:40:49.382580: step 2263, loss 0.000178311, acc 1\n",
      "2019-01-26T23:40:49.827951: step 2264, loss 0.00023517, acc 1\n",
      "2019-01-26T23:40:50.276189: step 2265, loss 0.00922221, acc 1\n",
      "2019-01-26T23:40:50.723540: step 2266, loss 0.00335025, acc 1\n",
      "2019-01-26T23:40:51.162874: step 2267, loss 6.1482e-06, acc 1\n",
      "2019-01-26T23:40:51.599021: step 2268, loss 1.79707e-05, acc 1\n",
      "2019-01-26T23:40:52.022613: step 2269, loss 2.87033e-05, acc 1\n",
      "2019-01-26T23:40:52.446343: step 2270, loss 6.63984e-06, acc 1\n",
      "2019-01-26T23:40:52.878200: step 2271, loss 2.01704e-05, acc 1\n",
      "2019-01-26T23:40:53.312515: step 2272, loss 0.000238184, acc 1\n",
      "2019-01-26T23:40:53.743357: step 2273, loss 0.000115387, acc 1\n",
      "2019-01-26T23:40:54.180822: step 2274, loss 8.51953e-05, acc 1\n",
      "2019-01-26T23:40:54.657779: step 2275, loss 0.00346577, acc 1\n",
      "2019-01-26T23:40:55.093381: step 2276, loss 0.000257016, acc 1\n",
      "2019-01-26T23:40:55.521555: step 2277, loss 0.000224456, acc 1\n",
      "2019-01-26T23:40:55.960637: step 2278, loss 9.49148e-06, acc 1\n",
      "2019-01-26T23:40:56.401318: step 2279, loss 0.00157486, acc 1\n",
      "2019-01-26T23:40:56.848773: step 2280, loss 0.000311958, acc 1\n",
      "2019-01-26T23:40:57.292927: step 2281, loss 6.63099e-07, acc 1\n",
      "2019-01-26T23:40:57.775571: step 2282, loss 8.54688e-05, acc 1\n",
      "2019-01-26T23:40:58.195946: step 2283, loss 0.0755548, acc 0.984375\n",
      "2019-01-26T23:40:58.626331: step 2284, loss 3.96473e-05, acc 1\n",
      "2019-01-26T23:40:59.052636: step 2285, loss 2.50775e-05, acc 1\n",
      "2019-01-26T23:40:59.487394: step 2286, loss 5.82242e-05, acc 1\n",
      "2019-01-26T23:40:59.918915: step 2287, loss 0.000367669, acc 1\n",
      "2019-01-26T23:41:00.346369: step 2288, loss 0.0265389, acc 0.984375\n",
      "2019-01-26T23:41:00.781332: step 2289, loss 0.000335719, acc 1\n",
      "2019-01-26T23:41:01.213531: step 2290, loss 0.000608113, acc 1\n",
      "2019-01-26T23:41:01.399666: step 2291, loss 4.29662e-06, acc 1\n",
      "2019-01-26T23:41:01.840761: step 2292, loss 0.0015984, acc 1\n",
      "2019-01-26T23:41:02.267424: step 2293, loss 0.000184992, acc 1\n",
      "2019-01-26T23:41:02.685228: step 2294, loss 0.00047303, acc 1\n",
      "2019-01-26T23:41:03.129510: step 2295, loss 3.35149e-05, acc 1\n",
      "2019-01-26T23:41:03.540703: step 2296, loss 4.00103e-05, acc 1\n",
      "2019-01-26T23:41:03.961729: step 2297, loss 0.000644123, acc 1\n",
      "2019-01-26T23:41:04.374934: step 2298, loss 6.44161e-05, acc 1\n",
      "2019-01-26T23:41:04.792138: step 2299, loss 0.000890288, acc 1\n",
      "2019-01-26T23:41:05.203253: step 2300, loss 3.76365e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:41:06.237170: step 2300, loss 0.073759, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2300\n",
      "\n",
      "2019-01-26T23:41:06.732335: step 2301, loss 9.94786e-05, acc 1\n",
      "2019-01-26T23:41:07.142642: step 2302, loss 0.000142069, acc 1\n",
      "2019-01-26T23:41:07.558801: step 2303, loss 7.8288e-05, acc 1\n",
      "2019-01-26T23:41:07.978721: step 2304, loss 0.00011776, acc 1\n",
      "2019-01-26T23:41:08.394730: step 2305, loss 9.36284e-05, acc 1\n",
      "2019-01-26T23:41:08.818945: step 2306, loss 7.12596e-05, acc 1\n",
      "2019-01-26T23:41:09.234811: step 2307, loss 1.42658e-05, acc 1\n",
      "2019-01-26T23:41:09.656971: step 2308, loss 6.74249e-05, acc 1\n",
      "2019-01-26T23:41:10.069876: step 2309, loss 0.00051221, acc 1\n",
      "2019-01-26T23:41:10.499138: step 2310, loss 2.56165e-05, acc 1\n",
      "2019-01-26T23:41:10.933486: step 2311, loss 0.000106676, acc 1\n",
      "2019-01-26T23:41:11.385515: step 2312, loss 0.00364959, acc 1\n",
      "2019-01-26T23:41:11.835427: step 2313, loss 1.91217e-05, acc 1\n",
      "2019-01-26T23:41:12.283138: step 2314, loss 2.90249e-05, acc 1\n",
      "2019-01-26T23:41:12.723918: step 2315, loss 0.000108138, acc 1\n",
      "2019-01-26T23:41:13.168631: step 2316, loss 0.000116624, acc 1\n",
      "2019-01-26T23:41:13.619894: step 2317, loss 0.00248031, acc 1\n",
      "2019-01-26T23:41:14.087673: step 2318, loss 0.000167303, acc 1\n",
      "2019-01-26T23:41:14.537634: step 2319, loss 8.68139e-06, acc 1\n",
      "2019-01-26T23:41:15.030957: step 2320, loss 0.000343794, acc 1\n",
      "2019-01-26T23:41:15.475808: step 2321, loss 0.000125406, acc 1\n",
      "2019-01-26T23:41:15.922659: step 2322, loss 2.66429e-05, acc 1\n",
      "2019-01-26T23:41:16.368536: step 2323, loss 6.88705e-05, acc 1\n",
      "2019-01-26T23:41:16.809886: step 2324, loss 0.000149302, acc 1\n",
      "2019-01-26T23:41:17.258056: step 2325, loss 0.000896121, acc 1\n",
      "2019-01-26T23:41:17.710500: step 2326, loss 0.000211332, acc 1\n",
      "2019-01-26T23:41:18.156087: step 2327, loss 1.42386e-05, acc 1\n",
      "2019-01-26T23:41:18.604850: step 2328, loss 0.00900828, acc 1\n",
      "2019-01-26T23:41:19.050349: step 2329, loss 0.000101049, acc 1\n",
      "2019-01-26T23:41:19.497924: step 2330, loss 0.00138484, acc 1\n",
      "2019-01-26T23:41:19.935022: step 2331, loss 0.000775405, acc 1\n",
      "2019-01-26T23:41:20.363178: step 2332, loss 5.83564e-05, acc 1\n",
      "2019-01-26T23:41:20.797953: step 2333, loss 2.50746e-05, acc 1\n",
      "2019-01-26T23:41:21.227297: step 2334, loss 8.43102e-05, acc 1\n",
      "2019-01-26T23:41:21.660078: step 2335, loss 0.000278904, acc 1\n",
      "2019-01-26T23:41:22.092287: step 2336, loss 0.0296067, acc 0.984375\n",
      "2019-01-26T23:41:22.525682: step 2337, loss 0.000613892, acc 1\n",
      "2019-01-26T23:41:22.959377: step 2338, loss 2.24665e-05, acc 1\n",
      "2019-01-26T23:41:23.385851: step 2339, loss 4.62812e-05, acc 1\n",
      "2019-01-26T23:41:23.829147: step 2340, loss 5.08011e-05, acc 1\n",
      "2019-01-26T23:41:24.274704: step 2341, loss 3.96983e-05, acc 1\n",
      "2019-01-26T23:41:24.719068: step 2342, loss 0.000582354, acc 1\n",
      "2019-01-26T23:41:25.166033: step 2343, loss 0.000119129, acc 1\n",
      "2019-01-26T23:41:25.610888: step 2344, loss 0.0156811, acc 0.984375\n",
      "2019-01-26T23:41:26.054117: step 2345, loss 3.67862e-05, acc 1\n",
      "2019-01-26T23:41:26.477273: step 2346, loss 0.000735197, acc 1\n",
      "2019-01-26T23:41:26.906728: step 2347, loss 0.000388389, acc 1\n",
      "2019-01-26T23:41:27.351565: step 2348, loss 0.000240472, acc 1\n",
      "2019-01-26T23:41:27.789024: step 2349, loss 0.000132851, acc 1\n",
      "2019-01-26T23:41:28.222544: step 2350, loss 0.000775535, acc 1\n",
      "2019-01-26T23:41:28.656037: step 2351, loss 2.30404e-06, acc 1\n",
      "2019-01-26T23:41:29.083850: step 2352, loss 4.16352e-05, acc 1\n",
      "2019-01-26T23:41:29.511587: step 2353, loss 6.43342e-05, acc 1\n",
      "2019-01-26T23:41:29.948324: step 2354, loss 2.11051e-05, acc 1\n",
      "2019-01-26T23:41:30.377885: step 2355, loss 0.000463287, acc 1\n",
      "2019-01-26T23:41:30.812590: step 2356, loss 0.00528114, acc 1\n",
      "2019-01-26T23:41:31.244214: step 2357, loss 0.000381432, acc 1\n",
      "2019-01-26T23:41:31.681422: step 2358, loss 0.00157476, acc 1\n",
      "2019-01-26T23:41:32.111149: step 2359, loss 0.00187266, acc 1\n",
      "2019-01-26T23:41:32.540935: step 2360, loss 2.76225e-06, acc 1\n",
      "2019-01-26T23:41:32.986485: step 2361, loss 0.000846084, acc 1\n",
      "2019-01-26T23:41:33.429014: step 2362, loss 0.000388416, acc 1\n",
      "2019-01-26T23:41:33.873105: step 2363, loss 1.92894e-05, acc 1\n",
      "2019-01-26T23:41:34.307781: step 2364, loss 3.7997e-06, acc 1\n",
      "2019-01-26T23:41:34.771852: step 2365, loss 9.46641e-05, acc 1\n",
      "2019-01-26T23:41:35.203644: step 2366, loss 7.21001e-05, acc 1\n",
      "2019-01-26T23:41:35.635223: step 2367, loss 3.90281e-05, acc 1\n",
      "2019-01-26T23:41:36.065365: step 2368, loss 9.76572e-05, acc 1\n",
      "2019-01-26T23:41:36.497114: step 2369, loss 0.000323986, acc 1\n",
      "2019-01-26T23:41:36.683032: step 2370, loss 7.50959e-06, acc 1\n",
      "2019-01-26T23:41:37.112336: step 2371, loss 0.000349886, acc 1\n",
      "2019-01-26T23:41:37.549945: step 2372, loss 6.15367e-05, acc 1\n",
      "2019-01-26T23:41:37.985444: step 2373, loss 0.00380149, acc 1\n",
      "2019-01-26T23:41:38.419424: step 2374, loss 0.00244627, acc 1\n",
      "2019-01-26T23:41:38.854790: step 2375, loss 1.5096e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:41:39.286294: step 2376, loss 1.28752e-05, acc 1\n",
      "2019-01-26T23:41:39.715241: step 2377, loss 9.10311e-05, acc 1\n",
      "2019-01-26T23:41:40.138614: step 2378, loss 4.9437e-05, acc 1\n",
      "2019-01-26T23:41:40.570388: step 2379, loss 0.000218715, acc 1\n",
      "2019-01-26T23:41:41.004818: step 2380, loss 0.000307965, acc 1\n",
      "2019-01-26T23:41:41.442613: step 2381, loss 9.27083e-05, acc 1\n",
      "2019-01-26T23:41:41.884622: step 2382, loss 3.52777e-06, acc 1\n",
      "2019-01-26T23:41:42.318327: step 2383, loss 0.000218608, acc 1\n",
      "2019-01-26T23:41:42.748208: step 2384, loss 0.000286975, acc 1\n",
      "2019-01-26T23:41:43.176901: step 2385, loss 6.30561e-05, acc 1\n",
      "2019-01-26T23:41:43.606064: step 2386, loss 4.78024e-05, acc 1\n",
      "2019-01-26T23:41:44.035489: step 2387, loss 0.000562499, acc 1\n",
      "2019-01-26T23:41:44.469053: step 2388, loss 0.000292328, acc 1\n",
      "2019-01-26T23:41:44.897278: step 2389, loss 2.30988e-05, acc 1\n",
      "2019-01-26T23:41:45.317330: step 2390, loss 2.04862e-05, acc 1\n",
      "2019-01-26T23:41:45.741191: step 2391, loss 0.000113604, acc 1\n",
      "2019-01-26T23:41:46.165667: step 2392, loss 0.000182616, acc 1\n",
      "2019-01-26T23:41:46.577995: step 2393, loss 2.25157e-05, acc 1\n",
      "2019-01-26T23:41:47.009128: step 2394, loss 3.1453e-05, acc 1\n",
      "2019-01-26T23:41:47.421000: step 2395, loss 8.52183e-05, acc 1\n",
      "2019-01-26T23:41:47.838411: step 2396, loss 0.00607569, acc 1\n",
      "2019-01-26T23:41:48.250328: step 2397, loss 1.95457e-05, acc 1\n",
      "2019-01-26T23:41:48.672279: step 2398, loss 2.16254e-05, acc 1\n",
      "2019-01-26T23:41:49.093646: step 2399, loss 0.000403438, acc 1\n",
      "2019-01-26T23:41:49.546227: step 2400, loss 0.000117502, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:41:50.582389: step 2400, loss 0.0798527, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2400\n",
      "\n",
      "2019-01-26T23:41:51.073932: step 2401, loss 0.00012278, acc 1\n",
      "2019-01-26T23:41:51.490963: step 2402, loss 6.83384e-05, acc 1\n",
      "2019-01-26T23:41:51.929767: step 2403, loss 0.0286034, acc 0.984375\n",
      "2019-01-26T23:41:52.343997: step 2404, loss 5.82118e-05, acc 1\n",
      "2019-01-26T23:41:52.796709: step 2405, loss 8.34124e-05, acc 1\n",
      "2019-01-26T23:41:53.207912: step 2406, loss 0.000225363, acc 1\n",
      "2019-01-26T23:41:53.623233: step 2407, loss 0.000737849, acc 1\n",
      "2019-01-26T23:41:54.036939: step 2408, loss 7.79099e-06, acc 1\n",
      "2019-01-26T23:41:54.465776: step 2409, loss 0.000182235, acc 1\n",
      "2019-01-26T23:41:54.890222: step 2410, loss 5.39335e-05, acc 1\n",
      "2019-01-26T23:41:55.320250: step 2411, loss 0.00248172, acc 1\n",
      "2019-01-26T23:41:55.770429: step 2412, loss 0.00010628, acc 1\n",
      "2019-01-26T23:41:56.215533: step 2413, loss 4.18527e-06, acc 1\n",
      "2019-01-26T23:41:56.663922: step 2414, loss 2.07025e-05, acc 1\n",
      "2019-01-26T23:41:57.111310: step 2415, loss 1.07337e-05, acc 1\n",
      "2019-01-26T23:41:57.560231: step 2416, loss 0.000147066, acc 1\n",
      "2019-01-26T23:41:58.021987: step 2417, loss 7.01257e-06, acc 1\n",
      "2019-01-26T23:41:58.470476: step 2418, loss 0.00030348, acc 1\n",
      "2019-01-26T23:41:58.924745: step 2419, loss 0.000275472, acc 1\n",
      "2019-01-26T23:41:59.365088: step 2420, loss 8.30987e-05, acc 1\n",
      "2019-01-26T23:41:59.816316: step 2421, loss 1.24918e-05, acc 1\n",
      "2019-01-26T23:42:00.259944: step 2422, loss 0.000746713, acc 1\n",
      "2019-01-26T23:42:00.705805: step 2423, loss 0.0151446, acc 0.984375\n",
      "2019-01-26T23:42:01.147300: step 2424, loss 3.39185e-05, acc 1\n",
      "2019-01-26T23:42:01.595843: step 2425, loss 0.00175868, acc 1\n",
      "2019-01-26T23:42:02.043092: step 2426, loss 8.22718e-05, acc 1\n",
      "2019-01-26T23:42:02.489949: step 2427, loss 0.000193326, acc 1\n",
      "2019-01-26T23:42:02.935610: step 2428, loss 0.00112406, acc 1\n",
      "2019-01-26T23:42:03.380589: step 2429, loss 0.000117759, acc 1\n",
      "2019-01-26T23:42:03.821616: step 2430, loss 0.000249706, acc 1\n",
      "2019-01-26T23:42:04.251719: step 2431, loss 3.62288e-05, acc 1\n",
      "2019-01-26T23:42:04.690092: step 2432, loss 2.64093e-05, acc 1\n",
      "2019-01-26T23:42:05.122854: step 2433, loss 0.000300601, acc 1\n",
      "2019-01-26T23:42:05.558355: step 2434, loss 0.000188, acc 1\n",
      "2019-01-26T23:42:05.980571: step 2435, loss 2.54799e-05, acc 1\n",
      "2019-01-26T23:42:06.418292: step 2436, loss 2.25685e-05, acc 1\n",
      "2019-01-26T23:42:06.844781: step 2437, loss 2.53706e-05, acc 1\n",
      "2019-01-26T23:42:07.282565: step 2438, loss 0.000181256, acc 1\n",
      "2019-01-26T23:42:07.717680: step 2439, loss 3.07704e-06, acc 1\n",
      "2019-01-26T23:42:08.164707: step 2440, loss 0.000177455, acc 1\n",
      "2019-01-26T23:42:08.615906: step 2441, loss 5.07029e-05, acc 1\n",
      "2019-01-26T23:42:09.059301: step 2442, loss 0.000708573, acc 1\n",
      "2019-01-26T23:42:09.509785: step 2443, loss 3.4376e-05, acc 1\n",
      "2019-01-26T23:42:09.957668: step 2444, loss 0.00111393, acc 1\n",
      "2019-01-26T23:42:10.384741: step 2445, loss 0.00226392, acc 1\n",
      "2019-01-26T23:42:10.816517: step 2446, loss 7.78341e-06, acc 1\n",
      "2019-01-26T23:42:11.240798: step 2447, loss 5.92301e-05, acc 1\n",
      "2019-01-26T23:42:11.681176: step 2448, loss 0.00015444, acc 1\n",
      "2019-01-26T23:42:11.867174: step 2449, loss 1.07288e-06, acc 1\n",
      "2019-01-26T23:42:12.303770: step 2450, loss 2.5733e-05, acc 1\n",
      "2019-01-26T23:42:12.738941: step 2451, loss 1.55549e-05, acc 1\n",
      "2019-01-26T23:42:13.173226: step 2452, loss 0.00158189, acc 1\n",
      "2019-01-26T23:42:13.605532: step 2453, loss 2.08367e-05, acc 1\n",
      "2019-01-26T23:42:14.039264: step 2454, loss 2.59443e-05, acc 1\n",
      "2019-01-26T23:42:14.474833: step 2455, loss 2.2089e-05, acc 1\n",
      "2019-01-26T23:42:14.915332: step 2456, loss 0.000285295, acc 1\n",
      "2019-01-26T23:42:15.349804: step 2457, loss 0.000403326, acc 1\n",
      "2019-01-26T23:42:15.826865: step 2458, loss 0.000806543, acc 1\n",
      "2019-01-26T23:42:16.255438: step 2459, loss 3.05342e-05, acc 1\n",
      "2019-01-26T23:42:16.692247: step 2460, loss 7.57041e-05, acc 1\n",
      "2019-01-26T23:42:17.134684: step 2461, loss 4.85755e-06, acc 1\n",
      "2019-01-26T23:42:17.576031: step 2462, loss 0.000673966, acc 1\n",
      "2019-01-26T23:42:18.017658: step 2463, loss 2.92698e-05, acc 1\n",
      "2019-01-26T23:42:18.463386: step 2464, loss 1.38296e-05, acc 1\n",
      "2019-01-26T23:42:18.912398: step 2465, loss 5.37315e-05, acc 1\n",
      "2019-01-26T23:42:19.366631: step 2466, loss 0.00112787, acc 1\n",
      "2019-01-26T23:42:19.840647: step 2467, loss 4.31052e-05, acc 1\n",
      "2019-01-26T23:42:20.302380: step 2468, loss 1.63573e-05, acc 1\n",
      "2019-01-26T23:42:20.788802: step 2469, loss 1.81095e-05, acc 1\n",
      "2019-01-26T23:42:21.267462: step 2470, loss 0.000189431, acc 1\n",
      "2019-01-26T23:42:21.753038: step 2471, loss 3.59671e-06, acc 1\n",
      "2019-01-26T23:42:22.239450: step 2472, loss 8.88216e-05, acc 1\n",
      "2019-01-26T23:42:22.724082: step 2473, loss 4.22543e-05, acc 1\n",
      "2019-01-26T23:42:23.209196: step 2474, loss 5.63995e-06, acc 1\n",
      "2019-01-26T23:42:23.692481: step 2475, loss 4.45334e-06, acc 1\n",
      "2019-01-26T23:42:24.179143: step 2476, loss 9.96644e-05, acc 1\n",
      "2019-01-26T23:42:24.667673: step 2477, loss 9.10884e-05, acc 1\n",
      "2019-01-26T23:42:25.153750: step 2478, loss 0.000189756, acc 1\n",
      "2019-01-26T23:42:25.635573: step 2479, loss 0.000832937, acc 1\n",
      "2019-01-26T23:42:26.107460: step 2480, loss 2.56848e-06, acc 1\n",
      "2019-01-26T23:42:26.574374: step 2481, loss 6.91024e-05, acc 1\n",
      "2019-01-26T23:42:27.054201: step 2482, loss 0.00439137, acc 1\n",
      "2019-01-26T23:42:27.512070: step 2483, loss 5.04172e-05, acc 1\n",
      "2019-01-26T23:42:27.983410: step 2484, loss 0.000194129, acc 1\n",
      "2019-01-26T23:42:28.450244: step 2485, loss 2.00417e-06, acc 1\n",
      "2019-01-26T23:42:28.916528: step 2486, loss 0.000133893, acc 1\n",
      "2019-01-26T23:42:29.365998: step 2487, loss 0.000198814, acc 1\n",
      "2019-01-26T23:42:29.818586: step 2488, loss 6.77511e-05, acc 1\n",
      "2019-01-26T23:42:30.270799: step 2489, loss 0.00226566, acc 1\n",
      "2019-01-26T23:42:30.744191: step 2490, loss 0.0359314, acc 0.984375\n",
      "2019-01-26T23:42:31.201286: step 2491, loss 1.81485e-05, acc 1\n",
      "2019-01-26T23:42:31.666229: step 2492, loss 0.000142761, acc 1\n",
      "2019-01-26T23:42:32.132131: step 2493, loss 1.39309e-05, acc 1\n",
      "2019-01-26T23:42:32.594198: step 2494, loss 3.28772e-05, acc 1\n",
      "2019-01-26T23:42:33.059537: step 2495, loss 3.49622e-05, acc 1\n",
      "2019-01-26T23:42:33.522053: step 2496, loss 4.72741e-05, acc 1\n",
      "2019-01-26T23:42:33.988888: step 2497, loss 1.11077e-05, acc 1\n",
      "2019-01-26T23:42:34.454403: step 2498, loss 0.00123731, acc 1\n",
      "2019-01-26T23:42:34.923022: step 2499, loss 0.00723938, acc 1\n",
      "2019-01-26T23:42:35.387998: step 2500, loss 0.00088134, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:42:36.535190: step 2500, loss 0.0690674, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:42:37.062345: step 2501, loss 3.96292e-05, acc 1\n",
      "2019-01-26T23:42:37.498512: step 2502, loss 0.00153319, acc 1\n",
      "2019-01-26T23:42:37.945657: step 2503, loss 0.000259283, acc 1\n",
      "2019-01-26T23:42:38.387785: step 2504, loss 0.000583221, acc 1\n",
      "2019-01-26T23:42:38.836506: step 2505, loss 0.000227443, acc 1\n",
      "2019-01-26T23:42:39.285404: step 2506, loss 0.000190807, acc 1\n",
      "2019-01-26T23:42:39.750876: step 2507, loss 0.000159374, acc 1\n",
      "2019-01-26T23:42:40.176567: step 2508, loss 0.000330776, acc 1\n",
      "2019-01-26T23:42:40.604167: step 2509, loss 0.000210221, acc 1\n",
      "2019-01-26T23:42:41.033969: step 2510, loss 0.000100721, acc 1\n",
      "2019-01-26T23:42:41.461950: step 2511, loss 0.000776579, acc 1\n",
      "2019-01-26T23:42:41.893259: step 2512, loss 0.0037937, acc 1\n",
      "2019-01-26T23:42:42.321496: step 2513, loss 8.05099e-05, acc 1\n",
      "2019-01-26T23:42:42.744788: step 2514, loss 0.000254995, acc 1\n",
      "2019-01-26T23:42:43.155100: step 2515, loss 0.00038888, acc 1\n",
      "2019-01-26T23:42:43.572431: step 2516, loss 7.3625e-06, acc 1\n",
      "2019-01-26T23:42:43.986776: step 2517, loss 1.96311e-05, acc 1\n",
      "2019-01-26T23:42:44.401005: step 2518, loss 8.06398e-05, acc 1\n",
      "2019-01-26T23:42:44.820168: step 2519, loss 5.61074e-05, acc 1\n",
      "2019-01-26T23:42:45.233426: step 2520, loss 0.0788665, acc 0.984375\n",
      "2019-01-26T23:42:45.644820: step 2521, loss 0.00057196, acc 1\n",
      "2019-01-26T23:42:46.048881: step 2522, loss 0.000343535, acc 1\n",
      "2019-01-26T23:42:46.461581: step 2523, loss 0.000151134, acc 1\n",
      "2019-01-26T23:42:46.885241: step 2524, loss 0.000242108, acc 1\n",
      "2019-01-26T23:42:47.297677: step 2525, loss 6.3905e-05, acc 1\n",
      "2019-01-26T23:42:47.711431: step 2526, loss 2.72979e-05, acc 1\n",
      "2019-01-26T23:42:48.125101: step 2527, loss 5.88797e-05, acc 1\n",
      "2019-01-26T23:42:48.304528: step 2528, loss 2.39213e-05, acc 1\n",
      "2019-01-26T23:42:48.721674: step 2529, loss 2.9141e-05, acc 1\n",
      "2019-01-26T23:42:49.135981: step 2530, loss 0.00515277, acc 1\n",
      "2019-01-26T23:42:49.544019: step 2531, loss 0.000163985, acc 1\n",
      "2019-01-26T23:42:49.957610: step 2532, loss 3.61103e-05, acc 1\n",
      "2019-01-26T23:42:50.366239: step 2533, loss 4.11493e-05, acc 1\n",
      "2019-01-26T23:42:50.778960: step 2534, loss 0.000307854, acc 1\n",
      "2019-01-26T23:42:51.193917: step 2535, loss 6.55993e-05, acc 1\n",
      "2019-01-26T23:42:51.612051: step 2536, loss 0.000674212, acc 1\n",
      "2019-01-26T23:42:52.026709: step 2537, loss 0.0505107, acc 0.984375\n",
      "2019-01-26T23:42:52.428808: step 2538, loss 0.000105027, acc 1\n",
      "2019-01-26T23:42:52.830103: step 2539, loss 6.62232e-05, acc 1\n",
      "2019-01-26T23:42:53.229454: step 2540, loss 0.000386245, acc 1\n",
      "2019-01-26T23:42:53.636507: step 2541, loss 0.00124195, acc 1\n",
      "2019-01-26T23:42:54.045926: step 2542, loss 0.0085775, acc 1\n",
      "2019-01-26T23:42:54.461950: step 2543, loss 0.000236813, acc 1\n",
      "2019-01-26T23:42:54.876181: step 2544, loss 1.74315e-05, acc 1\n",
      "2019-01-26T23:42:55.290278: step 2545, loss 4.10126e-05, acc 1\n",
      "2019-01-26T23:42:55.705032: step 2546, loss 0.00199984, acc 1\n",
      "2019-01-26T23:42:56.119449: step 2547, loss 8.37936e-05, acc 1\n",
      "2019-01-26T23:42:56.535360: step 2548, loss 0.000152327, acc 1\n",
      "2019-01-26T23:42:56.954768: step 2549, loss 8.90411e-05, acc 1\n",
      "2019-01-26T23:42:57.370035: step 2550, loss 0.00131416, acc 1\n",
      "2019-01-26T23:42:57.790941: step 2551, loss 0.000955803, acc 1\n",
      "2019-01-26T23:42:58.207430: step 2552, loss 2.23336e-05, acc 1\n",
      "2019-01-26T23:42:58.632187: step 2553, loss 0.000143338, acc 1\n",
      "2019-01-26T23:42:59.059913: step 2554, loss 0.000251726, acc 1\n",
      "2019-01-26T23:42:59.491891: step 2555, loss 5.79521e-05, acc 1\n",
      "2019-01-26T23:42:59.982278: step 2556, loss 0.000383035, acc 1\n",
      "2019-01-26T23:43:00.431268: step 2557, loss 6.86235e-05, acc 1\n",
      "2019-01-26T23:43:00.878626: step 2558, loss 2.38159e-05, acc 1\n",
      "2019-01-26T23:43:01.330892: step 2559, loss 0.000246445, acc 1\n",
      "2019-01-26T23:43:01.785305: step 2560, loss 2.93954e-05, acc 1\n",
      "2019-01-26T23:43:02.244142: step 2561, loss 9.17706e-05, acc 1\n",
      "2019-01-26T23:43:02.693052: step 2562, loss 0.000260817, acc 1\n",
      "2019-01-26T23:43:03.138990: step 2563, loss 0.000252567, acc 1\n",
      "2019-01-26T23:43:03.581270: step 2564, loss 0.0973566, acc 0.984375\n",
      "2019-01-26T23:43:04.032458: step 2565, loss 6.09132e-05, acc 1\n",
      "2019-01-26T23:43:04.481655: step 2566, loss 0.00348048, acc 1\n",
      "2019-01-26T23:43:04.928870: step 2567, loss 0.0410021, acc 0.984375\n",
      "2019-01-26T23:43:05.371258: step 2568, loss 8.74425e-05, acc 1\n",
      "2019-01-26T23:43:05.822196: step 2569, loss 0.000625023, acc 1\n",
      "2019-01-26T23:43:06.263091: step 2570, loss 0.000151848, acc 1\n",
      "2019-01-26T23:43:06.715034: step 2571, loss 5.53502e-05, acc 1\n",
      "2019-01-26T23:43:07.159385: step 2572, loss 2.29092e-06, acc 1\n",
      "2019-01-26T23:43:07.604893: step 2573, loss 6.37131e-06, acc 1\n",
      "2019-01-26T23:43:08.033159: step 2574, loss 0.000896628, acc 1\n",
      "2019-01-26T23:43:08.466688: step 2575, loss 0.000467746, acc 1\n",
      "2019-01-26T23:43:08.902787: step 2576, loss 0.000132761, acc 1\n",
      "2019-01-26T23:43:09.332352: step 2577, loss 0.000828083, acc 1\n",
      "2019-01-26T23:43:09.762715: step 2578, loss 1.09544e-05, acc 1\n",
      "2019-01-26T23:43:10.188082: step 2579, loss 7.78581e-07, acc 1\n",
      "2019-01-26T23:43:10.621789: step 2580, loss 5.07686e-05, acc 1\n",
      "2019-01-26T23:43:11.051262: step 2581, loss 0.000207301, acc 1\n",
      "2019-01-26T23:43:11.495831: step 2582, loss 0.00384834, acc 1\n",
      "2019-01-26T23:43:11.967232: step 2583, loss 0.00014247, acc 1\n",
      "2019-01-26T23:43:12.405543: step 2584, loss 0.0159379, acc 0.984375\n",
      "2019-01-26T23:43:12.858566: step 2585, loss 0.000964644, acc 1\n",
      "2019-01-26T23:43:13.301939: step 2586, loss 0.0070192, acc 1\n",
      "2019-01-26T23:43:13.751510: step 2587, loss 4.81598e-05, acc 1\n",
      "2019-01-26T23:43:14.186785: step 2588, loss 7.88322e-06, acc 1\n",
      "2019-01-26T23:43:14.614605: step 2589, loss 3.93192e-06, acc 1\n",
      "2019-01-26T23:43:15.048266: step 2590, loss 0.000229409, acc 1\n",
      "2019-01-26T23:43:15.476063: step 2591, loss 3.01917e-06, acc 1\n",
      "2019-01-26T23:43:15.916300: step 2592, loss 0.00260614, acc 1\n",
      "2019-01-26T23:43:16.341192: step 2593, loss 4.68819e-05, acc 1\n",
      "2019-01-26T23:43:16.781547: step 2594, loss 0.00257843, acc 1\n",
      "2019-01-26T23:43:17.206007: step 2595, loss 6.87382e-05, acc 1\n",
      "2019-01-26T23:43:17.643198: step 2596, loss 0.000392635, acc 1\n",
      "2019-01-26T23:43:18.077769: step 2597, loss 0.000883066, acc 1\n",
      "2019-01-26T23:43:18.510080: step 2598, loss 2.0656e-06, acc 1\n",
      "2019-01-26T23:43:18.943936: step 2599, loss 2.61025e-05, acc 1\n",
      "2019-01-26T23:43:19.374306: step 2600, loss 0.000595845, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:43:20.461337: step 2600, loss 0.0761985, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2600\n",
      "\n",
      "2019-01-26T23:43:20.972953: step 2601, loss 3.25207e-06, acc 1\n",
      "2019-01-26T23:43:21.415964: step 2602, loss 0.000194969, acc 1\n",
      "2019-01-26T23:43:21.864507: step 2603, loss 0.0047559, acc 1\n",
      "2019-01-26T23:43:22.309088: step 2604, loss 9.00751e-05, acc 1\n",
      "2019-01-26T23:43:22.762621: step 2605, loss 0.00259916, acc 1\n",
      "2019-01-26T23:43:23.219683: step 2606, loss 5.86344e-06, acc 1\n",
      "2019-01-26T23:43:23.419234: step 2607, loss 5.71336e-05, acc 1\n",
      "2019-01-26T23:43:23.894011: step 2608, loss 6.61994e-05, acc 1\n",
      "2019-01-26T23:43:24.357583: step 2609, loss 8.05506e-05, acc 1\n",
      "2019-01-26T23:43:24.830464: step 2610, loss 0.000728598, acc 1\n",
      "2019-01-26T23:43:25.311520: step 2611, loss 0.000302222, acc 1\n",
      "2019-01-26T23:43:25.797316: step 2612, loss 0.000412823, acc 1\n",
      "2019-01-26T23:43:26.284544: step 2613, loss 0.0184938, acc 0.984375\n",
      "2019-01-26T23:43:26.780485: step 2614, loss 4.55107e-05, acc 1\n",
      "2019-01-26T23:43:27.281271: step 2615, loss 6.16275e-05, acc 1\n",
      "2019-01-26T23:43:27.759291: step 2616, loss 0.000296309, acc 1\n",
      "2019-01-26T23:43:28.243658: step 2617, loss 1.85716e-05, acc 1\n",
      "2019-01-26T23:43:28.730359: step 2618, loss 0.00132038, acc 1\n",
      "2019-01-26T23:43:29.205616: step 2619, loss 4.81587e-05, acc 1\n",
      "2019-01-26T23:43:29.692996: step 2620, loss 0.000202078, acc 1\n",
      "2019-01-26T23:43:30.155909: step 2621, loss 2.5674e-05, acc 1\n",
      "2019-01-26T23:43:30.626787: step 2622, loss 0.000136278, acc 1\n",
      "2019-01-26T23:43:31.100008: step 2623, loss 1.1178e-05, acc 1\n",
      "2019-01-26T23:43:31.575139: step 2624, loss 7.64751e-05, acc 1\n",
      "2019-01-26T23:43:32.041370: step 2625, loss 0.000185678, acc 1\n",
      "2019-01-26T23:43:32.511242: step 2626, loss 0.00058149, acc 1\n",
      "2019-01-26T23:43:32.972543: step 2627, loss 0.00969201, acc 1\n",
      "2019-01-26T23:43:33.418031: step 2628, loss 5.07368e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:43:33.871088: step 2629, loss 0.000157136, acc 1\n",
      "2019-01-26T23:43:34.309582: step 2630, loss 0.00217583, acc 1\n",
      "2019-01-26T23:43:34.751066: step 2631, loss 3.27196e-05, acc 1\n",
      "2019-01-26T23:43:35.194704: step 2632, loss 2.77583e-05, acc 1\n",
      "2019-01-26T23:43:35.641538: step 2633, loss 0.000705575, acc 1\n",
      "2019-01-26T23:43:36.064028: step 2634, loss 4.30008e-05, acc 1\n",
      "2019-01-26T23:43:36.497113: step 2635, loss 0.000576904, acc 1\n",
      "2019-01-26T23:43:36.924835: step 2636, loss 4.99301e-05, acc 1\n",
      "2019-01-26T23:43:37.359238: step 2637, loss 0.000374095, acc 1\n",
      "2019-01-26T23:43:37.793554: step 2638, loss 5.24625e-05, acc 1\n",
      "2019-01-26T23:43:38.218676: step 2639, loss 2.13276e-05, acc 1\n",
      "2019-01-26T23:43:38.643476: step 2640, loss 0.0177595, acc 0.984375\n",
      "2019-01-26T23:43:39.060602: step 2641, loss 0.000891938, acc 1\n",
      "2019-01-26T23:43:39.475659: step 2642, loss 0.000116169, acc 1\n",
      "2019-01-26T23:43:39.895457: step 2643, loss 0.000160758, acc 1\n",
      "2019-01-26T23:43:40.310831: step 2644, loss 4.59394e-05, acc 1\n",
      "2019-01-26T23:43:40.728471: step 2645, loss 7.22071e-05, acc 1\n",
      "2019-01-26T23:43:41.142795: step 2646, loss 7.99766e-06, acc 1\n",
      "2019-01-26T23:43:41.553074: step 2647, loss 0.0015817, acc 1\n",
      "2019-01-26T23:43:41.951665: step 2648, loss 1.53085e-05, acc 1\n",
      "2019-01-26T23:43:42.355636: step 2649, loss 4.53852e-05, acc 1\n",
      "2019-01-26T23:43:42.771528: step 2650, loss 1.28435e-05, acc 1\n",
      "2019-01-26T23:43:43.180037: step 2651, loss 1.99529e-05, acc 1\n",
      "2019-01-26T23:43:43.589528: step 2652, loss 7.54078e-05, acc 1\n",
      "2019-01-26T23:43:44.004530: step 2653, loss 0.00286159, acc 1\n",
      "2019-01-26T23:43:44.419172: step 2654, loss 7.98124e-05, acc 1\n",
      "2019-01-26T23:43:44.831661: step 2655, loss 2.99266e-05, acc 1\n",
      "2019-01-26T23:43:45.245853: step 2656, loss 1.96157e-05, acc 1\n",
      "2019-01-26T23:43:45.660205: step 2657, loss 0.000123454, acc 1\n",
      "2019-01-26T23:43:46.074050: step 2658, loss 0.0176374, acc 0.984375\n",
      "2019-01-26T23:43:46.514320: step 2659, loss 2.60047e-05, acc 1\n",
      "2019-01-26T23:43:46.946223: step 2660, loss 0.000424497, acc 1\n",
      "2019-01-26T23:43:47.373916: step 2661, loss 3.95717e-05, acc 1\n",
      "2019-01-26T23:43:47.822317: step 2662, loss 6.44474e-07, acc 1\n",
      "2019-01-26T23:43:48.267280: step 2663, loss 6.93692e-05, acc 1\n",
      "2019-01-26T23:43:48.717553: step 2664, loss 0.000277411, acc 1\n",
      "2019-01-26T23:43:49.165436: step 2665, loss 4.1464e-05, acc 1\n",
      "2019-01-26T23:43:49.649650: step 2666, loss 1.26942e-05, acc 1\n",
      "2019-01-26T23:43:50.121967: step 2667, loss 3.56712e-05, acc 1\n",
      "2019-01-26T23:43:50.585497: step 2668, loss 1.68462e-05, acc 1\n",
      "2019-01-26T23:43:51.023382: step 2669, loss 0.00196463, acc 1\n",
      "2019-01-26T23:43:51.462445: step 2670, loss 2.65454e-05, acc 1\n",
      "2019-01-26T23:43:51.968805: step 2671, loss 0.000511418, acc 1\n",
      "2019-01-26T23:43:52.415406: step 2672, loss 0.00324226, acc 1\n",
      "2019-01-26T23:43:52.867879: step 2673, loss 0.00216321, acc 1\n",
      "2019-01-26T23:43:53.306084: step 2674, loss 0.000457189, acc 1\n",
      "2019-01-26T23:43:53.753956: step 2675, loss 3.40162e-05, acc 1\n",
      "2019-01-26T23:43:54.201962: step 2676, loss 5.5588e-05, acc 1\n",
      "2019-01-26T23:43:54.657830: step 2677, loss 0.000160159, acc 1\n",
      "2019-01-26T23:43:55.108912: step 2678, loss 0.0002948, acc 1\n",
      "2019-01-26T23:43:55.547941: step 2679, loss 0.000191728, acc 1\n",
      "2019-01-26T23:43:55.981509: step 2680, loss 8.41116e-05, acc 1\n",
      "2019-01-26T23:43:56.403813: step 2681, loss 0.000819349, acc 1\n",
      "2019-01-26T23:43:56.837139: step 2682, loss 0.00052472, acc 1\n",
      "2019-01-26T23:43:57.270075: step 2683, loss 0.00133083, acc 1\n",
      "2019-01-26T23:43:57.705358: step 2684, loss 0.000117197, acc 1\n",
      "2019-01-26T23:43:58.138262: step 2685, loss 0.000136737, acc 1\n",
      "2019-01-26T23:43:58.326131: step 2686, loss 1.46562e-05, acc 1\n",
      "2019-01-26T23:43:58.763565: step 2687, loss 0.000120884, acc 1\n",
      "2019-01-26T23:43:59.193247: step 2688, loss 1.18159e-05, acc 1\n",
      "2019-01-26T23:43:59.638499: step 2689, loss 9.9172e-05, acc 1\n",
      "2019-01-26T23:44:00.072525: step 2690, loss 0.000153772, acc 1\n",
      "2019-01-26T23:44:00.513939: step 2691, loss 0.000431817, acc 1\n",
      "2019-01-26T23:44:00.964151: step 2692, loss 6.16214e-05, acc 1\n",
      "2019-01-26T23:44:01.412309: step 2693, loss 0.000262324, acc 1\n",
      "2019-01-26T23:44:01.864330: step 2694, loss 0.000144004, acc 1\n",
      "2019-01-26T23:44:02.296923: step 2695, loss 0.000587756, acc 1\n",
      "2019-01-26T23:44:02.812904: step 2696, loss 0.000343408, acc 1\n",
      "2019-01-26T23:44:03.238789: step 2697, loss 0.000280152, acc 1\n",
      "2019-01-26T23:44:03.679974: step 2698, loss 0.000101851, acc 1\n",
      "2019-01-26T23:44:04.111594: step 2699, loss 0.00745023, acc 1\n",
      "2019-01-26T23:44:04.539603: step 2700, loss 4.1124e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:44:05.652044: step 2700, loss 0.0729393, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2700\n",
      "\n",
      "2019-01-26T23:44:06.177469: step 2701, loss 8.83934e-05, acc 1\n",
      "2019-01-26T23:44:06.610861: step 2702, loss 3.32859e-05, acc 1\n",
      "2019-01-26T23:44:07.040716: step 2703, loss 0.000101263, acc 1\n",
      "2019-01-26T23:44:07.482101: step 2704, loss 0.000303723, acc 1\n",
      "2019-01-26T23:44:07.919573: step 2705, loss 3.06463e-05, acc 1\n",
      "2019-01-26T23:44:08.348994: step 2706, loss 6.88205e-06, acc 1\n",
      "2019-01-26T23:44:08.787889: step 2707, loss 1.83127e-05, acc 1\n",
      "2019-01-26T23:44:09.224516: step 2708, loss 5.48919e-05, acc 1\n",
      "2019-01-26T23:44:09.671005: step 2709, loss 7.47451e-06, acc 1\n",
      "2019-01-26T23:44:10.113004: step 2710, loss 2.66552e-05, acc 1\n",
      "2019-01-26T23:44:10.545217: step 2711, loss 0.00150427, acc 1\n",
      "2019-01-26T23:44:11.009923: step 2712, loss 0.0173113, acc 0.984375\n",
      "2019-01-26T23:44:11.454376: step 2713, loss 3.20748e-05, acc 1\n",
      "2019-01-26T23:44:11.922777: step 2714, loss 0.000336117, acc 1\n",
      "2019-01-26T23:44:12.357519: step 2715, loss 0.000818287, acc 1\n",
      "2019-01-26T23:44:12.799501: step 2716, loss 0.000191205, acc 1\n",
      "2019-01-26T23:44:13.231052: step 2717, loss 0.000384149, acc 1\n",
      "2019-01-26T23:44:13.660387: step 2718, loss 0.000120053, acc 1\n",
      "2019-01-26T23:44:14.091170: step 2719, loss 3.73826e-05, acc 1\n",
      "2019-01-26T23:44:14.519314: step 2720, loss 0.000626301, acc 1\n",
      "2019-01-26T23:44:14.953778: step 2721, loss 0.000208439, acc 1\n",
      "2019-01-26T23:44:15.385309: step 2722, loss 0.000627496, acc 1\n",
      "2019-01-26T23:44:15.819158: step 2723, loss 0.000184301, acc 1\n",
      "2019-01-26T23:44:16.253321: step 2724, loss 0.000123868, acc 1\n",
      "2019-01-26T23:44:16.687238: step 2725, loss 0.000869979, acc 1\n",
      "2019-01-26T23:44:17.125232: step 2726, loss 6.94812e-05, acc 1\n",
      "2019-01-26T23:44:17.572297: step 2727, loss 0.00362033, acc 1\n",
      "2019-01-26T23:44:18.016288: step 2728, loss 0.00198958, acc 1\n",
      "2019-01-26T23:44:18.455759: step 2729, loss 0.000832939, acc 1\n",
      "2019-01-26T23:44:18.887620: step 2730, loss 0.000246705, acc 1\n",
      "2019-01-26T23:44:19.315119: step 2731, loss 0.000194521, acc 1\n",
      "2019-01-26T23:44:19.752803: step 2732, loss 0.000168989, acc 1\n",
      "2019-01-26T23:44:20.183865: step 2733, loss 0.000105203, acc 1\n",
      "2019-01-26T23:44:20.619693: step 2734, loss 3.65123e-05, acc 1\n",
      "2019-01-26T23:44:21.054970: step 2735, loss 0.000450478, acc 1\n",
      "2019-01-26T23:44:21.487257: step 2736, loss 1.6453e-05, acc 1\n",
      "2019-01-26T23:44:21.920614: step 2737, loss 6.56711e-05, acc 1\n",
      "2019-01-26T23:44:22.355106: step 2738, loss 7.44844e-05, acc 1\n",
      "2019-01-26T23:44:22.797091: step 2739, loss 0.0044783, acc 1\n",
      "2019-01-26T23:44:23.228779: step 2740, loss 2.63301e-05, acc 1\n",
      "2019-01-26T23:44:23.666394: step 2741, loss 2.37021e-05, acc 1\n",
      "2019-01-26T23:44:24.095403: step 2742, loss 0.000393984, acc 1\n",
      "2019-01-26T23:44:24.524559: step 2743, loss 5.12607e-05, acc 1\n",
      "2019-01-26T23:44:24.997492: step 2744, loss 0.000137838, acc 1\n",
      "2019-01-26T23:44:25.487725: step 2745, loss 4.78309e-06, acc 1\n",
      "2019-01-26T23:44:25.938088: step 2746, loss 2.89065e-05, acc 1\n",
      "2019-01-26T23:44:26.377017: step 2747, loss 0.00169402, acc 1\n",
      "2019-01-26T23:44:26.812981: step 2748, loss 2.64151e-05, acc 1\n",
      "2019-01-26T23:44:27.265538: step 2749, loss 5.6818e-05, acc 1\n",
      "2019-01-26T23:44:27.702422: step 2750, loss 1.82623e-05, acc 1\n",
      "2019-01-26T23:44:28.135551: step 2751, loss 0.00339828, acc 1\n",
      "2019-01-26T23:44:28.571863: step 2752, loss 0.000595208, acc 1\n",
      "2019-01-26T23:44:29.003482: step 2753, loss 0.00730855, acc 1\n",
      "2019-01-26T23:44:29.437211: step 2754, loss 0.000149779, acc 1\n",
      "2019-01-26T23:44:29.881223: step 2755, loss 3.63303e-05, acc 1\n",
      "2019-01-26T23:44:30.312283: step 2756, loss 4.7501e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:44:30.733823: step 2757, loss 6.62266e-05, acc 1\n",
      "2019-01-26T23:44:31.147453: step 2758, loss 0.0044254, acc 1\n",
      "2019-01-26T23:44:31.565448: step 2759, loss 0.000108507, acc 1\n",
      "2019-01-26T23:44:31.984965: step 2760, loss 4.67523e-07, acc 1\n",
      "2019-01-26T23:44:32.402117: step 2761, loss 9.23764e-05, acc 1\n",
      "2019-01-26T23:44:32.816880: step 2762, loss 2.58717e-06, acc 1\n",
      "2019-01-26T23:44:33.230930: step 2763, loss 1.80971e-05, acc 1\n",
      "2019-01-26T23:44:33.645447: step 2764, loss 0.000229818, acc 1\n",
      "2019-01-26T23:44:33.824448: step 2765, loss 9.84769e-07, acc 1\n",
      "2019-01-26T23:44:34.222809: step 2766, loss 7.3029e-06, acc 1\n",
      "2019-01-26T23:44:34.653303: step 2767, loss 7.85414e-05, acc 1\n",
      "2019-01-26T23:44:35.067020: step 2768, loss 0.0031071, acc 1\n",
      "2019-01-26T23:44:35.477992: step 2769, loss 2.88927e-05, acc 1\n",
      "2019-01-26T23:44:35.900054: step 2770, loss 0.000385614, acc 1\n",
      "2019-01-26T23:44:36.319204: step 2771, loss 4.39127e-05, acc 1\n",
      "2019-01-26T23:44:36.743386: step 2772, loss 1.99222e-05, acc 1\n",
      "2019-01-26T23:44:37.153320: step 2773, loss 0.000774331, acc 1\n",
      "2019-01-26T23:44:37.571804: step 2774, loss 7.64209e-06, acc 1\n",
      "2019-01-26T23:44:37.986269: step 2775, loss 7.03797e-06, acc 1\n",
      "2019-01-26T23:44:38.408698: step 2776, loss 0.000181713, acc 1\n",
      "2019-01-26T23:44:38.833532: step 2777, loss 1.83952e-05, acc 1\n",
      "2019-01-26T23:44:39.262441: step 2778, loss 9.94866e-06, acc 1\n",
      "2019-01-26T23:44:39.737544: step 2779, loss 0.00115101, acc 1\n",
      "2019-01-26T23:44:40.180957: step 2780, loss 8.82138e-05, acc 1\n",
      "2019-01-26T23:44:40.627406: step 2781, loss 0.000301577, acc 1\n",
      "2019-01-26T23:44:41.077196: step 2782, loss 2.39156e-06, acc 1\n",
      "2019-01-26T23:44:41.526822: step 2783, loss 1.32531e-05, acc 1\n",
      "2019-01-26T23:44:41.982575: step 2784, loss 0.0010894, acc 1\n",
      "2019-01-26T23:44:42.441849: step 2785, loss 2.51897e-05, acc 1\n",
      "2019-01-26T23:44:42.896718: step 2786, loss 0.00013433, acc 1\n",
      "2019-01-26T23:44:43.341231: step 2787, loss 7.35141e-06, acc 1\n",
      "2019-01-26T23:44:43.791638: step 2788, loss 2.59507e-05, acc 1\n",
      "2019-01-26T23:44:44.245845: step 2789, loss 6.38658e-05, acc 1\n",
      "2019-01-26T23:44:44.693656: step 2790, loss 0.001323, acc 1\n",
      "2019-01-26T23:44:45.136137: step 2791, loss 4.66939e-05, acc 1\n",
      "2019-01-26T23:44:45.582497: step 2792, loss 0.00140696, acc 1\n",
      "2019-01-26T23:44:46.023429: step 2793, loss 6.15955e-06, acc 1\n",
      "2019-01-26T23:44:46.463047: step 2794, loss 3.08628e-06, acc 1\n",
      "2019-01-26T23:44:46.909798: step 2795, loss 0.000351576, acc 1\n",
      "2019-01-26T23:44:47.352593: step 2796, loss 7.94625e-05, acc 1\n",
      "2019-01-26T23:44:47.788953: step 2797, loss 5.02709e-06, acc 1\n",
      "2019-01-26T23:44:48.218593: step 2798, loss 0.000109445, acc 1\n",
      "2019-01-26T23:44:48.651149: step 2799, loss 1.46587e-06, acc 1\n",
      "2019-01-26T23:44:49.081284: step 2800, loss 2.76162e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:44:50.146003: step 2800, loss 0.0822417, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2800\n",
      "\n",
      "2019-01-26T23:44:50.658348: step 2801, loss 0.00495135, acc 1\n",
      "2019-01-26T23:44:51.083872: step 2802, loss 0.00165269, acc 1\n",
      "2019-01-26T23:44:51.514092: step 2803, loss 4.21313e-06, acc 1\n",
      "2019-01-26T23:44:51.950403: step 2804, loss 1.04646e-05, acc 1\n",
      "2019-01-26T23:44:52.398780: step 2805, loss 0.000411189, acc 1\n",
      "2019-01-26T23:44:52.853283: step 2806, loss 1.71218e-05, acc 1\n",
      "2019-01-26T23:44:53.298417: step 2807, loss 0.0136201, acc 0.984375\n",
      "2019-01-26T23:44:53.736690: step 2808, loss 1.25746e-05, acc 1\n",
      "2019-01-26T23:44:54.181200: step 2809, loss 0.000103635, acc 1\n",
      "2019-01-26T23:44:54.624160: step 2810, loss 1.37579e-05, acc 1\n",
      "2019-01-26T23:44:55.058146: step 2811, loss 5.58783e-05, acc 1\n",
      "2019-01-26T23:44:55.491524: step 2812, loss 0.00012131, acc 1\n",
      "2019-01-26T23:44:55.923982: step 2813, loss 0.000399561, acc 1\n",
      "2019-01-26T23:44:56.353585: step 2814, loss 0.000662745, acc 1\n",
      "2019-01-26T23:44:56.784750: step 2815, loss 4.87016e-05, acc 1\n",
      "2019-01-26T23:44:57.212870: step 2816, loss 1.36567e-05, acc 1\n",
      "2019-01-26T23:44:57.671124: step 2817, loss 0.015272, acc 0.984375\n",
      "2019-01-26T23:44:58.106219: step 2818, loss 2.44002e-06, acc 1\n",
      "2019-01-26T23:44:58.524911: step 2819, loss 8.62138e-05, acc 1\n",
      "2019-01-26T23:44:58.938026: step 2820, loss 4.1163e-06, acc 1\n",
      "2019-01-26T23:44:59.353895: step 2821, loss 0.00391066, acc 1\n",
      "2019-01-26T23:44:59.774822: step 2822, loss 2.53882e-05, acc 1\n",
      "2019-01-26T23:45:00.200007: step 2823, loss 0.000354495, acc 1\n",
      "2019-01-26T23:45:00.618595: step 2824, loss 0.000573924, acc 1\n",
      "2019-01-26T23:45:01.035123: step 2825, loss 0.000801652, acc 1\n",
      "2019-01-26T23:45:01.454163: step 2826, loss 0.000130611, acc 1\n",
      "2019-01-26T23:45:01.874087: step 2827, loss 1.46905e-05, acc 1\n",
      "2019-01-26T23:45:02.282564: step 2828, loss 5.51458e-05, acc 1\n",
      "2019-01-26T23:45:02.697628: step 2829, loss 0.000125309, acc 1\n",
      "2019-01-26T23:45:03.113556: step 2830, loss 0.00379738, acc 1\n",
      "2019-01-26T23:45:03.530024: step 2831, loss 1.25126e-05, acc 1\n",
      "2019-01-26T23:45:03.953706: step 2832, loss 0.000114931, acc 1\n",
      "2019-01-26T23:45:04.374383: step 2833, loss 0.00386951, acc 1\n",
      "2019-01-26T23:45:04.793256: step 2834, loss 1.51159e-05, acc 1\n",
      "2019-01-26T23:45:05.209921: step 2835, loss 1.89989e-06, acc 1\n",
      "2019-01-26T23:45:05.631059: step 2836, loss 1.99829e-05, acc 1\n",
      "2019-01-26T23:45:06.045882: step 2837, loss 7.33833e-06, acc 1\n",
      "2019-01-26T23:45:06.465599: step 2838, loss 9.56915e-05, acc 1\n",
      "2019-01-26T23:45:06.899105: step 2839, loss 5.04377e-06, acc 1\n",
      "2019-01-26T23:45:07.323520: step 2840, loss 8.83676e-05, acc 1\n",
      "2019-01-26T23:45:07.767726: step 2841, loss 4.27665e-05, acc 1\n",
      "2019-01-26T23:45:08.218138: step 2842, loss 2.29506e-05, acc 1\n",
      "2019-01-26T23:45:08.671598: step 2843, loss 0.019416, acc 0.984375\n",
      "2019-01-26T23:45:08.941548: step 2844, loss 0.0328582, acc 0.956522\n",
      "2019-01-26T23:45:09.394876: step 2845, loss 6.91766e-05, acc 1\n",
      "2019-01-26T23:45:09.845707: step 2846, loss 0.000531329, acc 1\n",
      "2019-01-26T23:45:10.298600: step 2847, loss 0.0123552, acc 0.984375\n",
      "2019-01-26T23:45:10.749356: step 2848, loss 5.32631e-05, acc 1\n",
      "2019-01-26T23:45:11.194452: step 2849, loss 0.000180092, acc 1\n",
      "2019-01-26T23:45:11.669661: step 2850, loss 0.00023921, acc 1\n",
      "2019-01-26T23:45:12.114930: step 2851, loss 0.000516287, acc 1\n",
      "2019-01-26T23:45:12.582265: step 2852, loss 0.00782275, acc 1\n",
      "2019-01-26T23:45:13.062619: step 2853, loss 0.00132241, acc 1\n",
      "2019-01-26T23:45:13.505869: step 2854, loss 0.00134375, acc 1\n",
      "2019-01-26T23:45:13.956060: step 2855, loss 0.00074353, acc 1\n",
      "2019-01-26T23:45:14.398816: step 2856, loss 0.0351592, acc 0.984375\n",
      "2019-01-26T23:45:14.843989: step 2857, loss 0.0116186, acc 0.984375\n",
      "2019-01-26T23:45:15.302021: step 2858, loss 0.000103424, acc 1\n",
      "2019-01-26T23:45:15.741682: step 2859, loss 0.000498859, acc 1\n",
      "2019-01-26T23:45:16.170493: step 2860, loss 0.000162285, acc 1\n",
      "2019-01-26T23:45:16.604052: step 2861, loss 3.2077e-05, acc 1\n",
      "2019-01-26T23:45:17.034475: step 2862, loss 0.000338463, acc 1\n",
      "2019-01-26T23:45:17.459494: step 2863, loss 0.000776617, acc 1\n",
      "2019-01-26T23:45:17.889608: step 2864, loss 5.04683e-05, acc 1\n",
      "2019-01-26T23:45:18.324083: step 2865, loss 0.00195988, acc 1\n",
      "2019-01-26T23:45:18.757915: step 2866, loss 0.0060196, acc 1\n",
      "2019-01-26T23:45:19.178554: step 2867, loss 0.000463637, acc 1\n",
      "2019-01-26T23:45:19.598854: step 2868, loss 3.14702e-05, acc 1\n",
      "2019-01-26T23:45:20.016793: step 2869, loss 0.00243815, acc 1\n",
      "2019-01-26T23:45:20.435480: step 2870, loss 5.20857e-05, acc 1\n",
      "2019-01-26T23:45:20.855785: step 2871, loss 7.26728e-06, acc 1\n",
      "2019-01-26T23:45:21.275550: step 2872, loss 4.11785e-05, acc 1\n",
      "2019-01-26T23:45:21.695722: step 2873, loss 0.00220768, acc 1\n",
      "2019-01-26T23:45:22.110501: step 2874, loss 4.08622e-06, acc 1\n",
      "2019-01-26T23:45:22.517447: step 2875, loss 1.04309e-05, acc 1\n",
      "2019-01-26T23:45:22.921906: step 2876, loss 0.000362867, acc 1\n",
      "2019-01-26T23:45:23.331055: step 2877, loss 0.000111616, acc 1\n",
      "2019-01-26T23:45:23.740249: step 2878, loss 0.0025463, acc 1\n",
      "2019-01-26T23:45:24.141798: step 2879, loss 3.63383e-06, acc 1\n",
      "2019-01-26T23:45:24.550805: step 2880, loss 0.00152805, acc 1\n",
      "2019-01-26T23:45:24.957954: step 2881, loss 8.32107e-06, acc 1\n",
      "2019-01-26T23:45:25.364351: step 2882, loss 3.01099e-05, acc 1\n",
      "2019-01-26T23:45:25.769187: step 2883, loss 1.62541e-05, acc 1\n",
      "2019-01-26T23:45:26.175137: step 2884, loss 6.54408e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:45:26.577028: step 2885, loss 6.1574e-05, acc 1\n",
      "2019-01-26T23:45:26.995061: step 2886, loss 1.96461e-05, acc 1\n",
      "2019-01-26T23:45:27.391377: step 2887, loss 2.37473e-05, acc 1\n",
      "2019-01-26T23:45:27.795938: step 2888, loss 0.000149886, acc 1\n",
      "2019-01-26T23:45:28.198915: step 2889, loss 0.000147233, acc 1\n",
      "2019-01-26T23:45:28.607983: step 2890, loss 0.0010761, acc 1\n",
      "2019-01-26T23:45:29.002430: step 2891, loss 0.000376997, acc 1\n",
      "2019-01-26T23:45:29.401210: step 2892, loss 2.77073e-05, acc 1\n",
      "2019-01-26T23:45:29.809828: step 2893, loss 3.08972e-05, acc 1\n",
      "2019-01-26T23:45:30.209915: step 2894, loss 5.9936e-06, acc 1\n",
      "2019-01-26T23:45:30.609262: step 2895, loss 0.0100011, acc 1\n",
      "2019-01-26T23:45:31.018606: step 2896, loss 3.3648e-05, acc 1\n",
      "2019-01-26T23:45:31.433802: step 2897, loss 0.000203358, acc 1\n",
      "2019-01-26T23:45:31.848999: step 2898, loss 0.000155272, acc 1\n",
      "2019-01-26T23:45:32.266147: step 2899, loss 5.48201e-05, acc 1\n",
      "2019-01-26T23:45:32.691015: step 2900, loss 0.000396252, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:45:33.757464: step 2900, loss 0.0838461, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-2900\n",
      "\n",
      "2019-01-26T23:45:34.257143: step 2901, loss 8.17694e-05, acc 1\n",
      "2019-01-26T23:45:34.693348: step 2902, loss 0.000558039, acc 1\n",
      "2019-01-26T23:45:35.145989: step 2903, loss 3.10783e-05, acc 1\n",
      "2019-01-26T23:45:35.602407: step 2904, loss 1.83698e-05, acc 1\n",
      "2019-01-26T23:45:36.067434: step 2905, loss 1.18603e-05, acc 1\n",
      "2019-01-26T23:45:36.528214: step 2906, loss 0.000130668, acc 1\n",
      "2019-01-26T23:45:37.038161: step 2907, loss 1.6554e-05, acc 1\n",
      "2019-01-26T23:45:37.517390: step 2908, loss 7.35532e-06, acc 1\n",
      "2019-01-26T23:45:38.002298: step 2909, loss 1.56681e-05, acc 1\n",
      "2019-01-26T23:45:38.486105: step 2910, loss 1.7847e-05, acc 1\n",
      "2019-01-26T23:45:38.972529: step 2911, loss 0.0231724, acc 0.984375\n",
      "2019-01-26T23:45:39.453309: step 2912, loss 1.97669e-05, acc 1\n",
      "2019-01-26T23:45:39.938975: step 2913, loss 3.87975e-06, acc 1\n",
      "2019-01-26T23:45:40.420742: step 2914, loss 0.000172903, acc 1\n",
      "2019-01-26T23:45:40.908941: step 2915, loss 4.74915e-06, acc 1\n",
      "2019-01-26T23:45:41.384848: step 2916, loss 5.29025e-05, acc 1\n",
      "2019-01-26T23:45:41.862608: step 2917, loss 0.000863589, acc 1\n",
      "2019-01-26T23:45:42.325772: step 2918, loss 0.000100963, acc 1\n",
      "2019-01-26T23:45:42.797521: step 2919, loss 0.00139406, acc 1\n",
      "2019-01-26T23:45:43.257740: step 2920, loss 0.00035284, acc 1\n",
      "2019-01-26T23:45:43.727068: step 2921, loss 5.00156e-05, acc 1\n",
      "2019-01-26T23:45:44.195518: step 2922, loss 0.00899107, acc 1\n",
      "2019-01-26T23:45:44.390606: step 2923, loss 1.98418e-05, acc 1\n",
      "2019-01-26T23:45:44.846166: step 2924, loss 1.42999e-05, acc 1\n",
      "2019-01-26T23:45:45.290911: step 2925, loss 8.04658e-07, acc 1\n",
      "2019-01-26T23:45:45.734796: step 2926, loss 1.06144e-05, acc 1\n",
      "2019-01-26T23:45:46.176026: step 2927, loss 0.000131766, acc 1\n",
      "2019-01-26T23:45:46.619887: step 2928, loss 0.00333903, acc 1\n",
      "2019-01-26T23:45:47.070326: step 2929, loss 0.00170325, acc 1\n",
      "2019-01-26T23:45:47.514283: step 2930, loss 4.64465e-05, acc 1\n",
      "2019-01-26T23:45:47.965079: step 2931, loss 2.49618e-05, acc 1\n",
      "2019-01-26T23:45:48.396737: step 2932, loss 0.000678211, acc 1\n",
      "2019-01-26T23:45:48.832330: step 2933, loss 3.17248e-05, acc 1\n",
      "2019-01-26T23:45:49.259997: step 2934, loss 4.82987e-05, acc 1\n",
      "2019-01-26T23:45:49.743446: step 2935, loss 6.41451e-06, acc 1\n",
      "2019-01-26T23:45:50.171659: step 2936, loss 0.000111048, acc 1\n",
      "2019-01-26T23:45:50.598849: step 2937, loss 2.83858e-06, acc 1\n",
      "2019-01-26T23:45:51.024923: step 2938, loss 1.6438e-05, acc 1\n",
      "2019-01-26T23:45:51.455022: step 2939, loss 4.8073e-05, acc 1\n",
      "2019-01-26T23:45:51.953378: step 2940, loss 1.12688e-06, acc 1\n",
      "2019-01-26T23:45:52.402037: step 2941, loss 0.000573532, acc 1\n",
      "2019-01-26T23:45:52.851900: step 2942, loss 4.68818e-05, acc 1\n",
      "2019-01-26T23:45:53.297571: step 2943, loss 5.31127e-05, acc 1\n",
      "2019-01-26T23:45:53.745805: step 2944, loss 3.64769e-05, acc 1\n",
      "2019-01-26T23:45:54.185901: step 2945, loss 1.1064e-06, acc 1\n",
      "2019-01-26T23:45:54.616071: step 2946, loss 3.7184e-05, acc 1\n",
      "2019-01-26T23:45:55.054901: step 2947, loss 1.1188e-05, acc 1\n",
      "2019-01-26T23:45:55.484020: step 2948, loss 0.000154101, acc 1\n",
      "2019-01-26T23:45:55.918636: step 2949, loss 6.90836e-06, acc 1\n",
      "2019-01-26T23:45:56.347467: step 2950, loss 0.000153684, acc 1\n",
      "2019-01-26T23:45:56.791268: step 2951, loss 2.20355e-05, acc 1\n",
      "2019-01-26T23:45:57.217061: step 2952, loss 4.69505e-05, acc 1\n",
      "2019-01-26T23:45:57.654190: step 2953, loss 1.88497e-06, acc 1\n",
      "2019-01-26T23:45:58.076847: step 2954, loss 6.07591e-05, acc 1\n",
      "2019-01-26T23:45:58.498803: step 2955, loss 1.49658e-05, acc 1\n",
      "2019-01-26T23:45:58.934714: step 2956, loss 0.00099279, acc 1\n",
      "2019-01-26T23:45:59.368223: step 2957, loss 4.3454e-06, acc 1\n",
      "2019-01-26T23:45:59.796732: step 2958, loss 2.8855e-05, acc 1\n",
      "2019-01-26T23:46:00.235033: step 2959, loss 6.71588e-05, acc 1\n",
      "2019-01-26T23:46:00.680020: step 2960, loss 9.13309e-06, acc 1\n",
      "2019-01-26T23:46:01.127235: step 2961, loss 2.54528e-05, acc 1\n",
      "2019-01-26T23:46:01.571872: step 2962, loss 0.000188748, acc 1\n",
      "2019-01-26T23:46:02.022663: step 2963, loss 4.31934e-06, acc 1\n",
      "2019-01-26T23:46:02.464560: step 2964, loss 1.79209e-05, acc 1\n",
      "2019-01-26T23:46:02.918275: step 2965, loss 1.46289e-05, acc 1\n",
      "2019-01-26T23:46:03.373330: step 2966, loss 2.19645e-05, acc 1\n",
      "2019-01-26T23:46:03.839499: step 2967, loss 0.000113449, acc 1\n",
      "2019-01-26T23:46:04.302845: step 2968, loss 8.39569e-06, acc 1\n",
      "2019-01-26T23:46:04.773842: step 2969, loss 7.24271e-05, acc 1\n",
      "2019-01-26T23:46:05.282381: step 2970, loss 2.83084e-05, acc 1\n",
      "2019-01-26T23:46:05.765722: step 2971, loss 9.77183e-06, acc 1\n",
      "2019-01-26T23:46:06.250485: step 2972, loss 8.65429e-05, acc 1\n",
      "2019-01-26T23:46:06.735607: step 2973, loss 3.79127e-05, acc 1\n",
      "2019-01-26T23:46:07.223374: step 2974, loss 7.76891e-05, acc 1\n",
      "2019-01-26T23:46:07.712865: step 2975, loss 0.000355773, acc 1\n",
      "2019-01-26T23:46:08.202430: step 2976, loss 0.000204203, acc 1\n",
      "2019-01-26T23:46:08.686575: step 2977, loss 1.17507e-05, acc 1\n",
      "2019-01-26T23:46:09.168863: step 2978, loss 0.000168882, acc 1\n",
      "2019-01-26T23:46:09.654017: step 2979, loss 2.44e-06, acc 1\n",
      "2019-01-26T23:46:10.137512: step 2980, loss 5.16456e-05, acc 1\n",
      "2019-01-26T23:46:10.599991: step 2981, loss 0.000109496, acc 1\n",
      "2019-01-26T23:46:11.065861: step 2982, loss 0.00108381, acc 1\n",
      "2019-01-26T23:46:11.542344: step 2983, loss 0.00077788, acc 1\n",
      "2019-01-26T23:46:12.007478: step 2984, loss 6.16123e-05, acc 1\n",
      "2019-01-26T23:46:12.464643: step 2985, loss 0.00228054, acc 1\n",
      "2019-01-26T23:46:12.939869: step 2986, loss 9.23797e-06, acc 1\n",
      "2019-01-26T23:46:13.387827: step 2987, loss 5.73302e-06, acc 1\n",
      "2019-01-26T23:46:13.871407: step 2988, loss 0.000410002, acc 1\n",
      "2019-01-26T23:46:14.322677: step 2989, loss 9.47397e-06, acc 1\n",
      "2019-01-26T23:46:14.773064: step 2990, loss 0.00107534, acc 1\n",
      "2019-01-26T23:46:15.223634: step 2991, loss 6.2021e-06, acc 1\n",
      "2019-01-26T23:46:15.674598: step 2992, loss 0.0011241, acc 1\n",
      "2019-01-26T23:46:16.108227: step 2993, loss 7.72858e-05, acc 1\n",
      "2019-01-26T23:46:16.547845: step 2994, loss 0.000223686, acc 1\n",
      "2019-01-26T23:46:16.981086: step 2995, loss 4.68747e-05, acc 1\n",
      "2019-01-26T23:46:17.412605: step 2996, loss 0.000249908, acc 1\n",
      "2019-01-26T23:46:17.845838: step 2997, loss 5.22445e-05, acc 1\n",
      "2019-01-26T23:46:18.273670: step 2998, loss 0.000216434, acc 1\n",
      "2019-01-26T23:46:18.702358: step 2999, loss 9.98657e-05, acc 1\n",
      "2019-01-26T23:46:19.119226: step 3000, loss 4.64745e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:46:20.138062: step 3000, loss 0.0900565, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3000\n",
      "\n",
      "2019-01-26T23:46:20.639643: step 3001, loss 8.37331e-06, acc 1\n",
      "2019-01-26T23:46:20.820649: step 3002, loss 1.89456e-05, acc 1\n",
      "2019-01-26T23:46:21.244918: step 3003, loss 1.54366e-05, acc 1\n",
      "2019-01-26T23:46:21.665776: step 3004, loss 5.97907e-07, acc 1\n",
      "2019-01-26T23:46:22.082444: step 3005, loss 0.000127776, acc 1\n",
      "2019-01-26T23:46:22.493456: step 3006, loss 4.96182e-06, acc 1\n",
      "2019-01-26T23:46:22.918852: step 3007, loss 8.63576e-05, acc 1\n",
      "2019-01-26T23:46:23.335171: step 3008, loss 0.000854136, acc 1\n",
      "2019-01-26T23:46:23.757379: step 3009, loss 7.54092e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:46:24.178890: step 3010, loss 4.15466e-05, acc 1\n",
      "2019-01-26T23:46:24.589413: step 3011, loss 1.95613e-05, acc 1\n",
      "2019-01-26T23:46:25.005033: step 3012, loss 2.48744e-05, acc 1\n",
      "2019-01-26T23:46:25.415646: step 3013, loss 0.000118912, acc 1\n",
      "2019-01-26T23:46:25.832820: step 3014, loss 1.87509e-05, acc 1\n",
      "2019-01-26T23:46:26.248245: step 3015, loss 0.000709298, acc 1\n",
      "2019-01-26T23:46:26.668541: step 3016, loss 0.000426135, acc 1\n",
      "2019-01-26T23:46:27.109706: step 3017, loss 0.00136173, acc 1\n",
      "2019-01-26T23:46:27.535263: step 3018, loss 3.16083e-06, acc 1\n",
      "2019-01-26T23:46:27.977598: step 3019, loss 5.11514e-05, acc 1\n",
      "2019-01-26T23:46:28.417890: step 3020, loss 9.86713e-05, acc 1\n",
      "2019-01-26T23:46:28.867802: step 3021, loss 5.58224e-05, acc 1\n",
      "2019-01-26T23:46:29.312964: step 3022, loss 1.1225e-05, acc 1\n",
      "2019-01-26T23:46:29.758810: step 3023, loss 5.52031e-06, acc 1\n",
      "2019-01-26T23:46:30.218554: step 3024, loss 0.00014254, acc 1\n",
      "2019-01-26T23:46:30.676273: step 3025, loss 0.000894293, acc 1\n",
      "2019-01-26T23:46:31.119066: step 3026, loss 0.000414493, acc 1\n",
      "2019-01-26T23:46:31.564584: step 3027, loss 0.00178019, acc 1\n",
      "2019-01-26T23:46:32.007992: step 3028, loss 6.25812e-05, acc 1\n",
      "2019-01-26T23:46:32.448100: step 3029, loss 2.29372e-05, acc 1\n",
      "2019-01-26T23:46:32.894765: step 3030, loss 2.82522e-05, acc 1\n",
      "2019-01-26T23:46:33.331871: step 3031, loss 4.89117e-06, acc 1\n",
      "2019-01-26T23:46:33.781011: step 3032, loss 2.2201e-05, acc 1\n",
      "2019-01-26T23:46:34.222683: step 3033, loss 0.00931892, acc 1\n",
      "2019-01-26T23:46:34.674223: step 3034, loss 0.000109288, acc 1\n",
      "2019-01-26T23:46:35.115966: step 3035, loss 0.00012364, acc 1\n",
      "2019-01-26T23:46:35.559666: step 3036, loss 0.000803234, acc 1\n",
      "2019-01-26T23:46:35.989989: step 3037, loss 0.000140234, acc 1\n",
      "2019-01-26T23:46:36.415932: step 3038, loss 5.4416e-05, acc 1\n",
      "2019-01-26T23:46:36.852542: step 3039, loss 2.18484e-06, acc 1\n",
      "2019-01-26T23:46:37.279740: step 3040, loss 1.9239e-05, acc 1\n",
      "2019-01-26T23:46:37.702392: step 3041, loss 0.00317266, acc 1\n",
      "2019-01-26T23:46:38.127832: step 3042, loss 0.00129404, acc 1\n",
      "2019-01-26T23:46:38.561969: step 3043, loss 5.77227e-05, acc 1\n",
      "2019-01-26T23:46:39.018317: step 3044, loss 1.09907e-05, acc 1\n",
      "2019-01-26T23:46:39.447067: step 3045, loss 0.000126478, acc 1\n",
      "2019-01-26T23:46:39.877164: step 3046, loss 1.12941e-05, acc 1\n",
      "2019-01-26T23:46:40.315997: step 3047, loss 5.41901e-05, acc 1\n",
      "2019-01-26T23:46:40.763540: step 3048, loss 0.000130459, acc 1\n",
      "2019-01-26T23:46:41.203955: step 3049, loss 0.000110295, acc 1\n",
      "2019-01-26T23:46:41.651743: step 3050, loss 0.000150117, acc 1\n",
      "2019-01-26T23:46:42.090748: step 3051, loss 0.000188144, acc 1\n",
      "2019-01-26T23:46:42.526858: step 3052, loss 0.000572906, acc 1\n",
      "2019-01-26T23:46:42.965936: step 3053, loss 9.20963e-05, acc 1\n",
      "2019-01-26T23:46:43.394084: step 3054, loss 8.58505e-05, acc 1\n",
      "2019-01-26T23:46:43.826256: step 3055, loss 5.67488e-05, acc 1\n",
      "2019-01-26T23:46:44.257429: step 3056, loss 0.000149572, acc 1\n",
      "2019-01-26T23:46:44.692503: step 3057, loss 0.0063465, acc 1\n",
      "2019-01-26T23:46:45.116912: step 3058, loss 5.66186e-05, acc 1\n",
      "2019-01-26T23:46:45.552783: step 3059, loss 3.60582e-05, acc 1\n",
      "2019-01-26T23:46:45.984772: step 3060, loss 1.28907e-05, acc 1\n",
      "2019-01-26T23:46:46.410441: step 3061, loss 2.90759e-05, acc 1\n",
      "2019-01-26T23:46:46.848770: step 3062, loss 0.000699598, acc 1\n",
      "2019-01-26T23:46:47.275905: step 3063, loss 7.77969e-05, acc 1\n",
      "2019-01-26T23:46:47.707315: step 3064, loss 6.72511e-05, acc 1\n",
      "2019-01-26T23:46:48.135373: step 3065, loss 0.000113257, acc 1\n",
      "2019-01-26T23:46:48.561102: step 3066, loss 2.06748e-05, acc 1\n",
      "2019-01-26T23:46:48.995011: step 3067, loss 1.44704e-05, acc 1\n",
      "2019-01-26T23:46:49.438833: step 3068, loss 3.20552e-06, acc 1\n",
      "2019-01-26T23:46:49.884334: step 3069, loss 4.81862e-05, acc 1\n",
      "2019-01-26T23:46:50.323705: step 3070, loss 0.000443437, acc 1\n",
      "2019-01-26T23:46:50.756608: step 3071, loss 2.84412e-06, acc 1\n",
      "2019-01-26T23:46:51.193878: step 3072, loss 1.14196e-05, acc 1\n",
      "2019-01-26T23:46:51.625226: step 3073, loss 2.49401e-06, acc 1\n",
      "2019-01-26T23:46:52.055114: step 3074, loss 3.40104e-05, acc 1\n",
      "2019-01-26T23:46:52.487867: step 3075, loss 0.000438544, acc 1\n",
      "2019-01-26T23:46:52.915960: step 3076, loss 3.5314e-06, acc 1\n",
      "2019-01-26T23:46:53.342935: step 3077, loss 0.000240968, acc 1\n",
      "2019-01-26T23:46:53.771129: step 3078, loss 0.000204636, acc 1\n",
      "2019-01-26T23:46:54.206427: step 3079, loss 3.8313e-06, acc 1\n",
      "2019-01-26T23:46:54.645398: step 3080, loss 2.28058e-05, acc 1\n",
      "2019-01-26T23:46:54.833448: step 3081, loss 5.1294e-05, acc 1\n",
      "2019-01-26T23:46:55.268996: step 3082, loss 2.79201e-06, acc 1\n",
      "2019-01-26T23:46:55.705985: step 3083, loss 4.71602e-06, acc 1\n",
      "2019-01-26T23:46:56.141433: step 3084, loss 0.000307945, acc 1\n",
      "2019-01-26T23:46:56.576821: step 3085, loss 3.10516e-05, acc 1\n",
      "2019-01-26T23:46:57.012917: step 3086, loss 1.23643e-05, acc 1\n",
      "2019-01-26T23:46:57.452223: step 3087, loss 0.000308398, acc 1\n",
      "2019-01-26T23:46:57.894967: step 3088, loss 2.63046e-05, acc 1\n",
      "2019-01-26T23:46:58.335421: step 3089, loss 2.20527e-05, acc 1\n",
      "2019-01-26T23:46:58.764186: step 3090, loss 0.000322542, acc 1\n",
      "2019-01-26T23:46:59.194721: step 3091, loss 8.70327e-06, acc 1\n",
      "2019-01-26T23:46:59.626947: step 3092, loss 2.56717e-05, acc 1\n",
      "2019-01-26T23:47:00.055100: step 3093, loss 0.000259603, acc 1\n",
      "2019-01-26T23:47:00.487031: step 3094, loss 1.30063e-05, acc 1\n",
      "2019-01-26T23:47:00.965212: step 3095, loss 7.9333e-05, acc 1\n",
      "2019-01-26T23:47:01.400584: step 3096, loss 2.18899e-05, acc 1\n",
      "2019-01-26T23:47:01.840857: step 3097, loss 9.04141e-06, acc 1\n",
      "2019-01-26T23:47:02.273899: step 3098, loss 6.89175e-07, acc 1\n",
      "2019-01-26T23:47:02.705468: step 3099, loss 0.00176659, acc 1\n",
      "2019-01-26T23:47:03.134181: step 3100, loss 0.0260867, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:47:04.203699: step 3100, loss 0.0899595, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3100\n",
      "\n",
      "2019-01-26T23:47:04.715881: step 3101, loss 0.000749138, acc 1\n",
      "2019-01-26T23:47:05.148306: step 3102, loss 2.07123e-06, acc 1\n",
      "2019-01-26T23:47:05.598075: step 3103, loss 0.000117869, acc 1\n",
      "2019-01-26T23:47:06.041522: step 3104, loss 1.98267e-05, acc 1\n",
      "2019-01-26T23:47:06.474847: step 3105, loss 0.000378201, acc 1\n",
      "2019-01-26T23:47:06.908049: step 3106, loss 0.000137492, acc 1\n",
      "2019-01-26T23:47:07.341683: step 3107, loss 1.60691e-05, acc 1\n",
      "2019-01-26T23:47:07.783906: step 3108, loss 4.45806e-05, acc 1\n",
      "2019-01-26T23:47:08.215819: step 3109, loss 5.11268e-06, acc 1\n",
      "2019-01-26T23:47:08.652890: step 3110, loss 1.50077e-05, acc 1\n",
      "2019-01-26T23:47:09.093444: step 3111, loss 3.82041e-05, acc 1\n",
      "2019-01-26T23:47:09.533346: step 3112, loss 3.88789e-05, acc 1\n",
      "2019-01-26T23:47:09.962067: step 3113, loss 1.4603e-06, acc 1\n",
      "2019-01-26T23:47:10.399666: step 3114, loss 1.19696e-05, acc 1\n",
      "2019-01-26T23:47:10.829022: step 3115, loss 7.62724e-05, acc 1\n",
      "2019-01-26T23:47:11.260044: step 3116, loss 3.23023e-05, acc 1\n",
      "2019-01-26T23:47:11.739164: step 3117, loss 5.46986e-05, acc 1\n",
      "2019-01-26T23:47:12.173820: step 3118, loss 0.000213872, acc 1\n",
      "2019-01-26T23:47:12.610541: step 3119, loss 1.47146e-06, acc 1\n",
      "2019-01-26T23:47:13.055735: step 3120, loss 0.000408571, acc 1\n",
      "2019-01-26T23:47:13.497099: step 3121, loss 0.000219527, acc 1\n",
      "2019-01-26T23:47:13.942905: step 3122, loss 0.0200636, acc 0.984375\n",
      "2019-01-26T23:47:14.389804: step 3123, loss 0.000353346, acc 1\n",
      "2019-01-26T23:47:14.818009: step 3124, loss 7.45148e-05, acc 1\n",
      "2019-01-26T23:47:15.250190: step 3125, loss 4.19353e-05, acc 1\n",
      "2019-01-26T23:47:15.687274: step 3126, loss 3.87364e-05, acc 1\n",
      "2019-01-26T23:47:16.115593: step 3127, loss 4.03107e-05, acc 1\n",
      "2019-01-26T23:47:16.544825: step 3128, loss 0.00297695, acc 1\n",
      "2019-01-26T23:47:16.979622: step 3129, loss 0.00235528, acc 1\n",
      "2019-01-26T23:47:17.413502: step 3130, loss 2.00666e-05, acc 1\n",
      "2019-01-26T23:47:17.853872: step 3131, loss 0.000138005, acc 1\n",
      "2019-01-26T23:47:18.288430: step 3132, loss 0.000238101, acc 1\n",
      "2019-01-26T23:47:18.716021: step 3133, loss 0.000228905, acc 1\n",
      "2019-01-26T23:47:19.150984: step 3134, loss 0.000728224, acc 1\n",
      "2019-01-26T23:47:19.589390: step 3135, loss 2.85019e-05, acc 1\n",
      "2019-01-26T23:47:20.029188: step 3136, loss 0.000214115, acc 1\n",
      "2019-01-26T23:47:20.462741: step 3137, loss 0.000197392, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:47:20.901415: step 3138, loss 0.000478346, acc 1\n",
      "2019-01-26T23:47:21.338391: step 3139, loss 0.000171377, acc 1\n",
      "2019-01-26T23:47:21.782973: step 3140, loss 2.56038e-05, acc 1\n",
      "2019-01-26T23:47:22.224936: step 3141, loss 0.00314518, acc 1\n",
      "2019-01-26T23:47:22.653952: step 3142, loss 4.08102e-05, acc 1\n",
      "2019-01-26T23:47:23.079108: step 3143, loss 0.000195818, acc 1\n",
      "2019-01-26T23:47:23.504580: step 3144, loss 4.67462e-05, acc 1\n",
      "2019-01-26T23:47:23.934953: step 3145, loss 0.000128936, acc 1\n",
      "2019-01-26T23:47:24.359883: step 3146, loss 0.000115416, acc 1\n",
      "2019-01-26T23:47:24.787523: step 3147, loss 6.51212e-05, acc 1\n",
      "2019-01-26T23:47:25.215814: step 3148, loss 0.000115853, acc 1\n",
      "2019-01-26T23:47:25.645042: step 3149, loss 1.68504e-05, acc 1\n",
      "2019-01-26T23:47:26.073735: step 3150, loss 5.17504e-05, acc 1\n",
      "2019-01-26T23:47:26.503951: step 3151, loss 2.12658e-05, acc 1\n",
      "2019-01-26T23:47:26.939318: step 3152, loss 5.29399e-05, acc 1\n",
      "2019-01-26T23:47:27.382055: step 3153, loss 0.000125357, acc 1\n",
      "2019-01-26T23:47:27.813150: step 3154, loss 0.000100968, acc 1\n",
      "2019-01-26T23:47:28.237170: step 3155, loss 2.29379e-05, acc 1\n",
      "2019-01-26T23:47:28.674159: step 3156, loss 8.90577e-05, acc 1\n",
      "2019-01-26T23:47:29.118304: step 3157, loss 4.50372e-05, acc 1\n",
      "2019-01-26T23:47:29.581162: step 3158, loss 0.00024074, acc 1\n",
      "2019-01-26T23:47:30.044843: step 3159, loss 0.00274203, acc 1\n",
      "2019-01-26T23:47:30.229787: step 3160, loss 4.87518e-05, acc 1\n",
      "2019-01-26T23:47:30.678600: step 3161, loss 0.00513399, acc 1\n",
      "2019-01-26T23:47:31.127905: step 3162, loss 5.41626e-05, acc 1\n",
      "2019-01-26T23:47:31.593280: step 3163, loss 0.000161234, acc 1\n",
      "2019-01-26T23:47:32.055279: step 3164, loss 6.60277e-06, acc 1\n",
      "2019-01-26T23:47:32.518566: step 3165, loss 0.000285116, acc 1\n",
      "2019-01-26T23:47:32.987658: step 3166, loss 0.00025069, acc 1\n",
      "2019-01-26T23:47:33.463809: step 3167, loss 8.26924e-06, acc 1\n",
      "2019-01-26T23:47:33.952709: step 3168, loss 0.000512368, acc 1\n",
      "2019-01-26T23:47:34.434848: step 3169, loss 0.0110631, acc 1\n",
      "2019-01-26T23:47:34.914127: step 3170, loss 1.53475e-05, acc 1\n",
      "2019-01-26T23:47:35.390051: step 3171, loss 1.92233e-05, acc 1\n",
      "2019-01-26T23:47:35.876970: step 3172, loss 0.000341151, acc 1\n",
      "2019-01-26T23:47:36.357335: step 3173, loss 0.000484858, acc 1\n",
      "2019-01-26T23:47:36.841857: step 3174, loss 0.00201266, acc 1\n",
      "2019-01-26T23:47:37.321970: step 3175, loss 3.93224e-05, acc 1\n",
      "2019-01-26T23:47:37.804658: step 3176, loss 0.000352273, acc 1\n",
      "2019-01-26T23:47:38.276700: step 3177, loss 0.000453701, acc 1\n",
      "2019-01-26T23:47:38.741427: step 3178, loss 5.16785e-05, acc 1\n",
      "2019-01-26T23:47:39.203030: step 3179, loss 1.13839e-05, acc 1\n",
      "2019-01-26T23:47:39.664183: step 3180, loss 0.000422366, acc 1\n",
      "2019-01-26T23:47:40.124008: step 3181, loss 1.62029e-05, acc 1\n",
      "2019-01-26T23:47:40.588587: step 3182, loss 1.40573e-05, acc 1\n",
      "2019-01-26T23:47:41.043746: step 3183, loss 1.70225e-05, acc 1\n",
      "2019-01-26T23:47:41.492977: step 3184, loss 2.38198e-05, acc 1\n",
      "2019-01-26T23:47:41.942016: step 3185, loss 0.00033183, acc 1\n",
      "2019-01-26T23:47:42.388007: step 3186, loss 0.00031145, acc 1\n",
      "2019-01-26T23:47:42.839090: step 3187, loss 0.000660935, acc 1\n",
      "2019-01-26T23:47:43.288112: step 3188, loss 3.818e-06, acc 1\n",
      "2019-01-26T23:47:43.739905: step 3189, loss 1.11344e-05, acc 1\n",
      "2019-01-26T23:47:44.189162: step 3190, loss 6.07734e-06, acc 1\n",
      "2019-01-26T23:47:44.642066: step 3191, loss 2.64496e-05, acc 1\n",
      "2019-01-26T23:47:45.096302: step 3192, loss 2.47181e-05, acc 1\n",
      "2019-01-26T23:47:45.566416: step 3193, loss 2.56294e-06, acc 1\n",
      "2019-01-26T23:47:46.048007: step 3194, loss 3.69683e-05, acc 1\n",
      "2019-01-26T23:47:46.490109: step 3195, loss 5.93044e-06, acc 1\n",
      "2019-01-26T23:47:46.937622: step 3196, loss 2.06185e-06, acc 1\n",
      "2019-01-26T23:47:47.489440: step 3197, loss 2.63227e-05, acc 1\n",
      "2019-01-26T23:47:47.981224: step 3198, loss 1.61436e-05, acc 1\n",
      "2019-01-26T23:47:48.451291: step 3199, loss 0.00125736, acc 1\n",
      "2019-01-26T23:47:48.928738: step 3200, loss 8.68438e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:47:50.157733: step 3200, loss 0.0926478, acc 0.983842\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3200\n",
      "\n",
      "2019-01-26T23:47:50.681067: step 3201, loss 0.000122207, acc 1\n",
      "2019-01-26T23:47:51.109904: step 3202, loss 2.13266e-06, acc 1\n",
      "2019-01-26T23:47:51.547346: step 3203, loss 4.07715e-06, acc 1\n",
      "2019-01-26T23:47:51.993755: step 3204, loss 0.00132531, acc 1\n",
      "2019-01-26T23:47:52.450712: step 3205, loss 0.00210602, acc 1\n",
      "2019-01-26T23:47:52.891772: step 3206, loss 5.53356e-06, acc 1\n",
      "2019-01-26T23:47:53.327102: step 3207, loss 4.28128e-05, acc 1\n",
      "2019-01-26T23:47:53.758790: step 3208, loss 1.04125e-05, acc 1\n",
      "2019-01-26T23:47:54.187646: step 3209, loss 1.05291e-05, acc 1\n",
      "2019-01-26T23:47:54.625334: step 3210, loss 4.26899e-06, acc 1\n",
      "2019-01-26T23:47:55.063626: step 3211, loss 7.60574e-06, acc 1\n",
      "2019-01-26T23:47:55.497388: step 3212, loss 7.27469e-06, acc 1\n",
      "2019-01-26T23:47:55.930768: step 3213, loss 1.12401e-05, acc 1\n",
      "2019-01-26T23:47:56.375129: step 3214, loss 0.00701874, acc 1\n",
      "2019-01-26T23:47:56.828707: step 3215, loss 8.35869e-06, acc 1\n",
      "2019-01-26T23:47:57.278117: step 3216, loss 0.00092267, acc 1\n",
      "2019-01-26T23:47:57.795087: step 3217, loss 1.45042e-05, acc 1\n",
      "2019-01-26T23:47:58.288139: step 3218, loss 1.39203e-05, acc 1\n",
      "2019-01-26T23:47:58.742062: step 3219, loss 1.04644e-05, acc 1\n",
      "2019-01-26T23:47:59.198646: step 3220, loss 4.93214e-05, acc 1\n",
      "2019-01-26T23:47:59.665324: step 3221, loss 0.000155216, acc 1\n",
      "2019-01-26T23:48:00.131092: step 3222, loss 0.00238448, acc 1\n",
      "2019-01-26T23:48:00.638729: step 3223, loss 4.17773e-06, acc 1\n",
      "2019-01-26T23:48:01.120650: step 3224, loss 3.02631e-05, acc 1\n",
      "2019-01-26T23:48:01.612807: step 3225, loss 2.7087e-05, acc 1\n",
      "2019-01-26T23:48:02.099171: step 3226, loss 0.000279418, acc 1\n",
      "2019-01-26T23:48:02.640371: step 3227, loss 7.49386e-05, acc 1\n",
      "2019-01-26T23:48:03.188001: step 3228, loss 7.46774e-05, acc 1\n",
      "2019-01-26T23:48:03.724164: step 3229, loss 4.48407e-05, acc 1\n",
      "2019-01-26T23:48:04.242621: step 3230, loss 3.25584e-06, acc 1\n",
      "2019-01-26T23:48:04.857663: step 3231, loss 1.84494e-05, acc 1\n",
      "2019-01-26T23:48:05.351103: step 3232, loss 4.38439e-06, acc 1\n",
      "2019-01-26T23:48:05.844764: step 3233, loss 1.6839e-05, acc 1\n",
      "2019-01-26T23:48:06.335055: step 3234, loss 3.30276e-05, acc 1\n",
      "2019-01-26T23:48:06.839734: step 3235, loss 0.0234523, acc 0.984375\n",
      "2019-01-26T23:48:07.303385: step 3236, loss 5.21711e-06, acc 1\n",
      "2019-01-26T23:48:07.776943: step 3237, loss 2.92453e-05, acc 1\n",
      "2019-01-26T23:48:08.281981: step 3238, loss 0.00014902, acc 1\n",
      "2019-01-26T23:48:08.484785: step 3239, loss 3.80423e-06, acc 1\n",
      "2019-01-26T23:48:08.969997: step 3240, loss 2.52665e-05, acc 1\n",
      "2019-01-26T23:48:09.473378: step 3241, loss 1.9011e-05, acc 1\n",
      "2019-01-26T23:48:09.954283: step 3242, loss 8.69842e-07, acc 1\n",
      "2019-01-26T23:48:10.422931: step 3243, loss 2.58725e-05, acc 1\n",
      "2019-01-26T23:48:10.882523: step 3244, loss 7.92301e-06, acc 1\n",
      "2019-01-26T23:48:11.387723: step 3245, loss 0.000675542, acc 1\n",
      "2019-01-26T23:48:11.870509: step 3246, loss 1.41746e-06, acc 1\n",
      "2019-01-26T23:48:12.341787: step 3247, loss 7.39181e-06, acc 1\n",
      "2019-01-26T23:48:12.803249: step 3248, loss 0.00101497, acc 1\n",
      "2019-01-26T23:48:13.266845: step 3249, loss 0.0172891, acc 0.984375\n",
      "2019-01-26T23:48:13.738525: step 3250, loss 1.20285e-05, acc 1\n",
      "2019-01-26T23:48:14.189609: step 3251, loss 0.000153554, acc 1\n",
      "2019-01-26T23:48:14.678333: step 3252, loss 1.63809e-05, acc 1\n",
      "2019-01-26T23:48:15.137610: step 3253, loss 0.000296511, acc 1\n",
      "2019-01-26T23:48:15.574187: step 3254, loss 4.83683e-06, acc 1\n",
      "2019-01-26T23:48:16.016767: step 3255, loss 0.000253316, acc 1\n",
      "2019-01-26T23:48:16.449058: step 3256, loss 1.3094e-06, acc 1\n",
      "2019-01-26T23:48:16.874700: step 3257, loss 5.4125e-06, acc 1\n",
      "2019-01-26T23:48:17.317250: step 3258, loss 9.2856e-05, acc 1\n",
      "2019-01-26T23:48:17.744594: step 3259, loss 2.25184e-06, acc 1\n",
      "2019-01-26T23:48:18.272267: step 3260, loss 0.000175751, acc 1\n",
      "2019-01-26T23:48:18.725764: step 3261, loss 9.59814e-05, acc 1\n",
      "2019-01-26T23:48:19.169929: step 3262, loss 0.00026609, acc 1\n",
      "2019-01-26T23:48:19.606200: step 3263, loss 2.36473e-05, acc 1\n",
      "2019-01-26T23:48:20.021601: step 3264, loss 3.06835e-05, acc 1\n",
      "2019-01-26T23:48:20.452780: step 3265, loss 0.00149339, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:48:20.862547: step 3266, loss 1.34745e-05, acc 1\n",
      "2019-01-26T23:48:21.332935: step 3267, loss 1.166e-06, acc 1\n",
      "2019-01-26T23:48:21.751702: step 3268, loss 3.22773e-05, acc 1\n",
      "2019-01-26T23:48:22.163397: step 3269, loss 1.72663e-06, acc 1\n",
      "2019-01-26T23:48:22.569073: step 3270, loss 1.54942e-05, acc 1\n",
      "2019-01-26T23:48:22.974930: step 3271, loss 1.83281e-06, acc 1\n",
      "2019-01-26T23:48:23.402744: step 3272, loss 4.28069e-05, acc 1\n",
      "2019-01-26T23:48:23.831986: step 3273, loss 7.93956e-06, acc 1\n",
      "2019-01-26T23:48:24.292987: step 3274, loss 4.17769e-06, acc 1\n",
      "2019-01-26T23:48:24.702078: step 3275, loss 9.60532e-05, acc 1\n",
      "2019-01-26T23:48:25.164918: step 3276, loss 1.0465e-05, acc 1\n",
      "2019-01-26T23:48:25.594712: step 3277, loss 4.01723e-05, acc 1\n",
      "2019-01-26T23:48:26.004595: step 3278, loss 0.000179117, acc 1\n",
      "2019-01-26T23:48:26.421026: step 3279, loss 6.2133e-06, acc 1\n",
      "2019-01-26T23:48:26.827639: step 3280, loss 8.11248e-06, acc 1\n",
      "2019-01-26T23:48:27.267905: step 3281, loss 8.06481e-05, acc 1\n",
      "2019-01-26T23:48:27.688289: step 3282, loss 3.3873e-05, acc 1\n",
      "2019-01-26T23:48:28.113617: step 3283, loss 2.12698e-05, acc 1\n",
      "2019-01-26T23:48:28.521642: step 3284, loss 2.68272e-05, acc 1\n",
      "2019-01-26T23:48:28.927609: step 3285, loss 0.000813964, acc 1\n",
      "2019-01-26T23:48:29.330362: step 3286, loss 0.000140172, acc 1\n",
      "2019-01-26T23:48:29.749154: step 3287, loss 0.000841241, acc 1\n",
      "2019-01-26T23:48:30.163099: step 3288, loss 1.63063e-05, acc 1\n",
      "2019-01-26T23:48:30.562463: step 3289, loss 2.62254e-06, acc 1\n",
      "2019-01-26T23:48:30.977850: step 3290, loss 3.47563e-05, acc 1\n",
      "2019-01-26T23:48:31.385950: step 3291, loss 1.4308e-05, acc 1\n",
      "2019-01-26T23:48:31.790849: step 3292, loss 0.000195585, acc 1\n",
      "2019-01-26T23:48:32.194236: step 3293, loss 3.45656e-05, acc 1\n",
      "2019-01-26T23:48:32.603311: step 3294, loss 3.1551e-05, acc 1\n",
      "2019-01-26T23:48:33.012164: step 3295, loss 5.01841e-05, acc 1\n",
      "2019-01-26T23:48:33.412486: step 3296, loss 0.000102452, acc 1\n",
      "2019-01-26T23:48:33.817705: step 3297, loss 0.000125931, acc 1\n",
      "2019-01-26T23:48:34.225741: step 3298, loss 0.00182825, acc 1\n",
      "2019-01-26T23:48:34.635169: step 3299, loss 3.01734e-05, acc 1\n",
      "2019-01-26T23:48:35.047441: step 3300, loss 0.000204561, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:48:36.092295: step 3300, loss 0.0876332, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3300\n",
      "\n",
      "2019-01-26T23:48:36.604486: step 3301, loss 1.5981e-06, acc 1\n",
      "2019-01-26T23:48:37.038364: step 3302, loss 1.54596e-06, acc 1\n",
      "2019-01-26T23:48:37.471381: step 3303, loss 7.38291e-06, acc 1\n",
      "2019-01-26T23:48:37.913975: step 3304, loss 4.20357e-05, acc 1\n",
      "2019-01-26T23:48:38.355344: step 3305, loss 6.56343e-06, acc 1\n",
      "2019-01-26T23:48:38.853848: step 3306, loss 0.000164454, acc 1\n",
      "2019-01-26T23:48:39.316592: step 3307, loss 1.10816e-05, acc 1\n",
      "2019-01-26T23:48:39.800852: step 3308, loss 0.000176332, acc 1\n",
      "2019-01-26T23:48:40.270394: step 3309, loss 0.000100022, acc 1\n",
      "2019-01-26T23:48:40.737684: step 3310, loss 6.35531e-05, acc 1\n",
      "2019-01-26T23:48:41.219009: step 3311, loss 9.57392e-07, acc 1\n",
      "2019-01-26T23:48:41.716031: step 3312, loss 8.8669e-05, acc 1\n",
      "2019-01-26T23:48:42.201404: step 3313, loss 1.768e-05, acc 1\n",
      "2019-01-26T23:48:42.764065: step 3314, loss 2.82806e-05, acc 1\n",
      "2019-01-26T23:48:43.344567: step 3315, loss 4.64706e-06, acc 1\n",
      "2019-01-26T23:48:43.902148: step 3316, loss 0.000306765, acc 1\n",
      "2019-01-26T23:48:44.402147: step 3317, loss 6.57511e-07, acc 1\n",
      "2019-01-26T23:48:44.612745: step 3318, loss 8.36988e-06, acc 1\n",
      "2019-01-26T23:48:45.113532: step 3319, loss 1.34624e-05, acc 1\n",
      "2019-01-26T23:48:45.685568: step 3320, loss 7.02105e-05, acc 1\n",
      "2019-01-26T23:48:46.221138: step 3321, loss 3.62218e-05, acc 1\n",
      "2019-01-26T23:48:46.762623: step 3322, loss 7.89306e-06, acc 1\n",
      "2019-01-26T23:48:47.300829: step 3323, loss 9.25677e-06, acc 1\n",
      "2019-01-26T23:48:47.785907: step 3324, loss 2.43525e-05, acc 1\n",
      "2019-01-26T23:48:48.275863: step 3325, loss 7.54037e-06, acc 1\n",
      "2019-01-26T23:48:48.744030: step 3326, loss 4.11387e-05, acc 1\n",
      "2019-01-26T23:48:49.236834: step 3327, loss 0.00587945, acc 1\n",
      "2019-01-26T23:48:49.803101: step 3328, loss 1.66953e-05, acc 1\n",
      "2019-01-26T23:48:50.335958: step 3329, loss 2.09572e-05, acc 1\n",
      "2019-01-26T23:48:50.842902: step 3330, loss 1.24923e-05, acc 1\n",
      "2019-01-26T23:48:51.340823: step 3331, loss 6.97044e-05, acc 1\n",
      "2019-01-26T23:48:51.899361: step 3332, loss 3.43583e-05, acc 1\n",
      "2019-01-26T23:48:52.376993: step 3333, loss 0.00140536, acc 1\n",
      "2019-01-26T23:48:52.839205: step 3334, loss 8.39278e-06, acc 1\n",
      "2019-01-26T23:48:53.376139: step 3335, loss 6.884e-06, acc 1\n",
      "2019-01-26T23:48:53.883025: step 3336, loss 3.07514e-05, acc 1\n",
      "2019-01-26T23:48:54.326063: step 3337, loss 5.09402e-05, acc 1\n",
      "2019-01-26T23:48:54.856218: step 3338, loss 2.22954e-06, acc 1\n",
      "2019-01-26T23:48:55.289918: step 3339, loss 8.478e-05, acc 1\n",
      "2019-01-26T23:48:55.725198: step 3340, loss 1.35671e-05, acc 1\n",
      "2019-01-26T23:48:56.141879: step 3341, loss 0.000118254, acc 1\n",
      "2019-01-26T23:48:56.658279: step 3342, loss 1.07832e-05, acc 1\n",
      "2019-01-26T23:48:57.116778: step 3343, loss 0.000162946, acc 1\n",
      "2019-01-26T23:48:57.544873: step 3344, loss 0.00016194, acc 1\n",
      "2019-01-26T23:48:58.061780: step 3345, loss 1.0925e-05, acc 1\n",
      "2019-01-26T23:48:58.579324: step 3346, loss 1.42083e-05, acc 1\n",
      "2019-01-26T23:48:59.057232: step 3347, loss 0.000194537, acc 1\n",
      "2019-01-26T23:48:59.470666: step 3348, loss 5.08272e-05, acc 1\n",
      "2019-01-26T23:49:00.024616: step 3349, loss 0.000295193, acc 1\n",
      "2019-01-26T23:49:00.442755: step 3350, loss 4.00273e-06, acc 1\n",
      "2019-01-26T23:49:00.869821: step 3351, loss 5.92485e-06, acc 1\n",
      "2019-01-26T23:49:01.293416: step 3352, loss 4.08653e-06, acc 1\n",
      "2019-01-26T23:49:01.708730: step 3353, loss 1.60957e-05, acc 1\n",
      "2019-01-26T23:49:02.132035: step 3354, loss 1.95762e-06, acc 1\n",
      "2019-01-26T23:49:02.633476: step 3355, loss 8.34864e-06, acc 1\n",
      "2019-01-26T23:49:03.200616: step 3356, loss 5.79154e-05, acc 1\n",
      "2019-01-26T23:49:03.618230: step 3357, loss 2.57918e-05, acc 1\n",
      "2019-01-26T23:49:04.025955: step 3358, loss 6.63035e-05, acc 1\n",
      "2019-01-26T23:49:04.466276: step 3359, loss 0.000223839, acc 1\n",
      "2019-01-26T23:49:04.879726: step 3360, loss 0.000159697, acc 1\n",
      "2019-01-26T23:49:05.349368: step 3361, loss 1.02947e-05, acc 1\n",
      "2019-01-26T23:49:05.770922: step 3362, loss 6.39177e-06, acc 1\n",
      "2019-01-26T23:49:06.210257: step 3363, loss 0.0139786, acc 0.984375\n",
      "2019-01-26T23:49:06.621180: step 3364, loss 7.65823e-05, acc 1\n",
      "2019-01-26T23:49:07.034959: step 3365, loss 0.00028511, acc 1\n",
      "2019-01-26T23:49:07.460667: step 3366, loss 3.01851e-05, acc 1\n",
      "2019-01-26T23:49:07.880754: step 3367, loss 2.52909e-05, acc 1\n",
      "2019-01-26T23:49:08.288428: step 3368, loss 3.72158e-05, acc 1\n",
      "2019-01-26T23:49:08.696197: step 3369, loss 9.07362e-06, acc 1\n",
      "2019-01-26T23:49:09.097817: step 3370, loss 1.04017e-05, acc 1\n",
      "2019-01-26T23:49:09.494630: step 3371, loss 2.86847e-07, acc 1\n",
      "2019-01-26T23:49:09.912076: step 3372, loss 4.98404e-06, acc 1\n",
      "2019-01-26T23:49:10.327686: step 3373, loss 0.000196587, acc 1\n",
      "2019-01-26T23:49:10.745232: step 3374, loss 3.68042e-06, acc 1\n",
      "2019-01-26T23:49:11.163771: step 3375, loss 2.982e-06, acc 1\n",
      "2019-01-26T23:49:11.583820: step 3376, loss 0.000932711, acc 1\n",
      "2019-01-26T23:49:12.000875: step 3377, loss 1.95415e-05, acc 1\n",
      "2019-01-26T23:49:12.418210: step 3378, loss 1.48082e-05, acc 1\n",
      "2019-01-26T23:49:12.841719: step 3379, loss 4.90304e-05, acc 1\n",
      "2019-01-26T23:49:13.261133: step 3380, loss 5.88499e-05, acc 1\n",
      "2019-01-26T23:49:13.738104: step 3381, loss 0.000122432, acc 1\n",
      "2019-01-26T23:49:14.173552: step 3382, loss 9.68326e-06, acc 1\n",
      "2019-01-26T23:49:14.617340: step 3383, loss 4.97678e-06, acc 1\n",
      "2019-01-26T23:49:15.158575: step 3384, loss 5.4293e-06, acc 1\n",
      "2019-01-26T23:49:15.656827: step 3385, loss 1.92034e-06, acc 1\n",
      "2019-01-26T23:49:16.108631: step 3386, loss 0.00032246, acc 1\n",
      "2019-01-26T23:49:16.544285: step 3387, loss 2.198e-05, acc 1\n",
      "2019-01-26T23:49:17.116686: step 3388, loss 2.0973e-06, acc 1\n",
      "2019-01-26T23:49:17.587975: step 3389, loss 3.61352e-07, acc 1\n",
      "2019-01-26T23:49:18.002216: step 3390, loss 4.46795e-06, acc 1\n",
      "2019-01-26T23:49:18.416992: step 3391, loss 0.000108271, acc 1\n",
      "2019-01-26T23:49:18.846894: step 3392, loss 4.89293e-06, acc 1\n",
      "2019-01-26T23:49:19.280442: step 3393, loss 0.000293261, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:49:19.738776: step 3394, loss 0.000180156, acc 1\n",
      "2019-01-26T23:49:20.175897: step 3395, loss 7.35738e-07, acc 1\n",
      "2019-01-26T23:49:20.591404: step 3396, loss 1.08033e-07, acc 1\n",
      "2019-01-26T23:49:20.766940: step 3397, loss 3.15397e-05, acc 1\n",
      "2019-01-26T23:49:21.195097: step 3398, loss 5.96611e-05, acc 1\n",
      "2019-01-26T23:49:21.612544: step 3399, loss 0.000420283, acc 1\n",
      "2019-01-26T23:49:22.022711: step 3400, loss 5.68853e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:49:23.073248: step 3400, loss 0.0932973, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3400\n",
      "\n",
      "2019-01-26T23:49:23.566427: step 3401, loss 5.94305e-06, acc 1\n",
      "2019-01-26T23:49:24.095132: step 3402, loss 4.92416e-05, acc 1\n",
      "2019-01-26T23:49:24.662884: step 3403, loss 3.23324e-06, acc 1\n",
      "2019-01-26T23:49:25.172552: step 3404, loss 2.68949e-06, acc 1\n",
      "2019-01-26T23:49:25.697875: step 3405, loss 9.83305e-05, acc 1\n",
      "2019-01-26T23:49:26.200135: step 3406, loss 1.20658e-05, acc 1\n",
      "2019-01-26T23:49:26.760035: step 3407, loss 3.20375e-07, acc 1\n",
      "2019-01-26T23:49:27.322987: step 3408, loss 3.72142e-06, acc 1\n",
      "2019-01-26T23:49:27.830960: step 3409, loss 3.86871e-05, acc 1\n",
      "2019-01-26T23:49:28.281500: step 3410, loss 9.22276e-06, acc 1\n",
      "2019-01-26T23:49:28.783470: step 3411, loss 1.60109e-05, acc 1\n",
      "2019-01-26T23:49:29.279369: step 3412, loss 2.17738e-06, acc 1\n",
      "2019-01-26T23:49:29.710582: step 3413, loss 0.000163455, acc 1\n",
      "2019-01-26T23:49:30.216333: step 3414, loss 0.0820142, acc 0.984375\n",
      "2019-01-26T23:49:30.667649: step 3415, loss 5.22394e-06, acc 1\n",
      "2019-01-26T23:49:31.118931: step 3416, loss 4.90207e-06, acc 1\n",
      "2019-01-26T23:49:31.671676: step 3417, loss 5.86685e-06, acc 1\n",
      "2019-01-26T23:49:32.223082: step 3418, loss 6.70547e-05, acc 1\n",
      "2019-01-26T23:49:32.685060: step 3419, loss 4.43479e-05, acc 1\n",
      "2019-01-26T23:49:33.131865: step 3420, loss 0.000154599, acc 1\n",
      "2019-01-26T23:49:33.587937: step 3421, loss 0.000227774, acc 1\n",
      "2019-01-26T23:49:34.032446: step 3422, loss 7.87897e-07, acc 1\n",
      "2019-01-26T23:49:34.483182: step 3423, loss 0.000112373, acc 1\n",
      "2019-01-26T23:49:34.994439: step 3424, loss 0.000507197, acc 1\n",
      "2019-01-26T23:49:35.432669: step 3425, loss 1.3178e-05, acc 1\n",
      "2019-01-26T23:49:35.858483: step 3426, loss 2.11287e-05, acc 1\n",
      "2019-01-26T23:49:36.283856: step 3427, loss 2.27269e-05, acc 1\n",
      "2019-01-26T23:49:36.713773: step 3428, loss 0.00880608, acc 1\n",
      "2019-01-26T23:49:37.129742: step 3429, loss 0.0437192, acc 0.984375\n",
      "2019-01-26T23:49:37.548270: step 3430, loss 0.000363276, acc 1\n",
      "2019-01-26T23:49:37.964070: step 3431, loss 0.000152792, acc 1\n",
      "2019-01-26T23:49:38.393435: step 3432, loss 3.63495e-05, acc 1\n",
      "2019-01-26T23:49:38.813681: step 3433, loss 7.33033e-06, acc 1\n",
      "2019-01-26T23:49:39.216288: step 3434, loss 2.32943e-05, acc 1\n",
      "2019-01-26T23:49:39.639700: step 3435, loss 7.11478e-05, acc 1\n",
      "2019-01-26T23:49:40.054965: step 3436, loss 6.45911e-05, acc 1\n",
      "2019-01-26T23:49:40.482038: step 3437, loss 4.00089e-05, acc 1\n",
      "2019-01-26T23:49:40.881688: step 3438, loss 1.86815e-06, acc 1\n",
      "2019-01-26T23:49:41.284843: step 3439, loss 0.000338494, acc 1\n",
      "2019-01-26T23:49:41.689854: step 3440, loss 1.27906e-05, acc 1\n",
      "2019-01-26T23:49:42.103736: step 3441, loss 1.01327e-06, acc 1\n",
      "2019-01-26T23:49:42.508745: step 3442, loss 7.1923e-06, acc 1\n",
      "2019-01-26T23:49:42.921450: step 3443, loss 7.45052e-07, acc 1\n",
      "2019-01-26T23:49:43.330241: step 3444, loss 0.00732847, acc 1\n",
      "2019-01-26T23:49:43.785250: step 3445, loss 7.13245e-05, acc 1\n",
      "2019-01-26T23:49:44.189831: step 3446, loss 2.74053e-05, acc 1\n",
      "2019-01-26T23:49:44.599167: step 3447, loss 2.29744e-05, acc 1\n",
      "2019-01-26T23:49:45.084913: step 3448, loss 2.03991e-05, acc 1\n",
      "2019-01-26T23:49:45.604814: step 3449, loss 0.00110612, acc 1\n",
      "2019-01-26T23:49:46.035556: step 3450, loss 2.48464e-06, acc 1\n",
      "2019-01-26T23:49:46.440249: step 3451, loss 0.000182819, acc 1\n",
      "2019-01-26T23:49:46.854811: step 3452, loss 4.14341e-05, acc 1\n",
      "2019-01-26T23:49:47.282900: step 3453, loss 0.00119992, acc 1\n",
      "2019-01-26T23:49:47.702538: step 3454, loss 0.0116167, acc 0.984375\n",
      "2019-01-26T23:49:48.126965: step 3455, loss 1.7361e-05, acc 1\n",
      "2019-01-26T23:49:48.557843: step 3456, loss 0.00232933, acc 1\n",
      "2019-01-26T23:49:49.001918: step 3457, loss 0.000314877, acc 1\n",
      "2019-01-26T23:49:49.469855: step 3458, loss 3.35807e-05, acc 1\n",
      "2019-01-26T23:49:49.954371: step 3459, loss 6.46331e-07, acc 1\n",
      "2019-01-26T23:49:50.407035: step 3460, loss 2.87988e-05, acc 1\n",
      "2019-01-26T23:49:50.855909: step 3461, loss 2.13454e-06, acc 1\n",
      "2019-01-26T23:49:51.299413: step 3462, loss 4.79173e-05, acc 1\n",
      "2019-01-26T23:49:51.740765: step 3463, loss 0.00489433, acc 1\n",
      "2019-01-26T23:49:52.173960: step 3464, loss 5.00469e-06, acc 1\n",
      "2019-01-26T23:49:52.622988: step 3465, loss 0.000174666, acc 1\n",
      "2019-01-26T23:49:53.064212: step 3466, loss 2.36887e-05, acc 1\n",
      "2019-01-26T23:49:53.496612: step 3467, loss 0.000237465, acc 1\n",
      "2019-01-26T23:49:53.950090: step 3468, loss 3.83326e-06, acc 1\n",
      "2019-01-26T23:49:54.380371: step 3469, loss 9.97391e-05, acc 1\n",
      "2019-01-26T23:49:54.829288: step 3470, loss 3.50354e-06, acc 1\n",
      "2019-01-26T23:49:55.285529: step 3471, loss 0.000108714, acc 1\n",
      "2019-01-26T23:49:55.913576: step 3472, loss 2.36061e-05, acc 1\n",
      "2019-01-26T23:49:56.450880: step 3473, loss 1.66435e-05, acc 1\n",
      "2019-01-26T23:49:56.933507: step 3474, loss 0.00100975, acc 1\n",
      "2019-01-26T23:49:57.423079: step 3475, loss 1.86706e-05, acc 1\n",
      "2019-01-26T23:49:57.623329: step 3476, loss 4.66396e-05, acc 1\n",
      "2019-01-26T23:49:58.042224: step 3477, loss 2.75511e-05, acc 1\n",
      "2019-01-26T23:49:58.506613: step 3478, loss 1.33882e-05, acc 1\n",
      "2019-01-26T23:49:59.049132: step 3479, loss 0.000151938, acc 1\n",
      "2019-01-26T23:49:59.595087: step 3480, loss 0.000407914, acc 1\n",
      "2019-01-26T23:50:00.157932: step 3481, loss 6.59313e-06, acc 1\n",
      "2019-01-26T23:50:00.817309: step 3482, loss 2.64083e-05, acc 1\n",
      "2019-01-26T23:50:01.461302: step 3483, loss 1.32295e-05, acc 1\n",
      "2019-01-26T23:50:02.117696: step 3484, loss 3.03834e-05, acc 1\n",
      "2019-01-26T23:50:02.644513: step 3485, loss 4.2868e-05, acc 1\n",
      "2019-01-26T23:50:03.168033: step 3486, loss 6.97506e-06, acc 1\n",
      "2019-01-26T23:50:03.657039: step 3487, loss 0.00103482, acc 1\n",
      "2019-01-26T23:50:04.102209: step 3488, loss 1.91637e-05, acc 1\n",
      "2019-01-26T23:50:04.585206: step 3489, loss 0.000195359, acc 1\n",
      "2019-01-26T23:50:05.075993: step 3490, loss 1.45838e-05, acc 1\n",
      "2019-01-26T23:50:05.509467: step 3491, loss 8.9639e-05, acc 1\n",
      "2019-01-26T23:50:05.979534: step 3492, loss 0.000923319, acc 1\n",
      "2019-01-26T23:50:06.400266: step 3493, loss 0.000217854, acc 1\n",
      "2019-01-26T23:50:06.880352: step 3494, loss 6.44987e-06, acc 1\n",
      "2019-01-26T23:50:07.319666: step 3495, loss 0.000212088, acc 1\n",
      "2019-01-26T23:50:07.725632: step 3496, loss 0.0539679, acc 0.984375\n",
      "2019-01-26T23:50:08.177000: step 3497, loss 0.00118757, acc 1\n",
      "2019-01-26T23:50:08.632785: step 3498, loss 1.98957e-05, acc 1\n",
      "2019-01-26T23:50:09.101719: step 3499, loss 3.79095e-05, acc 1\n",
      "2019-01-26T23:50:09.546058: step 3500, loss 8.83928e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:50:10.658974: step 3500, loss 0.0770943, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3500\n",
      "\n",
      "2019-01-26T23:50:11.320888: step 3501, loss 0.000527502, acc 1\n",
      "2019-01-26T23:50:11.956716: step 3502, loss 0.000299009, acc 1\n",
      "2019-01-26T23:50:12.383628: step 3503, loss 2.3747e-05, acc 1\n",
      "2019-01-26T23:50:12.834134: step 3504, loss 5.08242e-05, acc 1\n",
      "2019-01-26T23:50:13.257614: step 3505, loss 0.000277106, acc 1\n",
      "2019-01-26T23:50:13.688156: step 3506, loss 0.000432359, acc 1\n",
      "2019-01-26T23:50:14.247399: step 3507, loss 0.00230245, acc 1\n",
      "2019-01-26T23:50:14.812366: step 3508, loss 0.000894202, acc 1\n",
      "2019-01-26T23:50:15.324127: step 3509, loss 0.00050642, acc 1\n",
      "2019-01-26T23:50:15.847964: step 3510, loss 0.000538705, acc 1\n",
      "2019-01-26T23:50:16.284884: step 3511, loss 0.000294383, acc 1\n",
      "2019-01-26T23:50:16.727338: step 3512, loss 0.00428729, acc 1\n",
      "2019-01-26T23:50:17.198541: step 3513, loss 0.000134791, acc 1\n",
      "2019-01-26T23:50:17.656728: step 3514, loss 5.74054e-05, acc 1\n",
      "2019-01-26T23:50:18.110145: step 3515, loss 0.000187407, acc 1\n",
      "2019-01-26T23:50:18.596908: step 3516, loss 0.000485511, acc 1\n",
      "2019-01-26T23:50:19.063838: step 3517, loss 0.000818807, acc 1\n",
      "2019-01-26T23:50:19.513857: step 3518, loss 0.000133172, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:50:19.991326: step 3519, loss 0.000655009, acc 1\n",
      "2019-01-26T23:50:20.428594: step 3520, loss 0.002387, acc 1\n",
      "2019-01-26T23:50:20.901241: step 3521, loss 2.60183e-05, acc 1\n",
      "2019-01-26T23:50:21.338533: step 3522, loss 0.00216776, acc 1\n",
      "2019-01-26T23:50:21.813196: step 3523, loss 0.000452787, acc 1\n",
      "2019-01-26T23:50:22.270434: step 3524, loss 0.000153691, acc 1\n",
      "2019-01-26T23:50:22.732781: step 3525, loss 2.78027e-05, acc 1\n",
      "2019-01-26T23:50:23.172787: step 3526, loss 7.13897e-05, acc 1\n",
      "2019-01-26T23:50:23.628126: step 3527, loss 0.000190027, acc 1\n",
      "2019-01-26T23:50:24.054083: step 3528, loss 4.95823e-06, acc 1\n",
      "2019-01-26T23:50:24.480520: step 3529, loss 1.42028e-05, acc 1\n",
      "2019-01-26T23:50:24.907087: step 3530, loss 1.96318e-06, acc 1\n",
      "2019-01-26T23:50:25.330613: step 3531, loss 7.29486e-05, acc 1\n",
      "2019-01-26T23:50:25.772746: step 3532, loss 0.000127633, acc 1\n",
      "2019-01-26T23:50:26.194008: step 3533, loss 7.54361e-05, acc 1\n",
      "2019-01-26T23:50:26.767101: step 3534, loss 0.000258468, acc 1\n",
      "2019-01-26T23:50:27.401941: step 3535, loss 7.3904e-06, acc 1\n",
      "2019-01-26T23:50:27.847704: step 3536, loss 7.67062e-05, acc 1\n",
      "2019-01-26T23:50:28.280969: step 3537, loss 3.76241e-06, acc 1\n",
      "2019-01-26T23:50:28.728997: step 3538, loss 1.68752e-06, acc 1\n",
      "2019-01-26T23:50:29.155276: step 3539, loss 6.56217e-05, acc 1\n",
      "2019-01-26T23:50:29.590219: step 3540, loss 2.35338e-05, acc 1\n",
      "2019-01-26T23:50:30.043690: step 3541, loss 0.000463603, acc 1\n",
      "2019-01-26T23:50:30.515260: step 3542, loss 0.00731253, acc 1\n",
      "2019-01-26T23:50:31.116833: step 3543, loss 5.53744e-06, acc 1\n",
      "2019-01-26T23:50:31.540296: step 3544, loss 5.61914e-06, acc 1\n",
      "2019-01-26T23:50:31.965065: step 3545, loss 8.62185e-05, acc 1\n",
      "2019-01-26T23:50:32.408301: step 3546, loss 2.71815e-05, acc 1\n",
      "2019-01-26T23:50:32.833325: step 3547, loss 1.22623e-05, acc 1\n",
      "2019-01-26T23:50:33.269366: step 3548, loss 2.11032e-06, acc 1\n",
      "2019-01-26T23:50:33.744564: step 3549, loss 2.46377e-05, acc 1\n",
      "2019-01-26T23:50:34.182127: step 3550, loss 0.0008765, acc 1\n",
      "2019-01-26T23:50:34.641787: step 3551, loss 1.28932e-05, acc 1\n",
      "2019-01-26T23:50:35.070157: step 3552, loss 0.000129062, acc 1\n",
      "2019-01-26T23:50:35.509539: step 3553, loss 2.36739e-06, acc 1\n",
      "2019-01-26T23:50:35.933427: step 3554, loss 4.67203e-05, acc 1\n",
      "2019-01-26T23:50:36.129913: step 3555, loss 1.44118e-05, acc 1\n",
      "2019-01-26T23:50:36.559775: step 3556, loss 8.1231e-05, acc 1\n",
      "2019-01-26T23:50:37.057039: step 3557, loss 7.86296e-05, acc 1\n",
      "2019-01-26T23:50:37.512330: step 3558, loss 0.00010718, acc 1\n",
      "2019-01-26T23:50:37.941099: step 3559, loss 1.19231e-05, acc 1\n",
      "2019-01-26T23:50:38.383564: step 3560, loss 4.54097e-05, acc 1\n",
      "2019-01-26T23:50:38.823788: step 3561, loss 3.40273e-06, acc 1\n",
      "2019-01-26T23:50:39.252616: step 3562, loss 1.27749e-05, acc 1\n",
      "2019-01-26T23:50:39.679829: step 3563, loss 1.87935e-06, acc 1\n",
      "2019-01-26T23:50:40.116684: step 3564, loss 2.63125e-05, acc 1\n",
      "2019-01-26T23:50:40.543295: step 3565, loss 2.32745e-05, acc 1\n",
      "2019-01-26T23:50:40.964774: step 3566, loss 7.78581e-07, acc 1\n",
      "2019-01-26T23:50:41.380611: step 3567, loss 0.000169876, acc 1\n",
      "2019-01-26T23:50:41.839392: step 3568, loss 2.6189e-05, acc 1\n",
      "2019-01-26T23:50:42.266654: step 3569, loss 0.000900096, acc 1\n",
      "2019-01-26T23:50:42.697593: step 3570, loss 2.20347e-06, acc 1\n",
      "2019-01-26T23:50:43.121700: step 3571, loss 6.05356e-07, acc 1\n",
      "2019-01-26T23:50:43.575400: step 3572, loss 2.2033e-05, acc 1\n",
      "2019-01-26T23:50:44.046347: step 3573, loss 0.000148886, acc 1\n",
      "2019-01-26T23:50:44.497554: step 3574, loss 0.000172746, acc 1\n",
      "2019-01-26T23:50:44.946190: step 3575, loss 9.01356e-05, acc 1\n",
      "2019-01-26T23:50:45.432627: step 3576, loss 6.05683e-06, acc 1\n",
      "2019-01-26T23:50:46.045892: step 3577, loss 5.5274e-06, acc 1\n",
      "2019-01-26T23:50:46.707685: step 3578, loss 1.75531e-05, acc 1\n",
      "2019-01-26T23:50:47.212823: step 3579, loss 1.01965e-05, acc 1\n",
      "2019-01-26T23:50:47.685467: step 3580, loss 0.000179719, acc 1\n",
      "2019-01-26T23:50:48.143800: step 3581, loss 0.000152691, acc 1\n",
      "2019-01-26T23:50:48.613515: step 3582, loss 9.39699e-06, acc 1\n",
      "2019-01-26T23:50:49.072589: step 3583, loss 0.00051814, acc 1\n",
      "2019-01-26T23:50:49.514341: step 3584, loss 4.84823e-06, acc 1\n",
      "2019-01-26T23:50:49.985342: step 3585, loss 0.00654349, acc 1\n",
      "2019-01-26T23:50:50.437932: step 3586, loss 3.64238e-05, acc 1\n",
      "2019-01-26T23:50:50.904030: step 3587, loss 0.00041598, acc 1\n",
      "2019-01-26T23:50:51.350059: step 3588, loss 9.46659e-05, acc 1\n",
      "2019-01-26T23:50:51.804160: step 3589, loss 0.000145502, acc 1\n",
      "2019-01-26T23:50:52.253974: step 3590, loss 1.74153e-06, acc 1\n",
      "2019-01-26T23:50:52.701944: step 3591, loss 7.12394e-06, acc 1\n",
      "2019-01-26T23:50:53.134444: step 3592, loss 1.6447e-06, acc 1\n",
      "2019-01-26T23:50:53.568477: step 3593, loss 1.28916e-05, acc 1\n",
      "2019-01-26T23:50:53.995761: step 3594, loss 1.09709e-06, acc 1\n",
      "2019-01-26T23:50:54.449385: step 3595, loss 3.79776e-05, acc 1\n",
      "2019-01-26T23:50:54.937698: step 3596, loss 1.27196e-05, acc 1\n",
      "2019-01-26T23:50:55.418534: step 3597, loss 5.89615e-06, acc 1\n",
      "2019-01-26T23:50:55.940298: step 3598, loss 1.83097e-06, acc 1\n",
      "2019-01-26T23:50:56.365357: step 3599, loss 9.89325e-05, acc 1\n",
      "2019-01-26T23:50:56.795816: step 3600, loss 5.85571e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:50:58.023339: step 3600, loss 0.0873578, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3600\n",
      "\n",
      "2019-01-26T23:50:58.570077: step 3601, loss 8.61918e-05, acc 1\n",
      "2019-01-26T23:50:59.027662: step 3602, loss 8.10245e-07, acc 1\n",
      "2019-01-26T23:50:59.543526: step 3603, loss 0.000163581, acc 1\n",
      "2019-01-26T23:50:59.979219: step 3604, loss 5.0869e-05, acc 1\n",
      "2019-01-26T23:51:00.474301: step 3605, loss 4.94748e-05, acc 1\n",
      "2019-01-26T23:51:00.937980: step 3606, loss 2.96329e-05, acc 1\n",
      "2019-01-26T23:51:01.395079: step 3607, loss 7.43115e-06, acc 1\n",
      "2019-01-26T23:51:01.858135: step 3608, loss 6.4753e-06, acc 1\n",
      "2019-01-26T23:51:02.298443: step 3609, loss 3.21951e-05, acc 1\n",
      "2019-01-26T23:51:02.722016: step 3610, loss 3.64123e-06, acc 1\n",
      "2019-01-26T23:51:03.144498: step 3611, loss 3.51009e-05, acc 1\n",
      "2019-01-26T23:51:03.579489: step 3612, loss 9.29445e-07, acc 1\n",
      "2019-01-26T23:51:04.035670: step 3613, loss 0.000255419, acc 1\n",
      "2019-01-26T23:51:04.458952: step 3614, loss 1.40815e-06, acc 1\n",
      "2019-01-26T23:51:04.888017: step 3615, loss 1.5991e-05, acc 1\n",
      "2019-01-26T23:51:05.314379: step 3616, loss 2.39152e-06, acc 1\n",
      "2019-01-26T23:51:05.744483: step 3617, loss 0.000388758, acc 1\n",
      "2019-01-26T23:51:06.161439: step 3618, loss 4.17502e-05, acc 1\n",
      "2019-01-26T23:51:06.584545: step 3619, loss 0.00730908, acc 1\n",
      "2019-01-26T23:51:07.011854: step 3620, loss 6.94743e-05, acc 1\n",
      "2019-01-26T23:51:07.438387: step 3621, loss 0.000172636, acc 1\n",
      "2019-01-26T23:51:07.865368: step 3622, loss 3.49065e-05, acc 1\n",
      "2019-01-26T23:51:08.275440: step 3623, loss 1.68492e-05, acc 1\n",
      "2019-01-26T23:51:08.693075: step 3624, loss 3.7157e-05, acc 1\n",
      "2019-01-26T23:51:09.111209: step 3625, loss 0.000122182, acc 1\n",
      "2019-01-26T23:51:09.526982: step 3626, loss 5.92242e-05, acc 1\n",
      "2019-01-26T23:51:09.950292: step 3627, loss 0.00268629, acc 1\n",
      "2019-01-26T23:51:10.368338: step 3628, loss 2.61873e-06, acc 1\n",
      "2019-01-26T23:51:10.793193: step 3629, loss 1.83409e-05, acc 1\n",
      "2019-01-26T23:51:11.226903: step 3630, loss 6.40746e-07, acc 1\n",
      "2019-01-26T23:51:11.664034: step 3631, loss 0.00230601, acc 1\n",
      "2019-01-26T23:51:12.083112: step 3632, loss 3.61759e-05, acc 1\n",
      "2019-01-26T23:51:12.501343: step 3633, loss 3.54723e-05, acc 1\n",
      "2019-01-26T23:51:12.682298: step 3634, loss 0.000406837, acc 1\n",
      "2019-01-26T23:51:13.098456: step 3635, loss 5.23707e-05, acc 1\n",
      "2019-01-26T23:51:13.516578: step 3636, loss 1.1888e-05, acc 1\n",
      "2019-01-26T23:51:13.953077: step 3637, loss 0.00267472, acc 1\n",
      "2019-01-26T23:51:14.391562: step 3638, loss 3.09114e-05, acc 1\n",
      "2019-01-26T23:51:14.831599: step 3639, loss 1.20383e-05, acc 1\n",
      "2019-01-26T23:51:15.278585: step 3640, loss 0.0122914, acc 0.984375\n",
      "2019-01-26T23:51:15.728654: step 3641, loss 0.00012598, acc 1\n",
      "2019-01-26T23:51:16.180601: step 3642, loss 1.89697e-05, acc 1\n",
      "2019-01-26T23:51:16.633640: step 3643, loss 0.0013598, acc 1\n",
      "2019-01-26T23:51:17.102648: step 3644, loss 1.69498e-06, acc 1\n",
      "2019-01-26T23:51:17.567487: step 3645, loss 7.89449e-05, acc 1\n",
      "2019-01-26T23:51:18.012930: step 3646, loss 1.56832e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:51:18.467939: step 3647, loss 1.28372e-05, acc 1\n",
      "2019-01-26T23:51:18.916541: step 3648, loss 8.04657e-07, acc 1\n",
      "2019-01-26T23:51:19.354626: step 3649, loss 1.39882e-06, acc 1\n",
      "2019-01-26T23:51:19.801752: step 3650, loss 3.90688e-05, acc 1\n",
      "2019-01-26T23:51:20.249532: step 3651, loss 1.61862e-06, acc 1\n",
      "2019-01-26T23:51:20.697327: step 3652, loss 4.71198e-05, acc 1\n",
      "2019-01-26T23:51:21.136265: step 3653, loss 5.90273e-05, acc 1\n",
      "2019-01-26T23:51:21.589597: step 3654, loss 5.2469e-06, acc 1\n",
      "2019-01-26T23:51:22.051574: step 3655, loss 8.82604e-05, acc 1\n",
      "2019-01-26T23:51:22.493789: step 3656, loss 6.92899e-07, acc 1\n",
      "2019-01-26T23:51:22.935325: step 3657, loss 0.000296211, acc 1\n",
      "2019-01-26T23:51:23.366900: step 3658, loss 9.25466e-06, acc 1\n",
      "2019-01-26T23:51:23.798794: step 3659, loss 8.23357e-06, acc 1\n",
      "2019-01-26T23:51:24.222693: step 3660, loss 8.92202e-07, acc 1\n",
      "2019-01-26T23:51:24.654022: step 3661, loss 1.98749e-05, acc 1\n",
      "2019-01-26T23:51:25.082125: step 3662, loss 6.85393e-06, acc 1\n",
      "2019-01-26T23:51:25.509423: step 3663, loss 2.34103e-05, acc 1\n",
      "2019-01-26T23:51:25.946314: step 3664, loss 0.000978237, acc 1\n",
      "2019-01-26T23:51:26.363251: step 3665, loss 3.82927e-06, acc 1\n",
      "2019-01-26T23:51:26.777901: step 3666, loss 2.61335e-05, acc 1\n",
      "2019-01-26T23:51:27.211416: step 3667, loss 2.06003e-06, acc 1\n",
      "2019-01-26T23:51:27.629852: step 3668, loss 4.63369e-05, acc 1\n",
      "2019-01-26T23:51:28.047055: step 3669, loss 2.06583e-05, acc 1\n",
      "2019-01-26T23:51:28.464276: step 3670, loss 3.1869e-06, acc 1\n",
      "2019-01-26T23:51:28.879922: step 3671, loss 2.38945e-05, acc 1\n",
      "2019-01-26T23:51:29.284808: step 3672, loss 1.969e-05, acc 1\n",
      "2019-01-26T23:51:29.704546: step 3673, loss 1.42304e-06, acc 1\n",
      "2019-01-26T23:51:30.123617: step 3674, loss 2.74176e-06, acc 1\n",
      "2019-01-26T23:51:30.536687: step 3675, loss 2.71538e-05, acc 1\n",
      "2019-01-26T23:51:30.958373: step 3676, loss 4.20966e-05, acc 1\n",
      "2019-01-26T23:51:31.371787: step 3677, loss 0.00200874, acc 1\n",
      "2019-01-26T23:51:31.813325: step 3678, loss 1.65773e-06, acc 1\n",
      "2019-01-26T23:51:32.226842: step 3679, loss 0.000101872, acc 1\n",
      "2019-01-26T23:51:32.645888: step 3680, loss 0.0007214, acc 1\n",
      "2019-01-26T23:51:33.064055: step 3681, loss 3.26494e-05, acc 1\n",
      "2019-01-26T23:51:33.479075: step 3682, loss 3.26218e-05, acc 1\n",
      "2019-01-26T23:51:33.909244: step 3683, loss 3.45171e-05, acc 1\n",
      "2019-01-26T23:51:34.341997: step 3684, loss 0.000174503, acc 1\n",
      "2019-01-26T23:51:34.786993: step 3685, loss 7.48265e-05, acc 1\n",
      "2019-01-26T23:51:35.232738: step 3686, loss 1.47918e-05, acc 1\n",
      "2019-01-26T23:51:35.680266: step 3687, loss 4.27972e-05, acc 1\n",
      "2019-01-26T23:51:36.127741: step 3688, loss 5.42551e-06, acc 1\n",
      "2019-01-26T23:51:36.574408: step 3689, loss 1.10267e-06, acc 1\n",
      "2019-01-26T23:51:37.024695: step 3690, loss 4.43652e-06, acc 1\n",
      "2019-01-26T23:51:37.484669: step 3691, loss 6.76086e-06, acc 1\n",
      "2019-01-26T23:51:37.931475: step 3692, loss 0.00109547, acc 1\n",
      "2019-01-26T23:51:38.374621: step 3693, loss 3.58922e-06, acc 1\n",
      "2019-01-26T23:51:38.824497: step 3694, loss 1.18872e-05, acc 1\n",
      "2019-01-26T23:51:39.268051: step 3695, loss 0.00662881, acc 1\n",
      "2019-01-26T23:51:39.714942: step 3696, loss 0.000470896, acc 1\n",
      "2019-01-26T23:51:40.166958: step 3697, loss 4.17034e-06, acc 1\n",
      "2019-01-26T23:51:40.621640: step 3698, loss 0.000229195, acc 1\n",
      "2019-01-26T23:51:41.068280: step 3699, loss 3.14511e-05, acc 1\n",
      "2019-01-26T23:51:41.518788: step 3700, loss 0.000208326, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:51:42.655813: step 3700, loss 0.0976529, acc 0.987433\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3700\n",
      "\n",
      "2019-01-26T23:51:43.169707: step 3701, loss 2.63792e-05, acc 1\n",
      "2019-01-26T23:51:43.599826: step 3702, loss 6.98755e-06, acc 1\n",
      "2019-01-26T23:51:44.030316: step 3703, loss 1.05797e-06, acc 1\n",
      "2019-01-26T23:51:44.461066: step 3704, loss 0.00124719, acc 1\n",
      "2019-01-26T23:51:44.892856: step 3705, loss 2.16993e-06, acc 1\n",
      "2019-01-26T23:51:45.316155: step 3706, loss 2.94289e-06, acc 1\n",
      "2019-01-26T23:51:45.744508: step 3707, loss 0.00309976, acc 1\n",
      "2019-01-26T23:51:46.178324: step 3708, loss 1.03306e-05, acc 1\n",
      "2019-01-26T23:51:46.608503: step 3709, loss 3.57425e-06, acc 1\n",
      "2019-01-26T23:51:47.040474: step 3710, loss 3.00794e-06, acc 1\n",
      "2019-01-26T23:51:47.483392: step 3711, loss 7.81128e-05, acc 1\n",
      "2019-01-26T23:51:47.939446: step 3712, loss 0.000710254, acc 1\n",
      "2019-01-26T23:51:48.133479: step 3713, loss 1.00907e-05, acc 1\n",
      "2019-01-26T23:51:48.580114: step 3714, loss 3.12899e-06, acc 1\n",
      "2019-01-26T23:51:49.028187: step 3715, loss 9.81604e-07, acc 1\n",
      "2019-01-26T23:51:49.502366: step 3716, loss 0.000718449, acc 1\n",
      "2019-01-26T23:51:49.987545: step 3717, loss 9.95124e-06, acc 1\n",
      "2019-01-26T23:51:50.418733: step 3718, loss 0.000873691, acc 1\n",
      "2019-01-26T23:51:50.850125: step 3719, loss 4.07917e-07, acc 1\n",
      "2019-01-26T23:51:51.273517: step 3720, loss 9.50733e-05, acc 1\n",
      "2019-01-26T23:51:51.726936: step 3721, loss 2.77533e-07, acc 1\n",
      "2019-01-26T23:51:52.154259: step 3722, loss 1.02258e-06, acc 1\n",
      "2019-01-26T23:51:52.587808: step 3723, loss 2.58872e-05, acc 1\n",
      "2019-01-26T23:51:53.010513: step 3724, loss 1.29823e-06, acc 1\n",
      "2019-01-26T23:51:53.443067: step 3725, loss 6.35319e-06, acc 1\n",
      "2019-01-26T23:51:53.866233: step 3726, loss 0.000192017, acc 1\n",
      "2019-01-26T23:51:54.285294: step 3727, loss 1.25727e-06, acc 1\n",
      "2019-01-26T23:51:54.714139: step 3728, loss 9.45008e-05, acc 1\n",
      "2019-01-26T23:51:55.131967: step 3729, loss 0.000102599, acc 1\n",
      "2019-01-26T23:51:55.551842: step 3730, loss 5.34577e-07, acc 1\n",
      "2019-01-26T23:51:56.014016: step 3731, loss 3.1851e-07, acc 1\n",
      "2019-01-26T23:51:56.430505: step 3732, loss 1.4547e-06, acc 1\n",
      "2019-01-26T23:51:56.841871: step 3733, loss 0.000184562, acc 1\n",
      "2019-01-26T23:51:57.246054: step 3734, loss 1.34924e-05, acc 1\n",
      "2019-01-26T23:51:57.662190: step 3735, loss 0.000359463, acc 1\n",
      "2019-01-26T23:51:58.079431: step 3736, loss 3.72889e-06, acc 1\n",
      "2019-01-26T23:51:58.498407: step 3737, loss 3.47745e-06, acc 1\n",
      "2019-01-26T23:51:58.919373: step 3738, loss 8.99648e-07, acc 1\n",
      "2019-01-26T23:51:59.338139: step 3739, loss 8.16271e-06, acc 1\n",
      "2019-01-26T23:51:59.786451: step 3740, loss 9.21992e-07, acc 1\n",
      "2019-01-26T23:52:00.209500: step 3741, loss 2.3897e-06, acc 1\n",
      "2019-01-26T23:52:00.631871: step 3742, loss 1.05053e-06, acc 1\n",
      "2019-01-26T23:52:01.082447: step 3743, loss 2.79403e-05, acc 1\n",
      "2019-01-26T23:52:01.507051: step 3744, loss 3.23881e-05, acc 1\n",
      "2019-01-26T23:52:01.949034: step 3745, loss 1.90173e-06, acc 1\n",
      "2019-01-26T23:52:02.379046: step 3746, loss 1.90171e-06, acc 1\n",
      "2019-01-26T23:52:02.823587: step 3747, loss 0.000683912, acc 1\n",
      "2019-01-26T23:52:03.275694: step 3748, loss 2.96898e-05, acc 1\n",
      "2019-01-26T23:52:03.732456: step 3749, loss 2.31615e-05, acc 1\n",
      "2019-01-26T23:52:04.181325: step 3750, loss 4.54262e-06, acc 1\n",
      "2019-01-26T23:52:04.633562: step 3751, loss 2.00651e-05, acc 1\n",
      "2019-01-26T23:52:05.104638: step 3752, loss 7.45057e-08, acc 1\n",
      "2019-01-26T23:52:05.573233: step 3753, loss 1.02791e-05, acc 1\n",
      "2019-01-26T23:52:06.031178: step 3754, loss 9.18277e-07, acc 1\n",
      "2019-01-26T23:52:06.477432: step 3755, loss 3.62572e-05, acc 1\n",
      "2019-01-26T23:52:06.926457: step 3756, loss 2.69514e-06, acc 1\n",
      "2019-01-26T23:52:07.375471: step 3757, loss 9.53547e-06, acc 1\n",
      "2019-01-26T23:52:07.824688: step 3758, loss 1.55967e-05, acc 1\n",
      "2019-01-26T23:52:08.273577: step 3759, loss 0.00141096, acc 1\n",
      "2019-01-26T23:52:08.725634: step 3760, loss 0.000940789, acc 1\n",
      "2019-01-26T23:52:09.173129: step 3761, loss 0.000212301, acc 1\n",
      "2019-01-26T23:52:09.626343: step 3762, loss 1.34107e-06, acc 1\n",
      "2019-01-26T23:52:10.080566: step 3763, loss 3.88129e-05, acc 1\n",
      "2019-01-26T23:52:10.534464: step 3764, loss 2.3118e-05, acc 1\n",
      "2019-01-26T23:52:10.974146: step 3765, loss 3.3488e-06, acc 1\n",
      "2019-01-26T23:52:11.422924: step 3766, loss 1.31129e-06, acc 1\n",
      "2019-01-26T23:52:11.858472: step 3767, loss 0.000158138, acc 1\n",
      "2019-01-26T23:52:12.294494: step 3768, loss 1.2475e-05, acc 1\n",
      "2019-01-26T23:52:12.732677: step 3769, loss 0.000163762, acc 1\n",
      "2019-01-26T23:52:13.164537: step 3770, loss 0.000175778, acc 1\n",
      "2019-01-26T23:52:13.598484: step 3771, loss 9.31993e-05, acc 1\n",
      "2019-01-26T23:52:14.025980: step 3772, loss 0.00121564, acc 1\n",
      "2019-01-26T23:52:14.441254: step 3773, loss 7.54365e-07, acc 1\n",
      "2019-01-26T23:52:14.855995: step 3774, loss 5.58945e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:52:15.271982: step 3775, loss 5.64168e-06, acc 1\n",
      "2019-01-26T23:52:15.683640: step 3776, loss 0.000263778, acc 1\n",
      "2019-01-26T23:52:16.094626: step 3777, loss 0.00175834, acc 1\n",
      "2019-01-26T23:52:16.508250: step 3778, loss 9.56955e-06, acc 1\n",
      "2019-01-26T23:52:16.917338: step 3779, loss 0.000267973, acc 1\n",
      "2019-01-26T23:52:17.322459: step 3780, loss 5.32691e-06, acc 1\n",
      "2019-01-26T23:52:17.742032: step 3781, loss 0.000473084, acc 1\n",
      "2019-01-26T23:52:18.155635: step 3782, loss 1.85494e-05, acc 1\n",
      "2019-01-26T23:52:18.572166: step 3783, loss 2.09914e-06, acc 1\n",
      "2019-01-26T23:52:18.984791: step 3784, loss 0.00264082, acc 1\n",
      "2019-01-26T23:52:19.403655: step 3785, loss 8.26849e-06, acc 1\n",
      "2019-01-26T23:52:19.817435: step 3786, loss 3.28209e-05, acc 1\n",
      "2019-01-26T23:52:20.228874: step 3787, loss 8.15163e-05, acc 1\n",
      "2019-01-26T23:52:20.640643: step 3788, loss 0.000741657, acc 1\n",
      "2019-01-26T23:52:21.054939: step 3789, loss 6.5751e-07, acc 1\n",
      "2019-01-26T23:52:21.470647: step 3790, loss 9.74753e-05, acc 1\n",
      "2019-01-26T23:52:21.965794: step 3791, loss 0.000130657, acc 1\n",
      "2019-01-26T23:52:22.148913: step 3792, loss 3.20818e-06, acc 1\n",
      "2019-01-26T23:52:22.580884: step 3793, loss 6.76974e-06, acc 1\n",
      "2019-01-26T23:52:23.018448: step 3794, loss 2.01195e-05, acc 1\n",
      "2019-01-26T23:52:23.466545: step 3795, loss 6.14805e-05, acc 1\n",
      "2019-01-26T23:52:23.914125: step 3796, loss 5.30467e-05, acc 1\n",
      "2019-01-26T23:52:24.360320: step 3797, loss 4.553e-05, acc 1\n",
      "2019-01-26T23:52:24.821218: step 3798, loss 0.00204976, acc 1\n",
      "2019-01-26T23:52:25.282099: step 3799, loss 5.86927e-05, acc 1\n",
      "2019-01-26T23:52:25.730590: step 3800, loss 2.36827e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:52:26.847107: step 3800, loss 0.0939151, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3800\n",
      "\n",
      "2019-01-26T23:52:27.389354: step 3801, loss 1.21632e-05, acc 1\n",
      "2019-01-26T23:52:27.835838: step 3802, loss 6.55644e-05, acc 1\n",
      "2019-01-26T23:52:28.288456: step 3803, loss 1.13775e-05, acc 1\n",
      "2019-01-26T23:52:28.733810: step 3804, loss 0.00510715, acc 1\n",
      "2019-01-26T23:52:29.179108: step 3805, loss 1.40652e-05, acc 1\n",
      "2019-01-26T23:52:29.631203: step 3806, loss 0.000142849, acc 1\n",
      "2019-01-26T23:52:30.090818: step 3807, loss 3.63752e-06, acc 1\n",
      "2019-01-26T23:52:30.539021: step 3808, loss 2.72867e-06, acc 1\n",
      "2019-01-26T23:52:30.976612: step 3809, loss 5.51912e-05, acc 1\n",
      "2019-01-26T23:52:31.409840: step 3810, loss 3.6694e-07, acc 1\n",
      "2019-01-26T23:52:31.844000: step 3811, loss 4.29177e-05, acc 1\n",
      "2019-01-26T23:52:32.278039: step 3812, loss 1.05825e-05, acc 1\n",
      "2019-01-26T23:52:32.710943: step 3813, loss 0.000404973, acc 1\n",
      "2019-01-26T23:52:33.142849: step 3814, loss 1.20293e-05, acc 1\n",
      "2019-01-26T23:52:33.574631: step 3815, loss 3.85108e-05, acc 1\n",
      "2019-01-26T23:52:34.001789: step 3816, loss 4.03433e-06, acc 1\n",
      "2019-01-26T23:52:34.434475: step 3817, loss 1.28406e-05, acc 1\n",
      "2019-01-26T23:52:34.871086: step 3818, loss 1.07287e-06, acc 1\n",
      "2019-01-26T23:52:35.307882: step 3819, loss 8.77291e-07, acc 1\n",
      "2019-01-26T23:52:35.749122: step 3820, loss 6.82472e-05, acc 1\n",
      "2019-01-26T23:52:36.190102: step 3821, loss 8.1989e-05, acc 1\n",
      "2019-01-26T23:52:36.639776: step 3822, loss 6.33244e-06, acc 1\n",
      "2019-01-26T23:52:37.082061: step 3823, loss 0.000324091, acc 1\n",
      "2019-01-26T23:52:37.511904: step 3824, loss 2.67094e-06, acc 1\n",
      "2019-01-26T23:52:37.947336: step 3825, loss 6.00478e-06, acc 1\n",
      "2019-01-26T23:52:38.373613: step 3826, loss 1.83278e-06, acc 1\n",
      "2019-01-26T23:52:38.840154: step 3827, loss 4.76947e-05, acc 1\n",
      "2019-01-26T23:52:39.265588: step 3828, loss 9.53042e-05, acc 1\n",
      "2019-01-26T23:52:39.702203: step 3829, loss 0.000161571, acc 1\n",
      "2019-01-26T23:52:40.134240: step 3830, loss 9.2075e-06, acc 1\n",
      "2019-01-26T23:52:40.566232: step 3831, loss 7.39708e-05, acc 1\n",
      "2019-01-26T23:52:41.000699: step 3832, loss 0.00232464, acc 1\n",
      "2019-01-26T23:52:41.426253: step 3833, loss 3.14953e-06, acc 1\n",
      "2019-01-26T23:52:41.849028: step 3834, loss 0.000351903, acc 1\n",
      "2019-01-26T23:52:42.263212: step 3835, loss 3.20579e-05, acc 1\n",
      "2019-01-26T23:52:42.680837: step 3836, loss 1.91238e-05, acc 1\n",
      "2019-01-26T23:52:43.105894: step 3837, loss 2.68138e-05, acc 1\n",
      "2019-01-26T23:52:43.524703: step 3838, loss 2.5674e-05, acc 1\n",
      "2019-01-26T23:52:43.945024: step 3839, loss 9.2432e-06, acc 1\n",
      "2019-01-26T23:52:44.367502: step 3840, loss 2.00235e-05, acc 1\n",
      "2019-01-26T23:52:44.781900: step 3841, loss 0.000188179, acc 1\n",
      "2019-01-26T23:52:45.187246: step 3842, loss 5.75555e-07, acc 1\n",
      "2019-01-26T23:52:45.604919: step 3843, loss 4.32897e-05, acc 1\n",
      "2019-01-26T23:52:46.023098: step 3844, loss 9.50914e-05, acc 1\n",
      "2019-01-26T23:52:46.441705: step 3845, loss 4.89873e-07, acc 1\n",
      "2019-01-26T23:52:46.864540: step 3846, loss 0.00277055, acc 1\n",
      "2019-01-26T23:52:47.277669: step 3847, loss 9.48745e-06, acc 1\n",
      "2019-01-26T23:52:47.698212: step 3848, loss 7.2084e-07, acc 1\n",
      "2019-01-26T23:52:48.114042: step 3849, loss 0.000105144, acc 1\n",
      "2019-01-26T23:52:48.536608: step 3850, loss 3.74481e-05, acc 1\n",
      "2019-01-26T23:52:48.956713: step 3851, loss 4.33751e-05, acc 1\n",
      "2019-01-26T23:52:49.374514: step 3852, loss 5.07332e-06, acc 1\n",
      "2019-01-26T23:52:49.811419: step 3853, loss 8.3859e-06, acc 1\n",
      "2019-01-26T23:52:50.241694: step 3854, loss 0.000163306, acc 1\n",
      "2019-01-26T23:52:50.670989: step 3855, loss 6.20889e-06, acc 1\n",
      "2019-01-26T23:52:51.112010: step 3856, loss 8.49016e-05, acc 1\n",
      "2019-01-26T23:52:51.563628: step 3857, loss 1.38538e-05, acc 1\n",
      "2019-01-26T23:52:52.020785: step 3858, loss 5.14613e-06, acc 1\n",
      "2019-01-26T23:52:52.466447: step 3859, loss 2.81111e-05, acc 1\n",
      "2019-01-26T23:52:52.931582: step 3860, loss 2.4962e-05, acc 1\n",
      "2019-01-26T23:52:53.393995: step 3861, loss 3.94662e-06, acc 1\n",
      "2019-01-26T23:52:53.853221: step 3862, loss 1.94801e-05, acc 1\n",
      "2019-01-26T23:52:54.297003: step 3863, loss 2.13032e-05, acc 1\n",
      "2019-01-26T23:52:54.743503: step 3864, loss 3.48307e-06, acc 1\n",
      "2019-01-26T23:52:55.191182: step 3865, loss 0.000271507, acc 1\n",
      "2019-01-26T23:52:55.638877: step 3866, loss 0.00799396, acc 1\n",
      "2019-01-26T23:52:56.081445: step 3867, loss 4.31926e-05, acc 1\n",
      "2019-01-26T23:52:56.534427: step 3868, loss 5.27819e-05, acc 1\n",
      "2019-01-26T23:52:56.988613: step 3869, loss 1.44591e-05, acc 1\n",
      "2019-01-26T23:52:57.438194: step 3870, loss 0.00020373, acc 1\n",
      "2019-01-26T23:52:57.633779: step 3871, loss 0, acc 1\n",
      "2019-01-26T23:52:58.089500: step 3872, loss 3.23007e-05, acc 1\n",
      "2019-01-26T23:52:58.542099: step 3873, loss 6.85193e-05, acc 1\n",
      "2019-01-26T23:52:58.981723: step 3874, loss 2.81744e-05, acc 1\n",
      "2019-01-26T23:52:59.417978: step 3875, loss 1.50873e-06, acc 1\n",
      "2019-01-26T23:52:59.859194: step 3876, loss 0.00106804, acc 1\n",
      "2019-01-26T23:53:00.306345: step 3877, loss 4.03046e-06, acc 1\n",
      "2019-01-26T23:53:00.752218: step 3878, loss 0.00162082, acc 1\n",
      "2019-01-26T23:53:01.191236: step 3879, loss 0.00368819, acc 1\n",
      "2019-01-26T23:53:01.621947: step 3880, loss 1.55494e-05, acc 1\n",
      "2019-01-26T23:53:02.109939: step 3881, loss 3.72587e-05, acc 1\n",
      "2019-01-26T23:53:02.544180: step 3882, loss 0.000513254, acc 1\n",
      "2019-01-26T23:53:02.976143: step 3883, loss 1.91629e-05, acc 1\n",
      "2019-01-26T23:53:03.412243: step 3884, loss 2.46981e-06, acc 1\n",
      "2019-01-26T23:53:03.893496: step 3885, loss 0.000131871, acc 1\n",
      "2019-01-26T23:53:04.346001: step 3886, loss 1.12795e-05, acc 1\n",
      "2019-01-26T23:53:04.799131: step 3887, loss 5.09954e-06, acc 1\n",
      "2019-01-26T23:53:05.241234: step 3888, loss 0.00083313, acc 1\n",
      "2019-01-26T23:53:05.672397: step 3889, loss 1.61675e-06, acc 1\n",
      "2019-01-26T23:53:06.104572: step 3890, loss 9.01183e-06, acc 1\n",
      "2019-01-26T23:53:06.539090: step 3891, loss 2.21012e-05, acc 1\n",
      "2019-01-26T23:53:06.973845: step 3892, loss 0.000108805, acc 1\n",
      "2019-01-26T23:53:07.412609: step 3893, loss 9.83628e-05, acc 1\n",
      "2019-01-26T23:53:07.850096: step 3894, loss 7.37153e-06, acc 1\n",
      "2019-01-26T23:53:08.286786: step 3895, loss 0.000284929, acc 1\n",
      "2019-01-26T23:53:08.725226: step 3896, loss 1.93715e-07, acc 1\n",
      "2019-01-26T23:53:09.156898: step 3897, loss 1.83467e-06, acc 1\n",
      "2019-01-26T23:53:09.590257: step 3898, loss 3.70665e-07, acc 1\n",
      "2019-01-26T23:53:10.052523: step 3899, loss 1.22118e-05, acc 1\n",
      "2019-01-26T23:53:10.482910: step 3900, loss 6.06634e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-01-26T23:53:11.597053: step 3900, loss 0.0945921, acc 0.985637\n",
      "\n",
      "Saved model checkpoint to /home/bardia/Documents/TA/CI/Project/runs/1548532450/checkpoints/model-3900\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26T23:53:12.107347: step 3901, loss 1.28892e-06, acc 1\n",
      "2019-01-26T23:53:12.540817: step 3902, loss 4.89263e-06, acc 1\n",
      "2019-01-26T23:53:12.990475: step 3903, loss 7.54491e-05, acc 1\n",
      "2019-01-26T23:53:13.433059: step 3904, loss 5.24991e-05, acc 1\n",
      "2019-01-26T23:53:13.868639: step 3905, loss 3.20464e-05, acc 1\n",
      "2019-01-26T23:53:14.293768: step 3906, loss 2.47912e-06, acc 1\n",
      "2019-01-26T23:53:14.726903: step 3907, loss 2.34869e-06, acc 1\n",
      "2019-01-26T23:53:15.149054: step 3908, loss 6.38885e-07, acc 1\n",
      "2019-01-26T23:53:15.579636: step 3909, loss 0.0207752, acc 0.984375\n",
      "2019-01-26T23:53:16.006702: step 3910, loss 2.72272e-05, acc 1\n",
      "2019-01-26T23:53:16.434125: step 3911, loss 0.000392408, acc 1\n",
      "2019-01-26T23:53:16.866690: step 3912, loss 4.4217e-05, acc 1\n",
      "2019-01-26T23:53:17.293739: step 3913, loss 2.52284e-05, acc 1\n",
      "2019-01-26T23:53:17.729235: step 3914, loss 0.000819575, acc 1\n",
      "2019-01-26T23:53:18.154519: step 3915, loss 9.01642e-06, acc 1\n",
      "2019-01-26T23:53:18.590783: step 3916, loss 8.08313e-05, acc 1\n",
      "2019-01-26T23:53:19.017634: step 3917, loss 5.05879e-06, acc 1\n",
      "2019-01-26T23:53:19.451532: step 3918, loss 0.000367488, acc 1\n",
      "2019-01-26T23:53:19.897990: step 3919, loss 6.40523e-05, acc 1\n",
      "2019-01-26T23:53:20.339856: step 3920, loss 7.60544e-05, acc 1\n",
      "2019-01-26T23:53:20.787865: step 3921, loss 0.000111549, acc 1\n",
      "2019-01-26T23:53:21.229260: step 3922, loss 1.89155e-05, acc 1\n",
      "2019-01-26T23:53:21.669263: step 3923, loss 0.00534207, acc 1\n",
      "2019-01-26T23:53:22.142083: step 3924, loss 3.18317e-06, acc 1\n",
      "2019-01-26T23:53:22.571644: step 3925, loss 0.000200572, acc 1\n",
      "2019-01-26T23:53:22.997867: step 3926, loss 8.5406e-05, acc 1\n",
      "2019-01-26T23:53:23.428611: step 3927, loss 5.04502e-05, acc 1\n",
      "2019-01-26T23:53:23.857359: step 3928, loss 5.24062e-05, acc 1\n",
      "2019-01-26T23:53:24.285712: step 3929, loss 2.08112e-05, acc 1\n",
      "2019-01-26T23:53:24.716722: step 3930, loss 0.000416789, acc 1\n",
      "2019-01-26T23:53:25.144357: step 3931, loss 5.64332e-06, acc 1\n",
      "2019-01-26T23:53:25.576389: step 3932, loss 5.32562e-05, acc 1\n",
      "2019-01-26T23:53:26.009263: step 3933, loss 0.00137044, acc 1\n",
      "2019-01-26T23:53:26.435865: step 3934, loss 9.79961e-06, acc 1\n",
      "2019-01-26T23:53:26.869805: step 3935, loss 6.64525e-05, acc 1\n",
      "2019-01-26T23:53:27.327367: step 3936, loss 0.000297764, acc 1\n",
      "2019-01-26T23:53:27.771068: step 3937, loss 0.0001362, acc 1\n",
      "2019-01-26T23:53:28.214332: step 3938, loss 0.000211551, acc 1\n",
      "2019-01-26T23:53:28.668704: step 3939, loss 2.42083e-05, acc 1\n",
      "2019-01-26T23:53:29.112499: step 3940, loss 0.000515945, acc 1\n",
      "2019-01-26T23:53:29.545820: step 3941, loss 1.10407e-05, acc 1\n",
      "2019-01-26T23:53:30.032595: step 3942, loss 9.82246e-06, acc 1\n",
      "2019-01-26T23:53:30.459295: step 3943, loss 0.000352554, acc 1\n",
      "2019-01-26T23:53:30.919110: step 3944, loss 7.53713e-05, acc 1\n",
      "2019-01-26T23:53:31.347391: step 3945, loss 0.00025443, acc 1\n",
      "2019-01-26T23:53:31.786801: step 3946, loss 0.0004918, acc 1\n",
      "2019-01-26T23:53:32.211151: step 3947, loss 6.92635e-06, acc 1\n",
      "2019-01-26T23:53:32.642224: step 3948, loss 0.000238702, acc 1\n",
      "2019-01-26T23:53:33.070537: step 3949, loss 1.52449e-05, acc 1\n",
      "2019-01-26T23:53:33.254670: step 3950, loss 4.30188e-07, acc 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
    "train(x_train, y_train, vocab_processor, x_dev, y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
